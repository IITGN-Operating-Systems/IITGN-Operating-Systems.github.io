<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"}>
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff" />
    <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1b" />

    <meta name="description" content="Lab 5: Multicore and Networking">

    
        <title>Lab 5: Multicore and Networking | OS Labs</title>
    

    
    <style>
        :root {
          --background: #ffffff;
        }
        @media (prefers-color-scheme: dark) {
          :root {
            --background: #1b1b1b;
          }
        }
        html {
            background-color: var(--background);
        }
        body {
            background-color: var(--background);
        }
    </style>

    
    <link rel="stylesheet" type="text/css" href="/style.min.375e5ed55aac1492efe66cf6fd686d0af88cb56b12d6bafab6571d3b39c73883.css" media="all">
  </head>

  <body>
        
        <nav>
          <ul class="menu">
            
                <li><a tabindex="-1" class="menu-link" href="/">Home</a></li>
            
                <li><a tabindex="-1" class="menu-link" href="/tags">Tags</a></li>
            
          </ul>
        </nav>
        




<div id="single-header">
  <h1>
    Lab 5: Multicore and Networking
  </h1>
  <div id="single-meta">
    
    
        <span class="datesub">Apr 14, 2020 &nbsp;&nbsp; m. Jan 12, 2025</span>
      
<span id="tags">
  &nbsp;&nbsp;
   <span>
    <a href="//localhost:1313/tags/lab/">#lab</a>
  </span>&nbsp;</span>


  </div>
</div>



<aside class="hidden lg:block toc " id="tableOfContentContainer">
  <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#phase-0-getting-started">Phase 0: Getting Started</a>
      <ul>
        <li><a href="#merge-guideline">Merge Guideline</a></li>
        <li><a href="#logging-infrastructure">Logging infrastructure</a></li>
        <li><a href="#arm-documentation">ARM Documentation</a></li>
      </ul>
    </li>
    <li><a href="#phase-1-enabling-multicore">Phase 1: Enabling Multicore</a>
      <ul>
        <li><a href="#subphase-a-waking-up-other-cores">Subphase A: Waking Up Other Cores</a></li>
        <li><a href="#subphase-b-mutex-revisited">Subphase B: Mutex, Revisited</a></li>
        <li><a href="#subphase-c-multicore-scheduling">Subphase C: Multicore Scheduling</a></li>
      </ul>
    </li>
    <li><a href="#phase-2-tcp-networking">Phase 2: TCP Networking</a>
      <ul>
        <li><a href="#subphase-a-networking-101">Subphase A: Networking 101</a></li>
        <li><a href="#subphase-b-network-driver">Subphase B: Network Driver</a></li>
        <li><a href="#subphase-c-process-resource-management">Subphase C: Process Resource Management</a></li>
        <li><a href="#subphase-d-socket-system-calls">Subphase D: Socket System Calls</a></li>
      </ul>
    </li>
    <li><a href="#phase-3-echo-server">Phase 3: Echo Server</a>
      <ul>
        <li><a href="#implementation-5">Implementation</a></li>
        <li><a href="#how-to-test">How to Test</a></li>
      </ul>
    </li>
    <li><a href="#submission">Submission</a></li>
  </ul>
</nav>
</aside>



<main><ul>
<li><strong>Handed out:</strong> Tuesday, April 14, 2020</li>
<li><strong>Due:</strong> Friday, May 1, 2020</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>So far, our kernel has utilized only one core among four cores on a RPi board.
In this assignment, you will enable the other three cores and adjust the existing
components to correctly run programs in parallel.
After that, you will integrate an existing Ethernet implementation for RPi
(USPi) and a minimal TCP stack (smoltcp) to our kernel, so that our host computer
and the Raspberry Pi board can communicate via an Ethernet cable.
Finally, you will write an echo server as a user program on our kernel and
interact with that server from your host computer with netcat command.</p>
<h2 id="phase-0-getting-started">Phase 0: Getting Started</h2>
<p>Fetch the update for lab 5 from our git repository
to your development machine.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>$ git fetch skeleton
</span></span><span style="display:flex;"><span>$ git merge skeleton/lab5
</span></span></code></pre></div><p>This is the directory structure of our repository.
The directories you will be working on this assignment are
marked with <code>*</code>.</p>
<pre tabindex="0"><code class="language-none" data-lang="none">.
├── bin : common binaries/utilities
├── doc : reference documents
├── ext : external files (e.g., resources for testing)
├── tut : tutorial/practices
│    ├── 0-rustlings
│    ├── 1-blinky
│    ├── 2-shell
│    ├── 3-fs
│    ├── 4-spawn
│    └── 5-multicore : questions for lab5 *
├── boot : bootloader
├── kern : the main os kernel *
├── lib  : required libraries
│     ├── aarch *
│     ├── kernel_api *
│     ├── fat32
│     ├── pi *
│     ├── shim
│     ├── stack-vec
│     ├── ttywrite
│     ├── volatile
│     └── xmodem
└── user : user level program *
      ├── fib
      ├── sleep
      └── socket *
</code></pre><h3 id="merge-guideline">Merge Guideline</h3>
<p>You may need to resolve conflicts before continuing. For example,
if you see a message that looks like:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>Auto-merging kern/src/main.rs
</span></span><span style="display:flex;"><span>CONFLICT <span style="color:#91d7e3;font-weight:bold">(</span>content<span style="color:#91d7e3;font-weight:bold">)</span>: Merge conflict in kern/src/main.rs
</span></span><span style="display:flex;"><span>Automatic merge failed; fix conflicts and <span style="color:#c6a0f6">then</span> commit the result.
</span></span></code></pre></div><p>You will need to manually modify the <code>main.rs</code> file to resolve
the conflict. Ensure you keep all of your changes from lab 4.
Once all conflicts are resolved, add the resolved files with
<code>git add</code> and commit. For more information on resolving merge
conflicts, <a href="https://githowto.com/resolving_conflicts"  target="_blank" rel="noreferrer nofollow">see this tutorial on
githowto.com</a>
.</p>
<p>Various design has been changed from lab 4.
See the following summary of change for a merge guideline.</p>
<ul>
<li>
<p><strong>Safe / Unsafe changes</strong></p>
<p>Several safe / unsafe definitions have been changed to conform
better with Rust&rsquo;s safety guarantee.
If there is a merge conflict in this regard,
follow the updated definition.</p>
</li>
<li>
<p><strong>Scheduler</strong></p>
<ul>
<li>GlobalScheduler now uses <code>Box</code> in the definition.
Update <code>Scheduler::new()</code> function accordingly.</li>
<li>Move timer manipulation from <code>start()</code> to
<code>initialize_global_timer_interrupt()</code> method.
Call this function in <code>GlobalScheduler::start()</code>.</li>
</ul>
</li>
<li>
<p><strong>PageTable</strong></p>
<p>The value of <code>IO_BASE_END</code> has been increased to allow the kernel to
access the local timer address.
To support that, the kernel page table now uses three L3 entries instead of two.
Adjust related functions in <code>kern/src/vm/pagetable.rs</code>.</p>
</li>
<li>
<p><strong>VMM</strong></p>
<p>VMM includes more fields than before.
Calculate the base address of the kernel table in <code>initialize()</code>,
and save it to <code>kern_pt_addr</code> field with
<code>self.kern_pt_addr.store(kern_pt_addr, Ordering::Relaxed);</code>.
You will learn what this line means throughout this lab.</p>
<p>In addition, <code>setup()</code> is not called automatically in <code>initialize()</code> anymore.
This is to separate the behavior of initializing the virtual memory manager
and setting up the MMU for the current core,
so that they can be invoked independently with multiple cores.
You have to add <code>VMM.setup()</code> call in <code>kmain()</code>, right after <code>VMM.initialize()</code>.</p>
</li>
<li>
<p><strong>IRQ / Traps</strong></p>
<p>IRQ has been redesigned to use trait-based logic.
Read the change in <code>kern/src/traps/irq.rs</code>.
Update the <code>register()</code> and <code>invoke()</code> function,
and also <code>handle_exception()</code> as necessary.</p>
</li>
<li>
<p><code>write_str</code> <strong>syscall</strong></p>
<p>In the previous lab,
there was only a <code>write()</code> syscall which prints a single byte to the serial.
We have added <code>write_str()</code> syscall,
which takes a slice from the user and prints it atomically.
<code>kernel_api</code> library has been updated to use this;
Recompile your user programs and copy them to the SD card.</p>
</li>
</ul>
<h3 id="logging-infrastructure">Logging infrastructure</h3>
<p>Our kernel code now uses Rust&rsquo;s <code>log</code> crate instead of <code>kprintln!</code> for message logging.
It enables five logging macros <code>trace!</code>, <code>debug!</code>, <code>info!</code>, <code>warn!</code>, and <code>error!</code>.</p>
<p>The logging code is defined in <code>kern/src/logger.rs</code>.
If <code>VERBOSE_BUILD</code> environment variable is set during the build
(e.g., <code>VERBOSE_BUILD=1 make</code>),
all logs will be enabled.
Otherwise, trace level logs will not be displayed.
There is no specific requirement for what level of log you should use in each circumstance,
but here is a brief guideline:</p>
<ul>
<li>
<p><strong>Trace</strong>: A piece of information that will help debugging the kernel,
but too verbose to be enabled by default</p>
<ul>
<li>Scheduler switch log</li>
<li>IRQ interrupt log</li>
</ul>
</li>
<li>
<p><strong>Debug</strong>: A piece of information that developers might be interested</p>
<ul>
<li>Page table address</li>
</ul>
</li>
<li>
<p><strong>Info</strong>: A piece of information that the user of the kernel might be interested</p>
<ul>
<li>Amount of memory in the system</li>
<li>Kernel initialization status</li>
</ul>
</li>
<li>
<p><strong>Warn</strong>: An indication of exceptional erroneous situation</p>
<ul>
<li>Out of memory</li>
<li>Unknown exception from a user program</li>
</ul>
</li>
<li>
<p><strong>Error</strong>: An indication of an event that should never happen during the
normal execution of the kernel</p>
<ul>
<li>Debug assertion violations</li>
<li>Unknown exception inside a kernel</li>
</ul>
</li>
</ul>
<h3 id="arm-documentation">ARM Documentation</h3>
<p>Along with three documents that we referred in lab 4,</p>
<ul>
<li>
<p><a href="../files/ARMv8-Reference-Manual.pdf" >ARMv8 Reference Manual</a>
</p>
<p>This is the official reference manual for the ARMv8
architecture. This is a wholistic manual covering the entire
architecture in a general manner. For the specific
implementation of the architecture for the Raspberry Pi 3, see
the ARM Cortex-A53 Manual. We will be referring to sections
from this manual with notes of the form
(<a href="../files/ARMv8-Reference-Manual.pdf" >ref</a>
:
C5.2) which indicates that you should refer to section C5.2 of
the <a href="../files/ARMv8-Reference-Manual.pdf" >ARMv8 Reference
Manual</a>
.</p>
</li>
<li>
<p><a href="../files/ARM-Cortex-A53-Manual.pdf" >ARM Cortex-A53 Manual</a>
</p>
<p>Manual for the specific implementation of the ARMv8 (v8.0-A)
architecture as used by the Raspberry Pi 3. We will be
referring to sections from this manual with notes of the form
(<a href="../files/ARM-Cortex-A53-Manual.pdf" >A53</a>
:
4.3.30) which indicates that you should refer to section 4.3.30
of the <a href="../files/ARM-Cortex-A53-Manual.pdf" >ARM Cortex-A53
Manual</a>
.</p>
</li>
<li>
<p><a href="../files/ARMv8-A-Programmer-Guide.pdf" >ARMv8-A Programmer Guide</a>
</p>
<p>A high-level guide on how to program an ARMv8-A process. We
will be referring to sections from this manual with notes of
the form
(<a href="../files/ARMv8-A-Programmer-Guide.pdf" >guide</a>
:
10.1) which indicates that you should refer to section 10.1 of
the <a href="../files/ARMv8-A-Programmer-Guide.pdf" >ARMv8-A Programmer
Guide</a>
.</p>
</li>
</ul>
<p>We will use two additional document in lab 5.</p>
<ul>
<li>
<p><a href="../files/aarch64-generic-timer.pdf" >AArch64 Programmer&rsquo;s Guides: Generic Timer</a>
</p>
<p>A guide for the generic timer in ARM architecture.
We will be refering to sections from this manual with notes of the form
(<a href="../files/aarch64-generic-timer.pdf" >timer</a>
: 3.2)
which indicates that you should refer to section 3.2 of the
<a href="../files/aarch64-generic-timer.pdf" >AArch64 Programmer&rsquo;s Guides: Generic Timer</a>
.</p>
</li>
<li>
<p><a href="../files/QA7_rev3.4.pdf" >Quad-A7 Control</a>
</p>
<p>A guide for Quad-A7 core control, which includes descriptions for per-core
timer and interrupt handling.
We will be refering to sections from this manual with notes of the form
(<a href="../files/QA7_rev3.4.pdf" >QA7</a>
: 4.10)
which indicates that you should refer to section 4.10 of the
<a href="../files/QA7_rev3.4.pdf" >Quad-A7 Control</a>
.</p>
</li>
</ul>
<p>You can find all those five documents under <code>doc/</code> subdirectory of our lab repo.
We recommend that you download these five documents now and maintain them within easy reach.</p>
<h2 id="phase-1-enabling-multicore">Phase 1: Enabling Multicore</h2>
<p>Lab 4 covered preemptive scheduling, which allowed multiple user programs to
run inside a single kernel at the same time by means of context switching.
Note that although multiple programs were running on our kernel <em>concurrently</em>,
only one user program occupied the core at a time.
In this phase, you will enable the other three cores of RPi board and support
running user programs <em>in parallel</em>.
Parallel programming involves many indigenous problems that do not exist in
single-thread programming.
To fix this, you&rsquo;ll revisit the design of mutex, IRQ handler,
and scheduler and adjust them accordingly to multicore environment.</p>
<h3 id="subphase-a-waking-up-other-cores">Subphase A: Waking Up Other Cores</h3>
<p>In this subphase, you&rsquo;ll enable the three other cores of BCM2837
using the spin table mechanism.</p>
<h4 id="spin-table">Spin Table</h4>
<p>All cores in a CPU share the main memory (RAM),
thus it can be used as a communication medium among cores.
<em>Spin table</em> is a booting mechanism that utilizes this characteristics of RAM.
When you power on your RPi, the first core, core 0, will jump to <code>_start</code> function
defined in <code>kern/src/init.rs</code>.
All the other cores are spinning outside of the kernel, polling their spinning address.</p>
<p>This code from <a href="https://github.com/raspberrypi/tools/blob/b0c869bc929587a7e1d20a98e2dc828a24ca396a/armstubs/armstub8.S#L132-L154"  target="_blank" rel="noreferrer nofollow">RPi firmware</a>

implements the spin table mechanism.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-asm" data-lang="asm"><span style="display:flex;"><span><span style="color:#91d7e3">in_el2:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">mrs</span> <span style="color:#eed49f">x6</span>, <span style="color:#eed49f">MPIDR_EL1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">and</span> <span style="color:#eed49f">x6</span>, <span style="color:#eed49f">x6</span>, <span style="color:#6e738d;font-style:italic">#0x3
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">cbz</span> <span style="color:#eed49f">x6</span>, <span style="color:#eed49f">primary_cpu</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">adr</span> <span style="color:#eed49f">x5</span>, <span style="color:#eed49f">spin_cpu0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#91d7e3">secondary_spin:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">wfe</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">ldr</span> <span style="color:#eed49f">x4</span>, [<span style="color:#eed49f">x5</span>, <span style="color:#eed49f">x6</span>, <span style="color:#eed49f">lsl</span> <span style="color:#6e738d;font-style:italic">#3]
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">cbz</span> <span style="color:#eed49f">x4</span>, <span style="color:#eed49f">secondary_spin</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#0
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">b</span> <span style="color:#eed49f">boot_kernel</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#91d7e3">primary_cpu:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">ldr</span> <span style="color:#eed49f">w4</span>, <span style="color:#eed49f">kernel_entry32</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">ldr</span> <span style="color:#eed49f">w0</span>, <span style="color:#eed49f">dtb_ptr32</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#91d7e3">boot_kernel:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x1</span>, <span style="color:#6e738d;font-style:italic">#0
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x2</span>, <span style="color:#6e738d;font-style:italic">#0
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x3</span>, <span style="color:#6e738d;font-style:italic">#0
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">br</span> <span style="color:#eed49f">x4</span>
</span></span></code></pre></div><p>When RPi boots,
core 0 loads the kernel address from symbol <code>kernel_entry32</code> and branch to that address.
The RPi firmware will fill <code>kernel_entry32</code> with <code>kernel_address</code> value
specified in <code>config.txt</code> before this routine.
All the other cores spin inside <code>secondary_spin</code> loop with <code>wfe</code>.
While looping, they load 8 bytes address from <code>spin_cpu0 + 8 * core_idx</code>,
and branch to that address if it is non-zero.
Therefore, in order to wake up other cores, core 0 needs to write the starting address
to their spinning address and send events with <code>sev</code> instruction.
You&rsquo;ll use <code>init::start2</code> as an entrypoint for other cores.</p>
<p>Each core should use its own stack pointer to not interfere with the other cores.
Function <code>_start</code> assigns <code>KERN_STACK_BASE</code> as the stack register for core 0.
In our kernel design, core <code>i+1</code> will use the stack right below the stack of core <code>i</code>.
That is, <code>KERN_STACK_BASE - KERN_STACK_SIZE * i</code> for core <code>i</code>.</p>
<h4 id="implementation">Implementation</h4>
<p>Now you&rsquo;re ready to implement the core initialization routine.
You&rsquo;ll write <code>kmain()</code> in <code>kern/src/main.rs</code> and
<code>initialize_app_cores()</code>, <code>start2()</code>, and <code>kmain2()</code> in <code>kern/src/init.rs</code>.</p>
<p>You can implement these functions in any order you wish:</p>
<ul>
<li>
<p><strong>In</strong> <code>initialize_app_cores()</code> <strong>, write the address of</strong> <code>start2()</code> <strong>to each core&rsquo;s spinning address.</strong></p>
<p>The spinning base, <code>spin_cpu0</code>, is defined as a constant <code>SPINNING_BASE</code> in <code>pi/src/common.rs</code>.
Core 0 should calculate each core&rsquo;s spinning address,
write the address of <code>start2()</code> to those addresses, invoke <code>sev()</code>,
and wait for each core&rsquo;s acknowledgement.
Other cores should write 0 to their spinning address in <code>kmain2()</code> when they are fully awoken.
Core 0 should check that all cores&rsquo; spinning address has been overwritten with 0
before returning from <code>initialize_app_cores()</code>.</p>
</li>
<li>
<p><strong>In</strong> <code>start2()</code> <strong>, setup the stack pointer and branch to</strong> <code>kinit2()</code> <strong>.</strong></p>
<p>Set the stack pointer as described above.
Recall that you can extract the core index from <code>MPIDR_EL1</code>.
Then, branch to <code>kinit2</code> after setting up the stack register.
Be careful not to use any stack variable in <code>start2()</code>,
since we are changing the stack pointer.</p>


<style type="text/css">
    .box-shortcode {
      padding: 1.2em;
      padding-top: 1em;
      line-height: 1.54em;
      margin-top: 1em;
      margin-bottom: 2em;
      border-radius: 3px;
      color: var(--text);
      background: #f3ebe850;
    }

    .box-title {
      margin: -18px -18px 12px;
      padding: 4px 18px;
      border-radius: 3px 3px 0 0;
      font-weight: 700;
      color: var(--text);
      background: #6ab0de;
    }
    .box-shortcode.warning .box-title {
      background: #ff6b6b;
    }
    .box-shortcode.warning {
      background: #ff6b6b4f;
    }
    .box-shortcode.info .box-title {
      background: #0089e488;
    }
    .box-shortcode.info {
      background: #0089e41c;
    }
    .box-shortcode.important .box-title {
      background: #f7ec2c;
    }
    .box-shortcode.important {
      background: #f7ec2c7d;
    }
    .box-shortcode.tip .box-title {
      background: #a3ffa34d;
    }
    .box-shortcode.tip {
      background: #a3ffa34d;
    }
    .box-shortcode.question .box-title {
      background: #ffffff;
    }
    .box-shortcode.question {
      background: #786a9e;
    }
    .icon-box {
      display: inline-flex;
      align-self: center;
      margin-right: 8px;
    }
    .icon-box img,
    .icon-box svg {
      height: 1em;
      width: 1em;
      fill: var(--text-dim);
    }
    .icon-box img,
    .icon-box.baseline svg {
      top: 0.125em;
      position: relative;
    }
    .box-shortcode p {
      margin-bottom: 0.6em;
    }
    .box-shortcode p:first-of-type {
      display: inline;
    }
    .box-shortcode p:nth-of-type(2) {
      margin-top: 0.6em;
    }
    .box-shortcode p:last-child {
      margin-bottom: 0;
    }
  </style>

  
  <svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
    <symbol id="tip-box" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
      <path
        d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/>
    </symbol>
    <symbol id="important-box" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
      <path
        d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/>
    </symbol>
    <symbol id="warning-box" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
      <path
        d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/>
    </symbol>
    <symbol id="info-box" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
      <path
        d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/>
    </symbol>
    <symbol id="question-box" viewBox="0 0 16 16" preserveAspectRatio="xMidYMid meet">
      <path
      d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0zM5.496 6.033h.825c.138 0 .248-.113.266-.25.09-.656.54-1.134 1.342-1.134.686 0 1.314.343 1.314 1.168 0 .635-.374.927-.965 1.371-.673.489-1.206 1.06-1.168 1.987l.003.217a.25.25 0 0 0 .25.246h.811a.25.25 0 0 0 .25-.25v-.105c0-.718.273-.927 1.01-1.486.609-.463 1.244-.977 1.244-2.056 0-1.511-1.276-2.241-2.673-2.241-1.267 0-2.655.59-2.75 2.286a.237.237 0 0 0 .241.247zm2.325 6.443c.61 0 1.029-.394 1.029-.927 0-.552-.42-.94-1.029-.94-.584 0-1.009.388-1.009.94 0 .533.425.927 1.01.927z"/>
    </symbol>
  </svg>
<div class="box box-shortcode warning" >
    <span class="icon-box baseline">
      <svg><use href="#warning-box"></use></svg>
    </span>
    <p><code>aarch64::affinity()</code> might not work</p>
<p><code>aarch64::affinity()</code> is not guaranteed to be inlined,
and it might use the stack space to call the function.
Because we don&rsquo;t set the stack pointer yet,
you may not be able to use <code>aarch64::affinity()</code>.
Try to access <code>MPIDR_EL1</code> register directly.</p>
  </div>

</li>
<li>
<p><strong>In</strong> <code>kmain2()</code> <strong>, print a message and acknowledge that the core is available.</strong></p>
<p>If the initialization is successful, each core will reach <code>kmain2()</code> and
start executing it in EL1.
In <code>kmain2()</code>, write 0 to the core&rsquo;s spinning address to notify core 0.
Then print any message to the console and loop indefinitely.
You&rsquo;re welcome to use the newly added logging macros.</p>
</li>
</ul>
<p>When you finish writing the code, change your <code>kmain()</code> like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#c6a0f6">unsafe</span> <span style="color:#c6a0f6">fn</span> <span style="color:#8aadf4">kmain</span>() -&gt; <span style="color:#91d7e3;font-weight:bold">!</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">ALLOCATOR</span>.initialize();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">FILESYSTEM</span>.initialize();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">VMM</span>.initialize();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">SCHEDULER</span>.initialize();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    init::initialize_app_cores();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">VMM</span>.setup();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">SCHEDULER</span>.start();
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>If you run the kernel on RPi board, you should see initialization message from each core.
However, these new cores are not doing any useful works yet,
because some components in our kernel are not parallel-ready.
The next step is to fix those outdated designs!</p>


<div class="box box-shortcode warning" >
    <span class="icon-box baseline">
      <svg><use href="#warning-box"></use></svg>
    </span>
    <p>Beware of unsound Mutex!</p>
<p>Note that our mutex is unsound, so <code>kprintln!()</code> and logging macros,
which internally uses mutex,
may deadlock or even trigger a data race when accessed by multiple cores.
This should be a rare case, and if you retry enough time,
you should see a successful case.
We are going to fix this soon.</p>
  </div>



<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>How to detect spinning base without hard coding the address? (spin-address)</p>
<p>We have hardcoded the spin address of Raspberry Pi to our kernel,
so the kernel won&rsquo;t run on another hardware if they use different spin
address.
How does Linux kernel solve this problem?</p>
  </div>

<h3 id="subphase-b-mutex-revisited">Subphase B: Mutex, Revisited</h3>
<p>The mutex code in our skeleton code doesn&rsquo;t provide an actual thread safety
and lies to Rust compiler by unsafely implementing Send and Sync trait.
We did this because atomic memory operations required for
thread-safe mutex implementation is only available when MMU is initialized
in AArch64 architecture, which was merely added in lab 4.
We justified our design with the fact that there is only one core available in the system,
but since we have both MMU and multiple cores enabled now,
it is the time to correct that lie.
In this subphase, you&rsquo;ll fix the mutex design so that multiple cores can use mutex in a sound manner.
You will be working in <code>kern/src/mutex.rs</code>.</p>
<h4 id="per-core-data-management">Per-core Data Management</h4>
<p>When multiple cores are active in a kernel,
the kernel must distinguish per-kernel resource initialization and per-core resource initialization.
Per-kernel resource needs to be initialized only once per startup.
In our kernel, per-kernel resources are defined in <code>main.rs</code>.
Static structs such as <code>ALLOCATOR</code>, <code>FILESYSTEM</code>, and <code>SCHEDULER</code> are per-kernel resources.
Core 0 should initialize per-kernel resources before waking up other cores.
On the other hand, per-core resource must be initialized for each core in <code>kmain()</code> and <code>kmain2()</code>.</p>
<p>Now go ahead and read per-core resource definitions in <code>kern/src/percore.rs</code>.
Each <code>PerCore</code> struct has three data fields:
<code>preemption</code>, <code>mmu_ready</code>, and <code>local_irq</code>.
<code>preemption</code> counts the number of lock held by the core, <code>mmu_ready</code> flag
saves whether MMU is set for each core, and finally <code>local_irq</code> saves
IRQ handler for each local interrupt.</p>
<p>You may notice atomic types used in the definition of <code>PerCore</code> struct.
For now, think of them as a plain type with internal mutability.
We will discuss about atomic types in detail later.</p>
<p>Then, start reading code in <code>kern/src/vm.rs</code>.
In lab 4, <code>setup()</code> was called directly in <code>initialize()</code>,
and core 0 immediately initialized its MMU after initializing the virtual memory manager.
However, in lab 5, the role of them are separated.
Core 0 initializes the virtual memory manager during the global initialization routine,
and each core, including core 0, sets up its MMU after the core initialization is done.</p>
<p>We have provided a partial implementation of <code>wait()</code>.
See how it calls <code>setup()</code> and, how <code>set_mmu_ready()</code> and <code>is_mmu_ready()</code>
is used to track per-core MMU initialization information.
Later you&rsquo;ll fill the rest of the code so that each core loops and waits until
the MMU initialization of all cores finishes
and returns together.</p>
<h4 id="memory-consistency-model">Memory Consistency Model</h4>
<p>Recall that RAM is shared among all cores.
When multiple cores are reading to and writing from a same address,
in which order they would read and write a value?
What can be the observable output from a program?
Can the same program behave differently on x86-64 and AArch64?
Architectures&rsquo; <em>memory consistency model</em> specifies the answer to these questions.</p>
<p>Let&rsquo;s look at a classic example of memory ordering,
where two threads are mutating values and printing them.
Rust&rsquo;s type system prevents this example to compile without an unsafe block,
but let&rsquo;s assume that this code was written in a racy variant of Rust
for the perpose of demonstration.
Assume A and B are both initialized to 0.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Thread 1 */</span>
</span></span><span style="display:flex;"><span>A <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (1)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">print!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, B); <span style="color:#6e738d;font-style:italic">// (2)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>
</span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Thread 2 */</span>
</span></span><span style="display:flex;"><span>B <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (3)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">print!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, A); <span style="color:#6e738d;font-style:italic">// (4)
</span></span></span></code></pre></div><p>One of the most intuitive memory consistency model is <strong>sequential consistency (SC)</strong>.
It assumes two things:
(1) there is a <em>global</em> order of all reads and writes, and
(2) the interleaving preserves the order of instructions from the same thread
(program order).
In sequential consistency, one of possible output of the above program is <code>01</code>,
where one thread completes the execution before the other thread start the execution.
The order of the program that prints this result 1-2-3-4 or 3-4-1-2.
<code>11</code> is slightly less obvious output, where the execution of two threads are
interleaved, for example like 1-3-2-4 or 3-1-2-4.
In this model, <code>00</code> is not a valid output of the above program.</p>


<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>Multithread vs Multicore</p>
<p>Strictly speaking, multithread and multicore are different concepts.
If multiple threads are running on a single core,
there is no memory consistency problem.
However, almost every CPU has multiple cores these days,
and two terms are used somewhat interchangably.
When you see a term &ldquo;multiple threads&rdquo; in this lab document,
you can assume that it implies multiple threads running on a multicore CPU
if not stated otherwise.</p>
  </div>

<p>Modern CPUs leverage numerous optimizations such as out-of-order execution, per-core caches,
and load/store optimization which significantly improves the execution performance.
Unfortunately, SC is too strict to permit these optimizations.
SC requires a global ordering of every memory operation running in multiple cores,
which is essentially requiring single threaded behavior for multicore architecture.
In result, modern architectures adopt weaker memory consistency model than SC.</p>
<figure><img src="./res/a53-diagram.jpg"><figcaption>
      <h4>Each core in Cortex-A53 has its own instruction and data cache</h4>
    </figcaption>
</figure>

<p><strong>Total store ordering (TSO)</strong> is slightly weaker consistency model than SC.
It allows store buffering, which may delay the propagation of write operations to other cores.
This weakening allows significant performance improvement over the SC model,
and x86 and x86-64 specifies a memory consistency that is very similar to TSO.</p>
<p>TSO upholds many guarantees of SC, but it allows a behavior that is ruled out by SC.
Let&rsquo;s look at the example code again:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Thread 1 */</span>
</span></span><span style="display:flex;"><span>A <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (1)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">print!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, B); <span style="color:#6e738d;font-style:italic">// (2)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>
</span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Thread 2 */</span>
</span></span><span style="display:flex;"><span>B <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (3)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">print!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, A); <span style="color:#6e738d;font-style:italic">// (4)
</span></span></span></code></pre></div><p>Under TSO model, it is possible to observe the result <code>00</code> from this program.
If <code>A = 1</code> is stored in thread 1&rsquo;s cache and <code>B = 1</code> is stored in thread 2&rsquo;s cache,
and each thread continues the execution before the write propagates to each other,
they may both print 0 as the result.
This is not an allowed behavior under SC.</p>
<p>ARM architecture specifies even weaker memory model.
Consider the following program:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Thread 1 */</span>
</span></span><span style="display:flex;"><span>B <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (1)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>A <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (2)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>
</span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Thread 2 */</span>
</span></span><span style="display:flex;"><span><span style="color:#8aadf4">print!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, A); <span style="color:#6e738d;font-style:italic">// (3)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">print!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, B); <span style="color:#6e738d;font-style:italic">// (4)
</span></span></span></code></pre></div><p>In ARM consistency model, surprisingly the program can print <code>10</code> as a result.
Which means, thread 2 may observe the writes from thread 1 in different order
from the order that they are written in thread 1.
Under TSO, this is not allowed because subsequent writes become visible to other cores in
the order that they have written
(hence the name, &ldquo;total store ordering&rdquo;).</p>
<p>ARM architecture said to have &ldquo;weak memory ordering with data dependency&rdquo;.
In weak memory ordering model, in general,
there is no consistency guarantee between two different memory locations
regardless of the program order.
However, if a value in one memory location depends on a value in another memory
location, there is a data dependency guarantee.</p>
<p>Consider the following two examples:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Program 1 */</span>
</span></span><span style="display:flex;"><span>A <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (1)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>B <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (2)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>
</span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* Program 2 */</span>
</span></span><span style="display:flex;"><span>C <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">1</span>; <span style="color:#6e738d;font-style:italic">// (3)
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>D <span style="color:#91d7e3;font-weight:bold">=</span> C; <span style="color:#6e738d;font-style:italic">// (4)
</span></span></span></code></pre></div><p>In program 1, A and B are independent memory locations, so other threads
may observe (2) before (1).
However, in program 2, the value of D depends on the value of C,
so if other threads observe 1 at memory location D,
they are guaranteed to observe 1 at memory location C.</p>


<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Memory consistency example (consistency-handson)</p>
<p>Using two threads, write an example that will print different set of results
on SC, TSO, and ARM hardware. Specify what can be printed on each memory
consistency model.</p>
  </div>

<p>Moreover, not only hardware, but also a language defines its memory consistency model.
When a compiler optimizes program code,
it might reorder or even completely remove certain statements based on the language specification.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* program 1 */</span>
</span></span><span style="display:flex;"><span><span style="color:#ed8796">let</span> <span style="color:#c6a0f6">mut</span> x <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#91d7e3;font-weight:bold">&amp;</span><span style="color:#c6a0f6">mut</span> <span style="color:#f5a97f">1</span>;
</span></span><span style="display:flex;"><span><span style="color:#c6a0f6">for</span> i <span style="color:#c6a0f6">in</span> <span style="color:#f5a97f">0</span><span style="color:#91d7e3;font-weight:bold">..</span><span style="color:#f5a97f">1000</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">println!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, <span style="color:#91d7e3;font-weight:bold">*</span>x);
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* program 2 */</span>
</span></span><span style="display:flex;"><span><span style="color:#c6a0f6">for</span> i <span style="color:#c6a0f6">in</span> <span style="color:#f5a97f">0</span><span style="color:#91d7e3;font-weight:bold">..</span><span style="color:#f5a97f">1000</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">println!</span>(<span style="color:#a6da95">&#34;</span><span style="color:#a6da95">{}</span><span style="color:#a6da95">&#34;</span>, <span style="color:#f5a97f">1</span>);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>In the above code snippet,
program 1 and 2 are equivalent if there is no other thread
modifying the variable <code>x</code> during the loop.
Is it valid for Rust compiler to optimize program 1 to program 2?
Recall that Rust&rsquo;s mutable reference implies the exclusive access to the underlying value.
Thus, the answer is yes in Rust;
The compiler can optimize program 1 to program 2
based on the assumption that <code>x</code> is a unique access to the underlying memory location.
To tell the compiler that <code>x</code> might be modified by other threads,
a raw pointer or an <code>UnsafeCell</code> must be used instead of a mutable reference <code>&amp;mut</code>.</p>
<h4 id="memory-barrier-and-atomic-instruction">Memory Barrier and Atomic Instruction</h4>
<p>As you should have felt at this point, writing a correct parallel program is
substantially harder than writing a correct single-threaded program.
Parallel programming involves numerous cognitive pitfalls,
and AArch64&rsquo;s weak memory consistency model further complicates the reasoning
of a parallel program.</p>
<p>Memory barriers and atomic instructions are tools that an architecture provide
to tame the complexity of memory ordering.
They allow programmers to manually introduce stronger memory ordering guarantee
in localized, controlled manner.
At language level, they also provide a way to write a portable code
that can be compiled to several architectures with different memory consistency guarantee.</p>
<p><strong>Memory barrier</strong> ensures the dependency between the instructions before and after the barrier.
With memory barrier, it is possible to explicitly specify the dependency between
instructions before and after the barrier
when strong memory ordering guarantee is required for program correctness.</p>
<p>In ARM architecture, there are three kinds of memory barrier instructions (ref: B2.3.5).
By default, they act as a full system barrier operation,
which fully synchronizes the instructions before and after the barrier
(according to their instruction type).
You can adjust the behavior with option parameter, such that they only wait for
specific instruction types.
For instance, it is possible to make them wait only for stores, not for load.</p>
<ul>
<li>
<p><strong>DMB</strong>, Data Memory Barrier</p>
<p>Data memory barrier acts as a memory barrier. It ensures that all explicit memory
access that appear in program order before the DMB instruction are observed before
any explicit memory access that appear in program order after the DMB instruction.</p>
</li>
<li>
<p><strong>DSB</strong>, Data Synchronization Barrier</p>
<p>Data Synchronization Barrier acts as a special kind of memory barrier.
No instruction in program order after this instruction executes until this instruction completes.</p>
</li>
<li>
<p><strong>ISB</strong>, Instruction Synchronization Barrier</p>
<p>Instruction Synchronization Barrier flushes the pipeline in the processor,
so that all instructions following the ISB are fetched from cache or memory,
after the instruction has been completed.</p>
</li>
</ul>


<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Barriers in <code>context_restore</code> (context-restore-barrier)<br>
Recall these four instructions that we put after overwriting TTBR registers in lab 4.
Explain what do these lines mean, and why they are needed when updating a page table.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-asm" data-lang="asm"><span style="display:flex;"><span><span style="color:#8aadf4">dsb</span>     <span style="color:#eed49f">ishst</span>
</span></span><span style="display:flex;"><span><span style="color:#8aadf4">tlbi</span>    <span style="color:#eed49f">vmalle1</span>
</span></span><span style="display:flex;"><span><span style="color:#8aadf4">dsb</span>     <span style="color:#eed49f">ish</span>
</span></span><span style="display:flex;"><span><span style="color:#8aadf4">isb</span>
</span></span></code></pre></div>
  </div>

<p><strong>Atomic instruction</strong> is another tool that architectures provide to facilitate parallel programming.
Atomic instructions guarantee that it is not possible to partially perform certain actions.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-asm" data-lang="asm"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* thread 1 */</span>
</span></span><span style="display:flex;"><span><span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#0
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x1</span>, <span style="color:#eed49f">addr</span> <span style="color:#eed49f">of</span> <span style="color:#eed49f">counter</span>
</span></span><span style="display:flex;"><span><span style="color:#91d7e3">loop:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">ldr</span> <span style="color:#eed49f">x2</span>, [<span style="color:#eed49f">x1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">add</span> <span style="color:#eed49f">x2</span>, <span style="color:#eed49f">x2</span>, <span style="color:#6e738d;font-style:italic">#1
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">str</span> <span style="color:#eed49f">x2</span>, [<span style="color:#eed49f">x1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">add</span> <span style="color:#eed49f">x0</span>, <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#1
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">cmp</span> <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#1000
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">ble</span> <span style="color:#eed49f">loop</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic">/* program 2 */</span>
</span></span><span style="display:flex;"><span><span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#0
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span><span style="color:#8aadf4">mov</span> <span style="color:#eed49f">x1</span>, <span style="color:#eed49f">addr</span> <span style="color:#eed49f">of</span> <span style="color:#eed49f">counter</span>
</span></span><span style="display:flex;"><span><span style="color:#91d7e3">loop:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">ldr</span> <span style="color:#eed49f">x2</span>, [<span style="color:#eed49f">x1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">add</span> <span style="color:#eed49f">x2</span>, <span style="color:#eed49f">x2</span>, <span style="color:#6e738d;font-style:italic">#1
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">str</span> <span style="color:#eed49f">x2</span>, [<span style="color:#eed49f">x1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#8aadf4">add</span> <span style="color:#eed49f">x0</span>, <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#1
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">cmp</span> <span style="color:#eed49f">x0</span>, <span style="color:#6e738d;font-style:italic">#1000
</span></span></span><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"></span>    <span style="color:#8aadf4">ble</span> <span style="color:#eed49f">loop</span>
</span></span></code></pre></div><p>Consider the above program, where two threads are incrementing the same memory address.
They both loop 1000 times, so ideally the counter should contain 2000 when the threads
finish the execution.
Sadly, the increment is not performed in an atomic manner, so each thread can
read the &ldquo;intermediate&rdquo; value of each other. The program does not prevent
thread 2 to load the value of the counter between the time frame of thread 1&rsquo;s
load and store.
When it happens, both thread 1 and 2 write the same value to the counter,
resulting in the counter to be only incremented by one instead of two.</p>
<p>Atomic instructions help resolving this problem by either performing an action
in non-preemptive manner (single instruction to perform fetch and add) or provide
a mechanism to detect the data contention so that the operation can be retried.
They are building blocks for higher level synchronization primitives such as mutex.
Check ARMv8-A Architecture Reference Manual C3.2.12-14 and
ARMv8-A Synchronization Primitives document to read more about
AArch64&rsquo;s atomic instructions.</p>


<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Writing atomic assembly code (atomic-handson)</p>
<p>Rewrite the above program with atomic instructions,
so that the answer is always correct.</p>
  </div>



<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>Check your architecture version</p>
<p>You should check the architecture version when writing an atomic instruction.
For instance, CAS, compare and swap instruction is only available after
ARMv8.1 and cannot be used in ARMv8.
If you are using Rust, the LLVM backend will automatically handle this.</p>
  </div>

<p>Except the case when you work at low-level inline assembly level,
you should use atomic data types such as
<a href="https://doc.rust-lang.org/core/sync/atomic/struct.AtomicBool.html"  target="_blank" rel="noreferrer nofollow">AtomicBool</a>

and
<a href="https://doc.rust-lang.org/core/sync/atomic/struct.AtomicUsize.html"  target="_blank" rel="noreferrer nofollow">AtomicUsize</a>

provided by Rust&rsquo;s standard library.
They provide high-level atomic operations such as <code>fetch_and()</code> and <code>compare_and_swap()</code>.
Even if the architecture does not support them as a single operation,
the compiler will preserve the semantics and implement them with
several smaller atomic operations.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#c6a0f6">impl</span> AtomicBool {
</span></span><span style="display:flex;"><span>    <span style="color:#c6a0f6">pub</span> <span style="color:#c6a0f6">fn</span> <span style="color:#8aadf4">fetch_and</span>(<span style="color:#91d7e3;font-weight:bold">&amp;</span><span style="color:#91d7e3">self</span>, val: <span style="color:#ed8796">bool</span>, order: <span style="color:#eed49f">Ordering</span>) -&gt; <span style="color:#ed8796">bool</span> { <span style="color:#91d7e3;font-weight:bold">..</span>. }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#c6a0f6">impl</span> AtomicUsize {
</span></span><span style="display:flex;"><span>    <span style="color:#c6a0f6">pub</span> <span style="color:#c6a0f6">fn</span> <span style="color:#8aadf4">compare_and_swap</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#91d7e3;font-weight:bold">&amp;</span><span style="color:#91d7e3">self</span>,
</span></span><span style="display:flex;"><span>        current: <span style="color:#ed8796">usize</span>,
</span></span><span style="display:flex;"><span>        new: <span style="color:#ed8796">usize</span>,
</span></span><span style="display:flex;"><span>        order: <span style="color:#eed49f">Ordering</span>
</span></span><span style="display:flex;"><span>    ) -&gt; <span style="color:#ed8796">usize</span> { <span style="color:#91d7e3;font-weight:bold">..</span>. }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Note the <code>&amp;self</code> requirement in those operations.
A non-mutable reference in Rust implies that the access to the value is shared.
Although multiple code accesses the value at the same time,
atomic operations guarantee that there will be no data race.
Thus, it is valid to use <code>&amp;self</code> instead of <code>&amp;mut self</code> in these operations.</p>
<figure><img src="./res/acq-rel.svg"><figcaption>
      <h4>A visualization of acquire and release semantics</h4>
    </figcaption>
</figure>

<p>These atomic instructions accept an <code>Ordering</code> parameter.
Rust provides <a href="https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html"  target="_blank" rel="noreferrer nofollow">five memory orderings</a>
,
which are the same as <a href="https://en.cppreference.com/w/cpp/atomic/memory_order"  target="_blank" rel="noreferrer nofollow">those of C++20</a>
.
<em>Relaxed</em> ordering provides atomic operation without ordering guarantee.
<em>Acquire</em> and <em>release</em> orderings are used in pair;
A load with acquire ordering prevents instructions after load to be reordered before it,
and a store with release ordering prevents instructions before store to be reordered after it.
In result, an acquire-release pair of an address creates a critical region that
prevents instructions between them from being reordered to outside of the critical region.
In addition, if thread A writes to a variable with release ordering and thread B
reads the same variable with acquire ordering, all subsequent loads in thread B
can see stores from thread B before the write with release ordering.
<em>AcqRel</em> is <em>Acquire</em> plus <em>Release</em> for atomic instructions that perform both load and store.
<em>SeqCst</em> is <em>AcqRel</em> with the additional guarantee that all threads see sequentially
consistent operations in the same order.</p>


<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>Further reading</p>
<p><a href="https://doc.rust-lang.org/nomicon/concurrency.html"  target="_blank" rel="noreferrer nofollow">Chapter 8 of Rustonomicon</a>

contains a summary of what we have covered.</p>
  </div>

<h4 id="implementation-1">Implementation</h4>
<p>Now you have enough knowledge to fix the mutex code.
You will primarily working in <code>kern/src/mutex.rs</code>, while fixing other functions
in <code>kern/src/vm.rs</code>, <code>kern/src/main.rs</code>, and <code>kern/src/init.rs</code>.</p>
<p>First, start with per-core MMU initialization.
Add <code>VMM.wait()</code> to <code>kmain()</code> like below:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#c6a0f6">unsafe</span> <span style="color:#c6a0f6">fn</span> <span style="color:#8aadf4">kmain</span>() -&gt; <span style="color:#91d7e3;font-weight:bold">!</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">ALLOCATOR</span>.initialize();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">FILESYSTEM</span>.initialize();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">VMM</span>.initialize();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">SCHEDULER</span>.initialize();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    init::initialize_app_cores();
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">VMM</span>.wait();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#eed49f">SCHEDULER</span>.start();
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Then, modify <code>kmain2()</code> in <code>kern/src/init.rs</code> so that it calls
<code>VMM.wait()</code> after writing 0 to its spinning address to acknowledge
the core boot sequence.
Once you are finished, print any message inside an infinite loop in the next line.</p>
<p>Next, finish the implementation of <code>wait()</code> in <code>kern/src/vm.rs</code>.
Specifically, after each core setup its MMU by calling <code>setup()</code>
(which is included in the skeleton code),
increment the <code>ready_core_cnt</code> by one
and loop until the count reaches the number of core, <code>pi::common::NCORES</code>.
Think about which atomic ordering should be used here.</p>
<p>The provided mutex code used relaxed load and store,
which do not synchronize among cores.
Therefore, if you test your code at this point,
the printing result from cores can be mixed together
or sometimes the kernel even deadlocks and stops making progress.</p>
<p>Now, fix <code>try_lock()</code> and <code>unlock()</code> in <code>kern/src/mutex.rs</code>.
You need to use atomic operations as well as APIs in <code>kern/src/percore.rs</code>
to properly implement the mutex.
Specifically,
use <code>is_mmu_ready()</code> to check whether MMU is enabled or disabled,
<code>getcpu()</code> to increment core&rsquo;s preemption counter when you lock a mutex,
and <code>putcpu()</code> to decrement core&rsquo;s preemption counter when you unlock a mutex.</p>
<p>Here is the description of how mutex should behave when MMU is enabled and disabled:</p>
<ul>
<li>
<p><strong>When MMU is disabled</strong><br>
A mutex in this state is used when core 0 is initializing per-kernel resources.
If MMU is disabled, general atomic operations are not available except
relaxed load and store.
As such, we will only allow core 0 to use mutex when MMU is disabled.
If only one core is accessing the mutex, these operations are sufficient
to implement a mutex.
In fact, it can be thought as a mutable-only version of <code>RefCell</code>.</p>
<p>You can reuse most of the current mutex code to implement this.
Add the necessary checks to make sure your mutex follows
Rust&rsquo;s safety requirement.</p>


<div class="box box-shortcode tip" >
    <span class="icon-box baseline">
      <svg><use href="#tip-box"></use></svg>
    </span>
    <p>The original mutex code supported reentrancy, which allows the owner of
a mutex to lock it more than once.
However, this design is not compatible with Rust&rsquo;s one mutable owner
model in general.
You don&rsquo;t have to implement this when you fix the mutex design.</p>
  </div>

</li>
<li>
<p><strong>When MMU is enabled</strong><br>
When MMU is enabled,
perform correct locking and unlocking with atomic operations.
The overall code structure will look similar with the MMU-disabled state,
but instead of relaxed load and store,
atomic operations such as <code>compare_and_swap</code> or <code>swap</code> should be used
with correct ordering parameter.
Check APIs of <a href="https://doc.rust-lang.org/core/sync/atomic/struct.AtomicBool.html"  target="_blank" rel="noreferrer nofollow">AtomicBool</a>

and choose whatever atomic operation you want.</p>
</li>
</ul>
<p>When you are done, messages from each core should be printed line by line
without any overlap or deadlock.
Double check your implementation even if you get the expected behavior.
Parallel code is indeterministic in nature,
which makes bugs in them very hard to be reproduced and fixed.
It&rsquo;s much better to take time now and prevent those bugs
than be puzzled with inscrutable behaviors later.
If you feel confident of your design, continue to the next subphase.</p>


<div class="box box-shortcode warning" >
    <span class="icon-box baseline">
      <svg><use href="#warning-box"></use></svg>
    </span>
    <p>Be careful not to use mutex before MMU initialization</p>
<p>If you have followed the instruction correctly,
<code>Mutex</code> will assert that the core number is equal to 0
when MMU is not initialized for the current core.
If other core tries to use mutex before MMU initialization,
it will halt the core with infinite recursion,
because <code>assert!()</code> calls <code>kprintln!()</code>, <code>kprintln!()</code>
calls <code>Mutex::lock()</code>, and <code>Mutex::lock()</code> calls <code>assert!()</code>.</p>
<p>When this happens,
the kernel becomes unresponsive without printing anything to the console.
If you find your kernel behave in this way,
try to find a lock usage before MMU.
Note that <code>kprintln!()</code> and logging macros use console lock internally.</p>
  </div>



<div class="box box-shortcode warning" >
    <span class="icon-box baseline">
      <svg><use href="#warning-box"></use></svg>
    </span>
    <p><code>Send</code> and <code>Sync</code> variance is hard</p>
<p>A trait implementation of a wrapper type sometimes depends on a trait
implementation of an inner type.
This is called &ldquo;variance&rdquo;.</p>
<p>Containers like <code>Vec&lt;T&gt;</code> implements <code>Send</code> if <code>T</code> is <code>Send</code>
and <code>Sync</code> if <code>T</code> is <code>Sync</code>.
<code>Mutex</code> implements <code>Send</code> if <code>T</code> is <code>Send</code>
and <code>Sync</code> if <code>T</code> is <code>Send</code>.
The variance rule for <code>Sync</code> and <code>Send</code> trait is tricky and hard to reason about.
When you are in doubt,
check the closest type in Rust standard library and follow the design.</p>
  </div>



<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Explain your mutex design (mutex-design)</p>
<p>Why did you choose such ordering requirement?
Why does this design guarantee the soundness?
Explain with brevity.</p>
  </div>



<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>State transition of Mutex (mutex-bad-state)</p>
<p>What can go wrong if a thread calls lock(), initialize MMU, and unlock()?
If you think this should be prevented,
describe why it can be a problem and how to prevent it.
You may add additional checks in <code>VMManager::wait()</code>.
If you think it is okay to allow such behavior,
justify your thought.
Either answer can be correct depends on how you implemented mutex.</p>
  </div>

<h3 id="subphase-c-multicore-scheduling">Subphase C: Multicore Scheduling</h3>
<p>Right now, only core 0 is scheduling user programs.
In this subphase, you will make other cores participate in scheduling.
As a result, our kernel will have around four times more throughput
to run processes compared to the single-core version.</p>
<h4 id="per-core-irq-handling">Per-Core IRQ Handling</h4>
<p>In this section, you&rsquo;ll enable per-core IRQ handling.
We have used timer register at <code>IO_BASE + 0x3000</code>,
but the interrupt from this timer only propagates to core 0.
In order to make other cores participate in scheduling,
we&rsquo;ll switch to another timer interrupt named <code>CNTPNSIRQ</code>,
which stands for Counter Physical Non-Secure IRQ.</p>
<p>You&rsquo;ll mainly work in <code>pi/src/local_interrupt.rs</code>.
The overall structure is mostly similar to
<code>pi/src/timer.rs</code> and <code>pi/src/interrupt.rs</code>.
You will also read &ldquo;AArch64 Programmer&rsquo;s Guides: Generic Timer&rdquo; (timer) and
&ldquo;Quad-A7 Control&rdquo; (QA7) as references while working on per-core IRQ handling.
&ldquo;AArch64 Programmer&rsquo;s Guides: Generic Timer&rdquo; describes how generic timer works
in AArch64 architecture,
and &ldquo;Quad-A7 Control&rdquo; describes how to propagate interrupts from generic timer
to each core.</p>
<p>First, fill in the definition of <code>Registers</code>.
See QA7 chapter 4 for register definition.
You only need to define registers up to &ldquo;Core3 FIQ Source&rdquo;.
Then, fill the definition of <code>LocalInterrupt</code>
and implement <code>From&lt;usize&gt;</code> for <code>LocalInterrupt</code>.
See QA7: 4.10 for the definition.</p>
<p>Next, implement the following functions of <code>LocalController</code>:</p>
<ul>
<li>
<p><code>enable_local_timer()</code></p>
<p>Enable CNTP timer by setting appropriate bits to <code>CNTP_CTL_EL0</code> register
(ref: D7.5.10).
Now enable per-core CNTPNS IRQ by writing values to Core X timers interrupt
control register (QA7: 4.6).
Use register definition in <code>lib/aarch64/src/regs.rs</code> whenever possible
instead of writing an inline assembly.</p>
</li>
<li>
<p><code>is_pending()</code></p>
<p>Read corresponding bits from Core X interrupt source register (QA7: 4.10)
and convert it to a boolean value.</p>
</li>
<li>
<p><code>tick_in()</code></p>
<p>Finally, implement <code>tick_in()</code> method for generic timer as we did in lab 4.
See timer: 3.1 to 3.3 to determine which register you should use.
You&rsquo;ll need to convert <code>Duration</code> to counter tick value
using the frequency of the timer.
Again, prefer using register definition in <code>lib/aarch64/src/regs.rs</code> over
writing an inline assembly.</p>
</li>
</ul>


<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>How de we clear CNTPNS IRQ bit? (cntpns-clear)</p>
<p>When local timer passes the specified time, CNTPNS IRQ bit is set and the
core will receive an IRQ interrupt.
When using the global timer, we wrote to CS register to clear the interrupt bit.
Then, how do we clear CNTPNS IRQ bit?</p>
  </div>

<h4 id="fixing-the-scheduler">Fixing the Scheduler</h4>
<p>The next step is to use this per-core timer interupt in the scheduler to
enable multicore scheduling.
Follow these instructions step by step:</p>
<ul>
<li>
<p><strong>Make the scheduler use per-core local timer interrupt</strong></p>
<p>Open <code>kern/src/scheduler.rs</code>.
You should have <code>initialize_global_timer_interrupt()</code> in
<code>GlobalScheduler::start()</code> if you followed the merge guideline
at the beginning of the lab.
Add the core number check around <code>initialize_global_timer_interrupt()</code>,
so that only core 0 invokes that function.
Then, call <code>initialize_local_timer_interrupt()</code> in the next line
(for all cores).</p>
<p>Next, erase what you have in <code>initialize_global_timer_interrupt</code>
and implement the same logic in <code>initialize_local_timer_interrupt</code>
with local interrupt controller.
Leave the content of <code>initialize_global_timer_interrupt</code> empty for now,
but do not erase the call to this function in <code>start()</code>.
We will use this function in the later part of the lab.</p>
</li>
<li>
<p><strong>Support local timer interrupt handling</strong></p>
<p>Implement <code>Index&lt;LocalInterrupt&gt;</code> for <code>LocalIrq</code> in <code>kern/src/traps/irq.rs</code>.
Then, add local timer interrupt handling logic in <code>kern/src/traps.rs</code>.
Global interrupt should be handled only by core 0,
and all cores should handle their local interrupts.</p>
</li>
<li>
<p><strong>Fix the scheduler logic for multicore environment</strong></p>
<p>Switching to multicore environment breaks a few characteristics of the scheduler.</p>
<ul>
<li>
<p>It is no more guaranteed that the first process in the queue matches
the current running process on the core.
If your scheduler logic relies on this assumption, fix that now.</p>
</li>
<li>
<p>It is not guaranteed that the process found with <code>switch_to()</code>
call in <code>start()</code> is in its initial state.
This can happen if the number of process is smaller than the number of cores.
In result, we can no longer assume that all registers are zero in the process&rsquo;s context,
and it is invalid to overwrite any register used by the user process
after returning from <code>context_restore</code>.</p>
<p>There are many ways to fix this.
One naive way is giving up the stack adjustment and set <code>SP</code>
to the address of the copied trap frame in the kernel stack.
This solution wastes about 300 bytes plus the size of the trap frame.
Another way to fix it is to copy the content of <code>tf</code> to the top of the kernel stack,
and rewind <code>SP</code> back before calling <code>context_restore</code>.
This involves more unsafe code, but it only wastes the size of the trap frame.
You can also save the value of general registers to the memory after
<code>context_restore</code>, use them as temporary regsiters and adjust <code>SP</code>,
and restore them back before <code>eret</code>.</p>
</li>
<li>
<p>You may need to insert <code>sev()</code> in <code>schedule_out()</code> if you loop
with <code>wfe()</code> in <code>switch_to()</code> to notify other cores waiting.</p>
</li>
</ul>
</li>
<li>
<p><strong>Make other cores participate in scheduling</strong></p>
<p>The last step is to replace the infinite loop at the end of <code>kmain2()</code>
in <code>kern/src/init.rs</code>
with <code>SCHEDULER::start()</code>.</p>
</li>
</ul>
<p>When you are finished, you should see that four cores are participating in scheduling.
Print a <code>trace!()</code> log in local IRQ handling routine and verify that all four
cores are participating in the scheduling.
Recall that you need to run <code>VERBOSE_BUILD=1 make</code> to enable trace-level log.
If everything works correctly, proceed to the next phase.</p>


<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Multicore performance experiment (multicore-performance)</p>
<p>Populate 4 fib processes in the scheduler.
Run them with and without multicore, and record the running time.
How much time did it take on a single core,
and how much time did it take on four cores?
Was it exactly four times faster on four cores?
If not, what will be the overhead?</p>
  </div>

<h2 id="phase-2-tcp-networking">Phase 2: TCP Networking</h2>
<p>In this phase, you&rsquo;ll add TCP networking capability to our kernel
by implementing a network driver and related system calls.
In subphase A, you will learn how TCP over Ethernet works.
Then, in subphase B,
You will integrate an existing Ethernet implementation for RPi (USPi)
and a minimal TCP stack (smoltcp) to our kernel.
In subphase C,
you will adjust process and scheduler so that sockets are properly managed
as a process resource.
Finally, in subphase D,
you will implement several socket related system calls
to expose the network driver to a user program.</p>
<h3 id="subphase-a-networking-101">Subphase A: Networking 101</h3>
<p>In this subphase, you&rsquo;ll learn the concepts of computer networking.
We will cover the basics of computer networking, network layer, how a packet
routes through computer networks, and socket abstraction provided by OS.</p>
<h4 id="network-layer-model">Network Layer Model</h4>
<p>Networking is a way to communcate with other computers.
There are many models that describe the network structure,
but the common concept among them is that they all model network as a layered structure.
In this lab, we will use TCP/IP&rsquo;s 4 layer model.
From the lowest to the highest, 4 layers of TCP/IP model are:</p>
<ul>
<li>Link Layer (Ethernet, IEEE 802.11)</li>
<li>Internet Layer (IP)</li>
<li>Transport Layer (TCP, UDP)</li>
<li>Application Layer (FTP, HTTP, Plain Text)</li>
</ul>
<p><strong>The link layer</strong> provides a way to exchange packets in local (i.e., directly connected) network,
such as computers connected to the same switch.
<em>Ethernet</em> and <em>IEEE 802.11 (Wi-Fi)</em> are two popular link layer protocol.
Typically, a <em>Media Access Control (MAC)</em> address is used as an identifier for link layer protocols.
TCP/IP is designed to be link-layer independent,
and there even exists a standard which uses
<a href="https://en.wikipedia.org/wiki/IP_over_Avian_Carriers"  target="_blank" rel="noreferrer nofollow">homing birds as a link layer</a>
.</p>
<p><strong>The internet layer</strong> defines an addressing system to identify a host
and how packets should be routed to the destination.
<em>Internet protocol version 4 (IPv4)</em> is the most widely used address system.
Under IPv4 scheme, an endpoint is identified by an address of four octets,
which is often denoted as four decimal numbers separated by dots,
such as 127.0.0.1.
Internet protocol only provides <a href="https://en.wikipedia.org/wiki/Best-effort_delivery"  target="_blank" rel="noreferrer nofollow">best-effort delivery</a>
;
it does not guarantee that a packet will actually reach the destination.
This concern is handled at the transport layer level.</p>
<p><strong>The transport layer</strong> provides an abstraction of data channels,
so that multiple connections can be established between two host computers.
The two most famous transport layer protocols are
<em>Transmission Control Protocol (TCP)</em> and <em>User Datagram Protocol (UDP)</em>.
They use <em>ports</em>, a 16-bit integer, as an endpoint identifier.
TCP provides reliable connection between two endpoints with packet delivery
acknowledgement, retransmission for error recovery, and congestion control.
On the other hand, UDP is a connectionless and lossy protocol that is much
lighter and faster.</p>
<p>Finally, <strong>the application layer</strong> serves various application level protocols.
For instance, <em>HyperText Transfer Protocol (HTTP)</em> for web content,
<em>File Transfer Protocol (FTP)</em> for file sharing,
or <em>Transport Layer Security (TLS)</em> for encrypted communcations.
In our lab, we will send and receive plain text messages over TCP/IP stack,
using an Ethernet cable.</p>
<h4 id="packet-routing">Packet Routing</h4>
<p>A route table is a data table managed by an operating system to determine
which interface and gateway should be used to reach an IP address.
On Ubuntu 18.04, a route table can be printed with <code>route</code> command
(if the command is not available,
install <code>net-tools</code> package by running <code>sudo apt install net-tools</code>).</p>
<p>An example route table looks like this:</p>
<pre tabindex="0"><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         192.168.0.1     0.0.0.0         UG    0      0        0 enp11s0f1
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 enp11s0f1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
</code></pre><p>An entry in a route table contains destination, gateway, subnet mask (genmask),
and interface ID (Iface).
Given an IP address, the operating system compares it with each entry in the
route table to determine where that packet should be delivered (or forwarded).
If the packet&rsquo;s target IP address masked by the subnet mask matches the
destination IP of an entry, that packet goes through the specified interface.
If the packet&rsquo;s target IP does not match any entry, it will be delivered to the
default gateway, which is 192.168.0.1 in our example.</p>
<p>A pair of destination address and a subnet mask is called <em>Classless
Internet-Domain Routing (CIDR)</em> block.
A subnet mask splits 32-bit address into two sections,
network address and host address.
The network address is a fixed part and represented as bit 1 in the subnet mask,
and the host address is a variable part and represented as bit 0 in the subnet
mask.
A subnet mask always consists of n bits of 1 followed by 32-n bits of 0,
and it is often shorthanded as &ldquo;/(# of 1 bit in the subnet mask&rdquo;
after the IP address.
For instance, destination IP 192.168.0.0 with subnet mask 255.255.255.0
is written as 192.168.0.0/24,
and destination IP 172.17.0.0 with subnet mask 255.255.0.0
is written as 172.17.0.0/16.</p>
<p>If an entry does not define a gateway, then a host with that IP can be found in
the local network connected through the interface.
For instance, since 192.168.0.4 matches 192.168.0.0/24, a host with IP
192.168.0.4 can be found in the local network connected to <code>enp11s0f1</code>.
If an entry defines a gateway, then a packet will be forwarded to that IP
first in order to reach the target address.
For instance, a packet with target IP 1.1.1.1 will be first forwarded to the
default gateway 192.168.0.1 to reach the destination.
Then, a machine at 192.168.0.1 (a router) routes the packet to the next host
according to its own routing table.</p>
<figure><img src="./res/ARP.png"><figcaption>
      <h4>When used with IPv4 and Ethernet, hardware address in ARP packet is MAC address and protocol address in ARP packet is IPv4 address.</h4>
    </figcaption>
</figure>

<p>Recall that MAC address is used as an address scheme
when delivering packets through Ethernet link layer.
<em>Address Resolution Protocol (ARP)</em> is used to discover the association of
the IPv4 address and the MAC address of local neighbors.
ARP packet consists of IP and MAC address pair, and each computer in the local
network saves (IP, MAC) pair in its neighbor cache for future use.</p>
<h4 id="socket">Socket</h4>
<p>Just like MAC is a link layer address and IP is a internet layer address,
a port can be thought as an address for transport layer.
An IP address is used to select a computer,
and a port is used to select specific process running on that computer.
Combined,
a pair of IP address and a port forms a transport layer endpoint.
Transport layer protocols such as TCP and UDP connects two different endpoints.</p>
<figure><img src="./res/packet-hierarchy.gif"><figcaption>
      <h4>A diagram of network layer hierarchy from &#34;TCP/IP Illustrated, Vol. 1: The Protocols&#34;</h4>
    </figcaption>
</figure>

<p>Most of user applications work at the application layer level.
They need to select an endpoint to connect (or listen on) and translation layer
protocol (TCP/UDP) to use when initializing the connection,
but after that, they only send and receive application data.
The operating system and the network driver take cares of how to deliver an
actual packet generated from user programs,
and during the process, they wrap and unwrap a packet with various protocol
headers.</p>
<p>Therefore, an operating system needs to manage the list of active connections
and remember which process is using which connections.
A socket is a system resource that abstracts this behavior.
User programs open, interact with, and close sockets with system calls;
They use a socket descriptor, a handle for a socket, to address a specific
socket.</p>
<p>Unix operating system uses
<a href="https://en.wikipedia.org/wiki/Berkeley_sockets"  target="_blank" rel="noreferrer nofollow">Berkeley Socket API</a>
.
A few important socket APIs of Berkeley Socket APIs are as follows.
These are descriptions of each API from Wikipedia:</p>
<ul>
<li><code>socket()</code> creates a new socket of a certain type, identified by an integer number, and allocates system resources to it.</li>
<li><code>bind()</code> is typically used on the server side, and associates a socket with a socket address structure, i.e. a specified local IP address and a port number.</li>
<li><code>listen()</code> is used on the server side, and causes a bound TCP socket to enter listening state.</li>
<li><code>connect()</code> is used on the client side, and assigns a free local port number to a socket. In case of a TCP socket, it causes an attempt to establish a new TCP connection.</li>
<li><code>accept()</code> is used on the server side. It accepts a received incoming attempt to create a new TCP connection from the remote client, and creates a new socket associated with the socket address pair of this connection.</li>
<li><code>send()</code>, <code>recv()</code>, <code>sendto()</code>, and <code>recvfrom()</code> are used for sending and receiving data.</li>
<li><code>close()</code> causes the system to release resources allocated to a socket. In case of TCP, the connection is terminated.</li>
</ul>
<p>In our lab, we will use a TCP/IP stack written in Rust
named <a href="https://github.com/smoltcp-rs/smoltcp"  target="_blank" rel="noreferrer nofollow">smoltcp</a>
.
Since <code>smoltcp</code>&rsquo;s API set provides a little different functionality compared
to Berkeley Socket API,
we will just use <code>smoltcp</code>&rsquo;s API.
These are notable differences compared to Berkeley Socket API:</p>
<ul>
<li>Instead of <code>bind(Local Addr)</code> and <code>listen()</code>,
it uses single <code>listen(Local Addr)</code> syscall.</li>
<li>Instead of using a blocking syscall <code>accept()</code> which waits for a new
connection and creates a new socket,
a server socket will automatically get connected when Ethernet device is
polled.
To accept a new connection,
a new socket need to be created.</li>
<li><code>send()</code> and <code>recv()</code> are non-blocking by default.
<code>send()</code> will queue the message to socket&rsquo;s buffer and return immediately,
and <code>recv()</code> will return immediately even if there&rsquo;s no message.</li>
</ul>
<h3 id="subphase-b-network-driver">Subphase B: Network Driver</h3>
<p>In this subphase, you&rsquo;ll add a network driver to our kernel.
TCP/IP specification is such a complex and huge protocol with numerous extensions,
so instead of writing the network stack from scratch,
we are going to use existing implementations and focus on their integration to OS.
Namely, we will use USPi as an Ethernet driver and smoltcp as a network stack.</p>
<h4 id="integrating-uspi">Integrating USPi</h4>
<p><a href="https://github.com/rsta2/uspi"  target="_blank" rel="noreferrer nofollow">USPi</a>
 is a bare metal USB driver for Raspberry Pi written in C.
We are using it to communicate with Raspberry Pi&rsquo;s Ethernet contoller through USB protocol.
USPi consists of two parts, <code>env</code> and <code>lib</code>.
<code>lib</code> is the main part of the library, and it expects a few API from the
environment, such as memory allocation, timer, interrpt handling, and logging.
<code>env</code> is a minimal kernel that exposes these functions,
but instead of using <code>env</code>, we are going to replace it with our RustOS kernel.</p>
<p><code>kern/src/net/uspi.rs</code> is responsible for the integration of USPi and RustOS.
Functions inside <code>extern &quot;C&quot;</code> block are exposed from USPi to Rust, and
functions with <code>#[no_mangle]</code> annotation are exposed from Rust to USPi.
Most of the provided functions are self-explanatory.
Go ahead and read the file now.
The code structure follows the usual design;
<code>USPi</code> struct implements non-thread safe version to interact with USPi and
<code>Usb</code> struct wraps <code>USPi</code> under a Mutex so that it can be accessed by
multiple cores at the same time.</p>
<p>Two things noteworthy in this module are <code>uspi_trace!()</code> macro and <code>start_kernel_timer()</code>.
Recall that <code>VERBOSE_BUILD</code> environment variable controls whether trace-level
logs are printed or not. <code>uspi_trace!()</code> wraps <code>trace!()</code> macro and provides
another knob to control, namely <code>DEBUG_USPI</code>. Set it to true when you are
working on USPi related functions and turn it off when you are done.
<code>start_kernel_timer()</code> uses USPi&rsquo;s <code>TimerStartKernelTimer()</code> to register
timer callback functions. The logic is very similar to the timer IRQ handling
of our kernel, but it implements software timer so that multiple callback
functions with different delays can be registered under the same interrupt.</p>
<p>USB and Timer3 interrupts need to be handled in the kernel to support USPi.
Moreover, USB interrupt needs to be handled in the kernel context.
Which means, our kernel needs <em>nested handling</em> of USB interrupt.
Instead of allowing general nested interrupt handling,
we are going to handle USB interrupt as FIQ and allow nested FIQ interrupt in
a few limited code places.</p>
<h4 id="network-driver">Network Driver</h4>
<p>We will use <a href="https://github.com/smoltcp-rs/smoltcp"  target="_blank" rel="noreferrer nofollow">smoltcp</a>
,
a minimal TCP/IP stack for bare-metal systems written in Rust,
as a network stack for our kernel.
Three important traits / structs in smoltcp are
<a href="https://docs.rs/smoltcp/0.6.0/smoltcp/socket/struct.SocketSet.html"  target="_blank" rel="noreferrer nofollow">SocketSet</a>
,
<a href="https://docs.rs/smoltcp/0.6.0/smoltcp/phy/trait.Device.html"  target="_blank" rel="noreferrer nofollow">Device</a>
, and
<a href="https://docs.rs/smoltcp/0.6.0/smoltcp/iface/struct.EthernetInterface.html"  target="_blank" rel="noreferrer nofollow">EthernetInterface</a>
.</p>
<ol>
<li><code>SocketSet</code> manages a set of sockets.
Each socket in a socket set manages its own Rx and Tx buffer,
and writing to and reading from a socket only accesses these buffers.</li>
<li>A type that implements <code>Device</code> trait represents a physical hardware device
that is capable of send and receive a raw network packet.
In our case, USPi library will be used to send and receive raw physical
packets.</li>
<li><code>EthernetInterface</code> internally manages <code>Device</code> and uses it to send and
receive packets. <code>EthernetInterface</code> can be <code>poll()</code>ed with a socket
set. When polled, pending data in Tx buffer of sockets will be sent and
received data will be buffered to Rx buffer of sockets.</li>
</ol>
<p><code>kern/src/net.rs</code> integrates smoltcp library with our kernel design.
Read the code of following structs to understand how this integration is done:</p>
<p><code>FrameBuf</code> and <code>Frame</code>:
Fixed size, 8-byte aligned, and length trackable <code>u8</code> buffer.</p>
<p><code>EthernetDriver</code> and <code>GlobalEthernetDriver</code>:
A thread-unsafe Ethernet driver struct and its corresponding struct wrapped
in a mutex.</p>
<p><code>UsbEthernet</code>, <code>RxToken</code>, and <code>TxToken</code>:
<code>UsbEthernet</code> implements <code>smoltcp::phy::Device</code> trait,
which is an interface for sending and receiving raw network frames.
<code>RxToken</code> and <code>TxToken</code> structs implement <code>smoltcp::phy::RxToken</code> and
<code>smoltcp::phy::TxToken</code>, which are traits that define how a single network
packet should be sent and received.</p>
<p>When administrating a network stack, an important role of an operating system is
managing port numbers on the system.
Typically, an operating system tracks which processes are using which port
numbers and prevents other processes to interfere with the connection.
To support this behavior, you need to implement port management functions in
<code>kern/src/net.rs</code> and use them properly in the process and scheduler code.</p>
<h4 id="implementation-2">Implementation</h4>
<p>You&rsquo;re now ready to implement a network driver to our kernel.
We recommend you to implement features in the following order:</p>
<ol>
<li>
<p><strong>Finish USPi environment integration in</strong> <code>kern/src/net/uspi.rs</code></p>
<p>First, implement logging functions, <code>DoLogWrite()</code> and
<code>uspi_assertion_failed()</code>. When implementing <code>DoLogWrite()</code>, ignore
<code>_pSource</code> and <code>_Severity</code>, translate <code>pMessage</code> as null-terminated
C-style string, and print it with <code>uspi_trace!()</code> macro.
When implementing <code>uspi_assertion_failed()</code>,
convert <code>pExpr</code> and <code>pFile</code> to Rust string similarly and print them
with line number <code>nLine</code>.
Both functions should not panic.</p>
<p>Then, implement four timer functions,
<code>TimerSimpleMsDelay()</code>, <code>TimerSimpleusDelay()</code>, <code>MsDelay()</code>, and
<code>usDelay()</code>.
Convert milliseconds and microseconds provided as a parameter
to Rust&rsquo;s <code>Duration</code> and use <code>pi::timer::spin_sleep()</code>.</p>
<p>Finally, implement <code>malloc()</code> and <code>free()</code>.
These are C-style allocation API, which means that they do not provide
layout parameter like Rust allocation API.
Carefully read the definition of <code>malloc()</code> and <code>free()</code> and think about
what information need to be tracked in addition to what has been provided
as their parameters.
Allocate additional space in a chunk to save those information.
Make sure all allocated pointers are 16-byte aligned.</p>
</li>
<li>
<p><strong>Enable FIQ interrupt handling</strong></p>
<p>USPi requires handling of USB and Timer3 interrupts.
Moreover, it requires to handle USB interrupt in the kernel context.
We are going to handle USB interrupt as FIQ interrupt instead of normal IRQ
interrupt and allow nested FIQ interrupt handling in small number of places
to support this behavior.</p>
<p>Start by implementing <code>enable_fiq()</code> in <code>pi/src/interrupt.rs</code>.
You&rsquo;ll need to revisit chapter 7 of the
<a href="../files/BCM2837-ARM-Peripherals.pdf" >BCM2837 ARM Peripherals Manual</a>

to see how an interupt can be selected as a FIQ interrupt.
Then, add FIQ handling routine to <code>handle_exception()</code> in <code>kern/src/traps.rs</code>
and implement <code>Index&lt;()&gt;</code> for <code>Fiq</code> in <code>kern/src/traps/irq.rs</code>.</p>


<div class="box box-shortcode tip" >
    <span class="icon-box baseline">
      <svg><use href="#tip-box"></use></svg>
    </span>
    <p>You may want to check FIQ control register when writing <code>enable_fiq()</code>.</p>
  </div>

<p>Once you are finished,
complete <code>ConnectInterrupt()</code> function in <code>kern/src/net/uspi.rs</code>.
First, assert that <code>nIRQ</code> is one of <code>Interrupt::Usb</code> or <code>Interrupt::Timer3</code>.
Then, if the request is for USB interrupt, enable FIQ handling of USB
interrupt and register a handler that invokes provided <code>pHandler</code>
to the FIQ handler registry (<code>crate::FIQ</code>).
Otherwise, enable IRQ handling of Timer3 interrupt
and register a handler that invokes provided <code>pHandler</code>
to the global IRQ handler registry (<code>crate::GLOBAL_IRQ</code>).</p>
<p>Recall that all exception flags are masked by default when an exception
handler is called.
To handle FIQ interrupt, F flag of PSTATE register should be unmasked.
<code>enable_fiq_interrupt()</code> and <code>disable_fiq_interrupt()</code> in <code>aarch64</code>
library give an ability to temporarily enable FIQ interrupt.
Enable FIQ handling in following places with these functions:</p>
<ol>
<li>When handling system calls (<code>kern/src/traps.rs</code>)</li>
<li>When handling IRQ interrupts (<code>kern/src/traps.rs</code>)</li>
<li>While waiting on the very first <code>switch_to</code> call in <code>start()</code>
(<code>kern/src/process/scheduler.rs</code>)</li>
</ol>
</li>
<li>
<p><strong>Implement Ethernet initialization</strong></p>
<p>Finish <code>create_interface()</code> in <code>kern/src/net.rs</code>.
You should use smoltcp&rsquo;s
<a href="https://docs.rs/smoltcp/0.6.0/smoltcp/iface/struct.EthernetInterfaceBuilder.html"  target="_blank" rel="noreferrer nofollow">EthernetInterfaceBuilder</a>
.
When creating the interface,
use <code>UsbEthernet</code> as an inner physical device and
MAC address obtained from USPi as Ethernet address of the interface.
Then, add an empty neighbor cache using <code>BTreeMap</code>.
Finally, add two CIDR blocks as its IP addresses:
169.254.32.10/16 and 127.0.0.1/8.
When you are done,
implement <code>EthernetDriver::new()</code> using <code>create_interface()</code>.</p>


<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>Link-local Address<br>
169.254.0.0/16 is a reserved address space for local communication.
If two computers are directly connected with an Ethernet cable,
two different link-local addresses will be assigned to each of them.
In our case, we are assigning a fixed link-local address of
169.254.32.10 to the kernel&rsquo;s network interface.</p>
  </div>



<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>How to setup with router<br>
If you have a router, you can connect RPi to the router instead of
connecting it directly to the computer.<br>
1.  Record the MAC address of your RPi board.<br>
2.  In router management page (check your router manual to learn how to access it),
manually assign an IP to the MAC address of your RPi.<br>
3.  In the network driver, manually assign the IP you chose.<br>
The IP address range managed by your router should be in
one of these three private network address range.<br>
- 192.168.0.0 - 192.168.255.255<br>
- 172.16.0.0 - 172.31.255.255<br>
- 10.0.0.0 - 10.255.255.255</p>
  </div>

<p>The next step is to initialize USB and ETHERNET in <code>kmain()</code>.
Write the following initialization routine after the scheduler initialization:</p>
<ol>
<li>Enable FIQ interrupt</li>
<li>Initialize <code>USB</code></li>
<li>Initialize <code>ETHERNET</code></li>
<li>Assert if the Ethernet is available with <code>is_eth_available()</code></li>
<li>Poll the Ethernet with <code>is_eth_link_up()</code> and loop until it returns true</li>
<li>Disable FIQ interrupt</li>
</ol>
</li>
<li>
<p><strong>Implement port management APIs</strong></p>
<p>Next, write port management APIs in <code>EthernetDriver</code>.
There are 65535 ports in our kernel, from 1 to 65535.
We will manage port availability with
<code>port_map</code> field of <code>EthernetDriver</code>,
which is an arrray of <code>u64</code>.
One <code>u64</code> integer variable has 64 bits,
and you&rsquo;ll use each bit in as an indicator of whether a port is
available or not.</p>


<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>Port permission<br>
A port number under 1024 are reserved in Linux operating system,
and you need root permission to listen on those ports.
Since we don&rsquo;t have &ldquo;user&rdquo; concept in RustOS,
we will not enforce this limitation.</p>
  </div>

<p>Now write <code>mark_port()</code>, <code>erase_port()</code>, and <code>get_ephemeral_port()</code> in <code>kern/src/net.rs</code>.
See the comments of the functions and follow the instruction.</p>
</li>
<li>
<p><strong>Implement Ethernet polling</strong></p>
<p>The next step is to implement Ethernet interface polling.
When Ethernet interface is polled with a <code>SocketSet</code>,
it should send pending Tx packets in the socket buffer
and queue pending Rx packets in the Ethernet to corresponding socket buffer.</p>
<p><code>smoltcp</code> already implements this logic.
Use them to implement <code>EthernetDriver::poll()</code> and <code>EthernetDriver::poll_delay()</code>
in <code>kern/src/net.rs</code>.
You may want to check <a href="https://docs.rs/smoltcp/0.6.0/smoltcp/iface/struct.EthernetInterface.html"  target="_blank" rel="noreferrer nofollow">EthernetInterface</a>

documentation.
You also need to implement <code>GlobalEthernetDriver::poll()</code>,
which is a wrapper function to <code>EthernetDriver:poll()</code>.
<code>GlobalEthernetDriver::poll()</code> should be only executed by core 0 in Timer3 handling context.
Check these assumptions in addition to the usual wrapper function semantics.</p>


<div class="box box-shortcode tip" >
    <span class="icon-box baseline">
      <svg><use href="#tip-box"></use></svg>
    </span>
    <p>When implementing the check in <code>GlobalEthernetDriver::poll()</code>,
you will need to check the core affinity and the preemptive counter.</p>
  </div>

</li>
<li>
<p><strong>Register Ethernet polling callback</strong></p>
<p>Ethernet driver should be polled regularly to provide continuous network
connectivity. Since we have already dedicated Timer1 interrupt to the
scheduler tick, we are going to use <code>Usb::start_kernel_timer()</code> for
Ethernet polling.</p>
<p>First, implement <code>poll_ethernet()</code> function in <code>kern/src/process/scheduler.rs</code>.
Poll the Ethernet interface with <code>GlobalEthernetDriver::poll()</code>,
calculate the next poll time with <code>poll_delay()</code>,
and register the timeout with <code>Usb::start_kernel_timer()</code>.
Once you are finished, register the first timeout in
<code>initialize_global_timer_interrupt()</code> with <code>Usb::start_kernel_timer()</code>.</p>
</li>
</ol>
<p>When you are done, make sure that USPi and Ethernet initialization in <code>kmain()</code>
works correctly, Ethernet interface is polled repetitively, and existing user
programs still work.
If everything works as expected, continue to the next subphase.</p>
<h3 id="subphase-c-process-resource-management">Subphase C: Process Resource Management</h3>
<p>In this subphase, you&rsquo;ll add process resource management to our kernel.
An operating system kernel needs to track which sockets and files are used by
which processes, so that they can be opened, updated, and closed according to
processes&rsquo;s requests and their lifetime.
In Unix operating systems,
actual sockets or files (inode) implementation reside in the kernel,
and they are exposed to user programs as a descriptor.
The process makes a syscall using a descriptor,
which is an index to the list of resources used by the current process.
For instance, if a process opens a file with <code>open()</code> syscall,
it returns &ldquo;3&rdquo; and the process uses &ldquo;3&rdquo; to refer that file henceforth.
(0, 1, 2 are reserved for standard I/O).
We will follow this design when implementing process resource management code.</p>


<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Socket descriptor design (descriptor-design)</p>
<p>What will be the benefit and the cost of using the index as a descriptor?
What will be better or worse if a random token is used instead of an index?</p>
  </div>

<h4 id="implementation-3">Implementation</h4>
<p>Start with adding <code>sockets</code> field of type <code>Vec&lt;SocketHandle&gt;</code> to <code>Process</code>
struct in <code>kern/src/process/process.rs</code>. Fix <code>new()</code> appropriately.
Then, implement <code>Scheduler::release_process_resources()</code> in <code>kern/src/process/scheduler.rs</code>.
Iterate through all sockets held by the current process,
close all local ports attached to them,
release the socket handle,
and prune the Ethernet socket set.
Don&rsquo;t forget to call <code>Scheduler::release_process_resources()</code> in <code>Scheduler::kill()</code>.</p>


<div class="box box-shortcode tip" >
    <span class="icon-box baseline">
      <svg><use href="#tip-box"></use></svg>
    </span>
    <p>Use <code>mem::replace</code> to swap out <code>process.sockets</code>.</p>
  </div>



<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>There are two handles in our network stack</p>
<p>smoltcp uses <code>SocketHandle</code> to refer a socket in <code>SocketSet</code>.
Our kernel uses a socket descriptor to refer a <code>SocketHandle</code> held by a process.
Be careful not to confuse these two handles.</p>
  </div>

<h3 id="subphase-d-socket-system-calls">Subphase D: Socket System Calls</h3>
<p>In this subphase, you will add a number of system calls to our kernel to allow
user programs to use the network functionality.
You&rsquo;ll implement total 6 socket syscalls.
Here is a short summary of them:</p>
<ul>
<li><code>sock_create()</code>: creates a new socket and registers it as current process&rsquo;s resource</li>
<li><code>sock_status()</code>: checks the status of the socket</li>
<li><code>sock_connect()</code>: connects to a remote endpoint with the socket</li>
<li><code>sock_listen()</code>: listens on a local endpoint with the socket</li>
<li><code>sock_send()</code>: sends a packet with a connected socket</li>
<li><code>sock_recv()</code>: receives a packet from a connected socket</li>
</ul>
<h4 id="implementation-4">Implementation</h4>
<p>Start writing socket related system call handlers in <code>kern/src/traps/syscall.rs</code>.
Refer to the comments to understand what those functions do.
Except <code>create()</code>, they will have the similar code structure:</p>
<ol>
<li>Find a socket handle held by the current process</li>
<li>Find a socket from Ethernet socket set using the handle</li>
<li>Perform an operation with socket (such as <code>status()</code>, <code>connect()</code>, etc.)</li>
<li>Update the scheduler and Ethernet driver accordingly;
For example, you should mark the local port number as used when <code>listen()</code>
or <code>connect()</code>.</li>
<li>Convert the result according to the system call semantics.</li>
</ol>
<p>You will need to revisit scheduler and Ethernet driver code to find necessary
APIs for this subphase. Also consult <a href="https://docs.rs/smoltcp/0.6.0/smoltcp/socket/struct.TcpSocket.html"  target="_blank" rel="noreferrer nofollow">TcpSocket</a>

page from smoltcp document to find useful functions.</p>


<div class="box box-shortcode warning" >
    <span class="icon-box baseline">
      <svg><use href="#warning-box"></use></svg>
    </span>
    <p>User address validation</p>
<p>When reading from or writing to an address from users,
you must ensure that the contents are really inside the user address space.
Passing a kernel address through syscalls and trick the kernel to overwrite
the kernel memory is a classic attack against operating system kernels.</p>
  </div>



<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>There is no <code>close()</code> syscall</p>
<p>We did not include <code>close()</code> syscall as a requirement,
but feel free to implement it if you want to.</p>
  </div>



<div class="box box-shortcode info" >
    <span class="icon-box baseline">
      <svg><use href="#info-box"></use></svg>
    </span>
    <p>smoltcp socket APIs are non-blocking</p>
<p>Unlike Berkeley socket API, our socket system calls are non-blocking
by default.
This is because we are directly wrapping smoltcp APIs as a syscall.
In result, instead of getting notified by the kernel when a new connection
is established, user programs need to check the socket status in a loop to
see if a connection has been established.</p>
  </div>



<div class="box box-shortcode question" >
    <span class="icon-box baseline">
      <svg><use href="#question-box"></use></svg>
    </span>
    <p>Implement Berkeley-like socket APIs (berkeley-socket)</p>
<p>How can you implement Berkeley-like socket APIs in our kernel design?
For instance, let&rsquo;s say a user program invokes <code>sock_listen()</code> syscall.
How can the kernel suspend a user program until a new connection is established?</p>
  </div>

<p>When you finish implementing socket related system call handlers in the kernel,
write their corresponding user-side system calls in <code>kernel_api/src/syscalls.rs</code>.
When implementing them,
check the comments in their kernel side handlers
for syscall argument and return value specification.
Convert wrapper types (e.g., <code>SocketDescriptor</code>, <code>IpAddr</code>, etc.)
to <code>u64</code> and back while implementing them,
so that user programs can invoke system calls with more natural types.</p>
<h2 id="phase-3-echo-server">Phase 3: Echo Server</h2>
<p>The last phase of our long journey is to implement a simple user program that
tests the network stack.
In this phase, you&rsquo;ll write an echo server, a server that sends back any message
that it reads from the client.
It is one of the simplest network application.
You will mainly work in <code>user/echo/src/main.rs</code>.</p>
<h3 id="implementation-5">Implementation</h3>
<p>We have provided a very basic error handling code in <code>main()</code>.
Your role is to implement an echo server in <code>main_inner()</code>.</p>
<ol>
<li>First, create a socket with <code>sock_create()</code>.</li>
<li>Then make that socket listen on port number 80.</li>
<li>Loop until a packet can be sent with the socket.
You need to check <code>can_send</code> field in the socket status.
Print a message in the loop and sleep for a short period of time (say, 1 sec).</li>
<li>Send a welcome message to the client.</li>
<li>Inside another loop, receive a packet and send it back through the socket.
Also print the message to the console with <code>print!()</code>.</li>
</ol>
<p>When you are done implementing the echo server,
add &ldquo;/echo&rdquo; to the scheduler initialization code.</p>
<h3 id="how-to-test">How to Test</h3>
<p>Run <code>make</code> in <code>user/echo</code> to build the program
and copy <code>user/echo/build/echo.bin</code> to the SD card.
Then, <code>make transmit</code> your kernel (in <code>kern</code> directory).
There are two connections to the kernel now:
a serial connection and the Ethernet connection.
If you <code>screen</code> to the serial connection after transmitting the kernel image,
you should see repeated messages which say that the server is waiting for
a client connection.
The next step is to connect to the echo server with netcat (<code>nc</code>) command.</p>


<div class="box box-shortcode warning" >
    <span class="icon-box baseline">
      <svg><use href="#warning-box"></use></svg>
    </span>
    <p>Don&rsquo;t forget to copy your user program to the SD card</p>
<p><code>make transmit</code> only transmits the kernel image.
Don&rsquo;t forget to copy the user programs to the SD card to make your kernel
detect them!</p>
  </div>

<p>If you are using a VM, we recommend you to <code>make transmit</code> in VM
but using netcat on your host operating system.
Using a link local address is a bit uncommon way to communicate these days,
and since the link-local address space is meant to be used only in local setups,
there is a possibility that the network translation layer of a virtual machine
doesn&rsquo;t work well with it.</p>
<p>You may need to install netcat command to your host system.
On Windows, <a href="https://eternallybored.org/misc/netcat/"  target="_blank" rel="noreferrer nofollow">netcat</a>
 seems to work
well. On Linux, you might need to configure your ethernet interface to be
<code>Link-Local Only</code> in the Network Manager or using <code>ifconfig</code>. Netcat will
mostly be pre-installed. If not, you can use <code>sudo apt-get install netcat</code>.
On MacOS, you can install netcat with <code>brew install nmap</code> command. If your RPi
server wakes up, you can see the light for <code>USB 10/100/1000 LAN</code> (or something
similar name) turns to yellow from red in <code>System Preferences</code> - <code>Network</code>
with its IP address.</p>
<p>If everything is ready,
type <code>nc 169.254.32.10 80</code> (or <code>ncat</code> on MacOS)
on your computer to connect to the echo server.
It would take a while for RPi to be recognized by the operating system,
so the first few tries might fail.
When connected, the echo server will print a welcome message,
and anything you type into the shell will go through the Ethernet cable and
come back in a while.</p>
<p><figure><img src="./res/echo-serial.png">
</figure>

<figure><img src="./res/echo-ethernet.png">
</figure>
</p>
<h2 id="submission">Submission</h2>
<p>Once you&rsquo;ve completed the tasks above, you&rsquo;re done and ready to
submit! Congratulations!</p>
<p>You can call <code>make check</code> in <code>tut/5-multicore</code> directory to check
if you&rsquo;ve answered every question.</p>
<p>Once you&rsquo;ve completed the tasks above, you&rsquo;re done and ready to
submit! Ensure you&rsquo;ve committed your changes. Any uncommitted
changes <em>will not</em> be visible to us, thus unconsidered for grading.</p>
<p>When you&rsquo;re ready, push a commit to your GitHub repository with a tag named <code>lab5-done</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#6e738d;font-style:italic"># submit lab5</span>
</span></span><span style="display:flex;"><span>$ git tag lab5-done
</span></span><span style="display:flex;"><span>$ git push --tags
</span></span></code></pre></div></main>




<br>

<footer>

<script defer>
  document.addEventListener("keydown", function (e) {
    if (document.activeElement.isContentEditable) {
      return false;
    }
    if (document.activeElement.tagName == "INPUT") {
      return false;
    }
    if (e.altKey || e.ctrlKey || e.shiftKey) {
      return false;
    }
    var key = e.key;
    if (key === "h") {
      e.preventDefault();
      e.stopPropagation();
      window.location.href = "/";
    } else if (key === "t") {
      e.preventDefault();
      e.stopPropagation();
      window.location.href = `https://${location.hostname}/tags`;
    } else if (key === "i") {
      e.preventDefault();
      e.stopPropagation();
      const inputs = document.querySelectorAll("input");
      for (let i = 0; i < inputs.length; i++) {
        if (inputs[i].offsetParent !== null) {
          inputs[i].selectionStart = inputs[i].selectionEnd =
            inputs[i].value.length;
          inputs[i].focus();
          break;
        }
      }
    }
    return false;
  });
</script>


<script defer>
  function throttle(fn, wait) {
    var time = Date.now();
    return function () {
      var now = Date.now()
      if (time + wait - now < 0) {
        fn();
        time = now;
      }
    };
  }

  function scrollHandler() {
    const anchors = Array.from(document.querySelectorAll("body h2, body h3"));

    function scrollCallback() {
      var scrollTop = window.pageYOffset || document.documentElement.scrollTop;

      
      for (var i = 0; i < anchors.length; i++) {
        var anchorId = anchors[i].getAttribute("id");
        var link = document.querySelector(
          'nav ul li a[href="#' + anchorId + '"]',
        );
        if (link) {
          link.classList.remove("active-toc");
        }
      }

      
      for (var i = anchors.length - 1; i >= 0; i--) {
        var offsetTop = anchors[i].offsetTop;
        if (scrollTop > offsetTop - 75) {
          var anchorId = anchors[i].getAttribute("id");
          var link = document.querySelector(
            'nav ul li a[href="#' + anchorId + '"]',
          );
          if (link) {
            link.classList.add("active-toc");
            break;
          }
        }
      }
    }

    window.addEventListener(
      "scroll",
      throttle(scrollCallback, 300),
    );
  }
  setTimeout(scrollHandler, 100);
</script>

<script defer>
  function addCopyButtonToCodeBlocks() {
    
    const codeBlocks = document.querySelectorAll('code[class^="language-"]');

    codeBlocks.forEach((codeBlock) => {
      const copyButton = document.createElement("button");
      copyButton.classList.add("copy-code-button");
      copyButton.innerHTML = "copy";

      
      copyButton.addEventListener("click", () => {
        
        const elements = codeBlock.querySelectorAll(".cl");
        let codeToCopy = "";
        elements.forEach((element) => {
          codeToCopy += element.innerText;
        });
        navigator.clipboard.writeText(codeToCopy);

        
        copyButton.innerHTML = "copied!";
        setTimeout(() => {
          copyButton.innerHTML = "copy";
        }, 1500);
      });

      
      codeBlock.parentNode.before(copyButton);
    });
  }
  setTimeout(function () {
    addCopyButtonToCodeBlocks();
  }, 100);
</script>

<script>
window.store = {
    
    "\/\/localhost:1313\/post\/": {
        "title": "Posts",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/post\/"
    },
    
    "\/\/localhost:1313\/tags\/lab\/": {
        "title": "Lab",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/lab\/"
    },
    
    "\/\/localhost:1313\/post\/lab5\/": {
        "title": "Lab 5: Multicore and Networking",
        "tags": ["Lab",],
        "content": " Handed out: Tuesday, April 14, 2020 Due: Friday, May 1, 2020 Introduction So far, our kernel has utilized only one core among four cores on a RPi board. In this assignment, you will enable the other three cores and adjust the existing components to correctly run programs in parallel. After that, you will integrate an existing Ethernet implementation for RPi (USPi) and a minimal TCP stack (smoltcp) to our kernel, so that our host computer and the Raspberry Pi board can communicate via an Ethernet cable. Finally, you will write an echo server as a user program on our kernel and interact with that server from your host computer with netcat command.\nPhase 0: Getting Started Fetch the update for lab 5 from our git repository to your development machine.\n$ git fetch skeleton $ git merge skeleton/lab5 This is the directory structure of our repository. The directories you will be working on this assignment are marked with *.\n. ├── bin : common binaries/utilities ├── doc : reference documents ├── ext : external files (e.g., resources for testing) ├── tut : tutorial/practices │ ├── 0-rustlings │ ├── 1-blinky │ ├── 2-shell │ ├── 3-fs │ ├── 4-spawn │ └── 5-multicore : questions for lab5 * ├── boot : bootloader ├── kern : the main os kernel * ├── lib : required libraries │ ├── aarch * │ ├── kernel_api * │ ├── fat32 │ ├── pi * │ ├── shim │ ├── stack-vec │ ├── ttywrite │ ├── volatile │ └── xmodem └── user : user level program * ├── fib ├── sleep └── socket * Merge Guideline You may need to resolve conflicts before continuing. For example, if you see a message that looks like:\nAuto-merging kern/src/main.rs CONFLICT (content): Merge conflict in kern/src/main.rs Automatic merge failed; fix conflicts and then commit the result. You will need to manually modify the main.rs file to resolve the conflict. Ensure you keep all of your changes from lab 4. Once all conflicts are resolved, add the resolved files with git add and commit. For more information on resolving merge conflicts, see this tutorial on githowto.com .\nVarious design has been changed from lab 4. See the following summary of change for a merge guideline.\nSafe / Unsafe changes\nSeveral safe / unsafe definitions have been changed to conform better with Rust\u0026rsquo;s safety guarantee. If there is a merge conflict in this regard, follow the updated definition.\nScheduler\nGlobalScheduler now uses Box in the definition. Update Scheduler::new() function accordingly. Move timer manipulation from start() to initialize_global_timer_interrupt() method. Call this function in GlobalScheduler::start(). PageTable\nThe value of IO_BASE_END has been increased to allow the kernel to access the local timer address. To support that, the kernel page table now uses three L3 entries instead of two. Adjust related functions in kern/src/vm/pagetable.rs.\nVMM\nVMM includes more fields than before. Calculate the base address of the kernel table in initialize(), and save it to kern_pt_addr field with self.kern_pt_addr.store(kern_pt_addr, Ordering::Relaxed);. You will learn what this line means throughout this lab.\nIn addition, setup() is not called automatically in initialize() anymore. This is to separate the behavior of initializing the virtual memory manager and setting up the MMU for the current core, so that they can be invoked independently with multiple cores. You have to add VMM.setup() call in kmain(), right after VMM.initialize().\nIRQ / Traps\nIRQ has been redesigned to use trait-based logic. Read the change in kern/src/traps/irq.rs. Update the register() and invoke() function, and also handle_exception() as necessary.\nwrite_str syscall\nIn the previous lab, there was only a write() syscall which prints a single byte to the serial. We have added write_str() syscall, which takes a slice from the user and prints it atomically. kernel_api library has been updated to use this; Recompile your user programs and copy them to the SD card.\nLogging infrastructure Our kernel code now uses Rust\u0026rsquo;s log crate instead of kprintln! for message logging. It enables five logging macros trace!, debug!, info!, warn!, and error!.\nThe logging code is defined in kern/src/logger.rs. If VERBOSE_BUILD environment variable is set during the build (e.g., VERBOSE_BUILD=1 make), all logs will be enabled. Otherwise, trace level logs will not be displayed. There is no specific requirement for what level of log you should use in each circumstance, but here is a brief guideline:\nTrace: A piece of information that will help debugging the kernel, but too verbose to be enabled by default\nScheduler switch log IRQ interrupt log Debug: A piece of information that developers might be interested\nPage table address Info: A piece of information that the user of the kernel might be interested\nAmount of memory in the system Kernel initialization status Warn: An indication of exceptional erroneous situation\nOut of memory Unknown exception from a user program Error: An indication of an event that should never happen during the normal execution of the kernel\nDebug assertion violations Unknown exception inside a kernel ARM Documentation Along with three documents that we referred in lab 4,\nARMv8 Reference Manual This is the official reference manual for the ARMv8 architecture. This is a wholistic manual covering the entire architecture in a general manner. For the specific implementation of the architecture for the Raspberry Pi 3, see the ARM Cortex-A53 Manual. We will be referring to sections from this manual with notes of the form (ref : C5.2) which indicates that you should refer to section C5.2 of the ARMv8 Reference Manual .\nARM Cortex-A53 Manual Manual for the specific implementation of the ARMv8 (v8.0-A) architecture as used by the Raspberry Pi 3. We will be referring to sections from this manual with notes of the form (A53 : 4.3.30) which indicates that you should refer to section 4.3.30 of the ARM Cortex-A53 Manual .\nARMv8-A Programmer Guide A high-level guide on how to program an ARMv8-A process. We will be referring to sections from this manual with notes of the form (guide : 10.1) which indicates that you should refer to section 10.1 of the ARMv8-A Programmer Guide .\nWe will use two additional document in lab 5.\nAArch64 Programmer\u0026rsquo;s Guides: Generic Timer A guide for the generic timer in ARM architecture. We will be refering to sections from this manual with notes of the form (timer : 3.2) which indicates that you should refer to section 3.2 of the AArch64 Programmer\u0026rsquo;s Guides: Generic Timer .\nQuad-A7 Control A guide for Quad-A7 core control, which includes descriptions for per-core timer and interrupt handling. We will be refering to sections from this manual with notes of the form (QA7 : 4.10) which indicates that you should refer to section 4.10 of the Quad-A7 Control .\nYou can find all those five documents under doc/ subdirectory of our lab repo. We recommend that you download these five documents now and maintain them within easy reach.\nPhase 1: Enabling Multicore Lab 4 covered preemptive scheduling, which allowed multiple user programs to run inside a single kernel at the same time by means of context switching. Note that although multiple programs were running on our kernel concurrently, only one user program occupied the core at a time. In this phase, you will enable the other three cores of RPi board and support running user programs in parallel. Parallel programming involves many indigenous problems that do not exist in single-thread programming. To fix this, you\u0026rsquo;ll revisit the design of mutex, IRQ handler, and scheduler and adjust them accordingly to multicore environment.\nSubphase A: Waking Up Other Cores In this subphase, you\u0026rsquo;ll enable the three other cores of BCM2837 using the spin table mechanism.\nSpin Table All cores in a CPU share the main memory (RAM), thus it can be used as a communication medium among cores. Spin table is a booting mechanism that utilizes this characteristics of RAM. When you power on your RPi, the first core, core 0, will jump to _start function defined in kern/src/init.rs. All the other cores are spinning outside of the kernel, polling their spinning address.\nThis code from RPi firmware implements the spin table mechanism.\nin_el2: mrs x6, MPIDR_EL1 and x6, x6, #0x3 cbz x6, primary_cpu adr x5, spin_cpu0 secondary_spin: wfe ldr x4, [x5, x6, lsl #3] cbz x4, secondary_spin mov x0, #0 b boot_kernel primary_cpu: ldr w4, kernel_entry32 ldr w0, dtb_ptr32 boot_kernel: mov x1, #0 mov x2, #0 mov x3, #0 br x4 When RPi boots, core 0 loads the kernel address from symbol kernel_entry32 and branch to that address. The RPi firmware will fill kernel_entry32 with kernel_address value specified in config.txt before this routine. All the other cores spin inside secondary_spin loop with wfe. While looping, they load 8 bytes address from spin_cpu0 + 8 * core_idx, and branch to that address if it is non-zero. Therefore, in order to wake up other cores, core 0 needs to write the starting address to their spinning address and send events with sev instruction. You\u0026rsquo;ll use init::start2 as an entrypoint for other cores.\nEach core should use its own stack pointer to not interfere with the other cores. Function _start assigns KERN_STACK_BASE as the stack register for core 0. In our kernel design, core i+1 will use the stack right below the stack of core i. That is, KERN_STACK_BASE - KERN_STACK_SIZE * i for core i.\nImplementation Now you\u0026rsquo;re ready to implement the core initialization routine. You\u0026rsquo;ll write kmain() in kern/src/main.rs and initialize_app_cores(), start2(), and kmain2() in kern/src/init.rs.\nYou can implement these functions in any order you wish:\nIn initialize_app_cores() , write the address of start2() to each core\u0026rsquo;s spinning address.\nThe spinning base, spin_cpu0, is defined as a constant SPINNING_BASE in pi/src/common.rs. Core 0 should calculate each core\u0026rsquo;s spinning address, write the address of start2() to those addresses, invoke sev(), and wait for each core\u0026rsquo;s acknowledgement. Other cores should write 0 to their spinning address in kmain2() when they are fully awoken. Core 0 should check that all cores\u0026rsquo; spinning address has been overwritten with 0 before returning from initialize_app_cores().\nIn start2() , setup the stack pointer and branch to kinit2() .\nSet the stack pointer as described above. Recall that you can extract the core index from MPIDR_EL1. Then, branch to kinit2 after setting up the stack register. Be careful not to use any stack variable in start2(), since we are changing the stack pointer.\naarch64::affinity() might not work\naarch64::affinity() is not guaranteed to be inlined, and it might use the stack space to call the function. Because we don\u0026rsquo;t set the stack pointer yet, you may not be able to use aarch64::affinity(). Try to access MPIDR_EL1 register directly.\nIn kmain2() , print a message and acknowledge that the core is available.\nIf the initialization is successful, each core will reach kmain2() and start executing it in EL1. In kmain2(), write 0 to the core\u0026rsquo;s spinning address to notify core 0. Then print any message to the console and loop indefinitely. You\u0026rsquo;re welcome to use the newly added logging macros.\nWhen you finish writing the code, change your kmain() like this:\nunsafe fn kmain() -\u0026gt; ! { ALLOCATOR.initialize(); FILESYSTEM.initialize(); VMM.initialize(); SCHEDULER.initialize(); init::initialize_app_cores(); VMM.setup(); SCHEDULER.start(); } If you run the kernel on RPi board, you should see initialization message from each core. However, these new cores are not doing any useful works yet, because some components in our kernel are not parallel-ready. The next step is to fix those outdated designs!\nBeware of unsound Mutex!\nNote that our mutex is unsound, so kprintln!() and logging macros, which internally uses mutex, may deadlock or even trigger a data race when accessed by multiple cores. This should be a rare case, and if you retry enough time, you should see a successful case. We are going to fix this soon.\nHow to detect spinning base without hard coding the address? (spin-address)\nWe have hardcoded the spin address of Raspberry Pi to our kernel, so the kernel won\u0026rsquo;t run on another hardware if they use different spin address. How does Linux kernel solve this problem?\nSubphase B: Mutex, Revisited The mutex code in our skeleton code doesn\u0026rsquo;t provide an actual thread safety and lies to Rust compiler by unsafely implementing Send and Sync trait. We did this because atomic memory operations required for thread-safe mutex implementation is only available when MMU is initialized in AArch64 architecture, which was merely added in lab 4. We justified our design with the fact that there is only one core available in the system, but since we have both MMU and multiple cores enabled now, it is the time to correct that lie. In this subphase, you\u0026rsquo;ll fix the mutex design so that multiple cores can use mutex in a sound manner. You will be working in kern/src/mutex.rs.\nPer-core Data Management When multiple cores are active in a kernel, the kernel must distinguish per-kernel resource initialization and per-core resource initialization. Per-kernel resource needs to be initialized only once per startup. In our kernel, per-kernel resources are defined in main.rs. Static structs such as ALLOCATOR, FILESYSTEM, and SCHEDULER are per-kernel resources. Core 0 should initialize per-kernel resources before waking up other cores. On the other hand, per-core resource must be initialized for each core in kmain() and kmain2().\nNow go ahead and read per-core resource definitions in kern/src/percore.rs. Each PerCore struct has three data fields: preemption, mmu_ready, and local_irq. preemption counts the number of lock held by the core, mmu_ready flag saves whether MMU is set for each core, and finally local_irq saves IRQ handler for each local interrupt.\nYou may notice atomic types used in the definition of PerCore struct. For now, think of them as a plain type with internal mutability. We will discuss about atomic types in detail later.\nThen, start reading code in kern/src/vm.rs. In lab 4, setup() was called directly in initialize(), and core 0 immediately initialized its MMU after initializing the virtual memory manager. However, in lab 5, the role of them are separated. Core 0 initializes the virtual memory manager during the global initialization routine, and each core, including core 0, sets up its MMU after the core initialization is done.\nWe have provided a partial implementation of wait(). See how it calls setup() and, how set_mmu_ready() and is_mmu_ready() is used to track per-core MMU initialization information. Later you\u0026rsquo;ll fill the rest of the code so that each core loops and waits until the MMU initialization of all cores finishes and returns together.\nMemory Consistency Model Recall that RAM is shared among all cores. When multiple cores are reading to and writing from a same address, in which order they would read and write a value? What can be the observable output from a program? Can the same program behave differently on x86-64 and AArch64? Architectures\u0026rsquo; memory consistency model specifies the answer to these questions.\nLet\u0026rsquo;s look at a classic example of memory ordering, where two threads are mutating values and printing them. Rust\u0026rsquo;s type system prevents this example to compile without an unsafe block, but let\u0026rsquo;s assume that this code was written in a racy variant of Rust for the perpose of demonstration. Assume A and B are both initialized to 0.\n/* Thread 1 */ A = 1; // (1) print!(\u0026#34;{}\u0026#34;, B); // (2) /* Thread 2 */ B = 1; // (3) print!(\u0026#34;{}\u0026#34;, A); // (4) One of the most intuitive memory consistency model is sequential consistency (SC). It assumes two things: (1) there is a global order of all reads and writes, and (2) the interleaving preserves the order of instructions from the same thread (program order). In sequential consistency, one of possible output of the above program is 01, where one thread completes the execution before the other thread start the execution. The order of the program that prints this result 1-2-3-4 or 3-4-1-2. 11 is slightly less obvious output, where the execution of two threads are interleaved, for example like 1-3-2-4 or 3-1-2-4. In this model, 00 is not a valid output of the above program.\nMultithread vs Multicore\nStrictly speaking, multithread and multicore are different concepts. If multiple threads are running on a single core, there is no memory consistency problem. However, almost every CPU has multiple cores these days, and two terms are used somewhat interchangably. When you see a term \u0026ldquo;multiple threads\u0026rdquo; in this lab document, you can assume that it implies multiple threads running on a multicore CPU if not stated otherwise.\nModern CPUs leverage numerous optimizations such as out-of-order execution, per-core caches, and load/store optimization which significantly improves the execution performance. Unfortunately, SC is too strict to permit these optimizations. SC requires a global ordering of every memory operation running in multiple cores, which is essentially requiring single threaded behavior for multicore architecture. In result, modern architectures adopt weaker memory consistency model than SC.\nEach core in Cortex-A53 has its own instruction and data cache Total store ordering (TSO) is slightly weaker consistency model than SC. It allows store buffering, which may delay the propagation of write operations to other cores. This weakening allows significant performance improvement over the SC model, and x86 and x86-64 specifies a memory consistency that is very similar to TSO.\nTSO upholds many guarantees of SC, but it allows a behavior that is ruled out by SC. Let\u0026rsquo;s look at the example code again:\n/* Thread 1 */ A = 1; // (1) print!(\u0026#34;{}\u0026#34;, B); // (2) /* Thread 2 */ B = 1; // (3) print!(\u0026#34;{}\u0026#34;, A); // (4) Under TSO model, it is possible to observe the result 00 from this program. If A = 1 is stored in thread 1\u0026rsquo;s cache and B = 1 is stored in thread 2\u0026rsquo;s cache, and each thread continues the execution before the write propagates to each other, they may both print 0 as the result. This is not an allowed behavior under SC.\nARM architecture specifies even weaker memory model. Consider the following program:\n/* Thread 1 */ B = 1; // (1) A = 1; // (2) /* Thread 2 */ print!(\u0026#34;{}\u0026#34;, A); // (3) print!(\u0026#34;{}\u0026#34;, B); // (4) In ARM consistency model, surprisingly the program can print 10 as a result. Which means, thread 2 may observe the writes from thread 1 in different order from the order that they are written in thread 1. Under TSO, this is not allowed because subsequent writes become visible to other cores in the order that they have written (hence the name, \u0026ldquo;total store ordering\u0026rdquo;).\nARM architecture said to have \u0026ldquo;weak memory ordering with data dependency\u0026rdquo;. In weak memory ordering model, in general, there is no consistency guarantee between two different memory locations regardless of the program order. However, if a value in one memory location depends on a value in another memory location, there is a data dependency guarantee.\nConsider the following two examples:\n/* Program 1 */ A = 1; // (1) B = 1; // (2) /* Program 2 */ C = 1; // (3) D = C; // (4) In program 1, A and B are independent memory locations, so other threads may observe (2) before (1). However, in program 2, the value of D depends on the value of C, so if other threads observe 1 at memory location D, they are guaranteed to observe 1 at memory location C.\nMemory consistency example (consistency-handson)\nUsing two threads, write an example that will print different set of results on SC, TSO, and ARM hardware. Specify what can be printed on each memory consistency model.\nMoreover, not only hardware, but also a language defines its memory consistency model. When a compiler optimizes program code, it might reorder or even completely remove certain statements based on the language specification.\n/* program 1 */ let mut x = \u0026amp;mut 1; for i in 0..1000 { println!(\u0026#34;{}\u0026#34;, *x); } /* program 2 */ for i in 0..1000 { println!(\u0026#34;{}\u0026#34;, 1); } In the above code snippet, program 1 and 2 are equivalent if there is no other thread modifying the variable x during the loop. Is it valid for Rust compiler to optimize program 1 to program 2? Recall that Rust\u0026rsquo;s mutable reference implies the exclusive access to the underlying value. Thus, the answer is yes in Rust; The compiler can optimize program 1 to program 2 based on the assumption that x is a unique access to the underlying memory location. To tell the compiler that x might be modified by other threads, a raw pointer or an UnsafeCell must be used instead of a mutable reference \u0026amp;mut.\nMemory Barrier and Atomic Instruction As you should have felt at this point, writing a correct parallel program is substantially harder than writing a correct single-threaded program. Parallel programming involves numerous cognitive pitfalls, and AArch64\u0026rsquo;s weak memory consistency model further complicates the reasoning of a parallel program.\nMemory barriers and atomic instructions are tools that an architecture provide to tame the complexity of memory ordering. They allow programmers to manually introduce stronger memory ordering guarantee in localized, controlled manner. At language level, they also provide a way to write a portable code that can be compiled to several architectures with different memory consistency guarantee.\nMemory barrier ensures the dependency between the instructions before and after the barrier. With memory barrier, it is possible to explicitly specify the dependency between instructions before and after the barrier when strong memory ordering guarantee is required for program correctness.\nIn ARM architecture, there are three kinds of memory barrier instructions (ref: B2.3.5). By default, they act as a full system barrier operation, which fully synchronizes the instructions before and after the barrier (according to their instruction type). You can adjust the behavior with option parameter, such that they only wait for specific instruction types. For instance, it is possible to make them wait only for stores, not for load.\nDMB, Data Memory Barrier\nData memory barrier acts as a memory barrier. It ensures that all explicit memory access that appear in program order before the DMB instruction are observed before any explicit memory access that appear in program order after the DMB instruction.\nDSB, Data Synchronization Barrier\nData Synchronization Barrier acts as a special kind of memory barrier. No instruction in program order after this instruction executes until this instruction completes.\nISB, Instruction Synchronization Barrier\nInstruction Synchronization Barrier flushes the pipeline in the processor, so that all instructions following the ISB are fetched from cache or memory, after the instruction has been completed.\nBarriers in context_restore (context-restore-barrier)\nRecall these four instructions that we put after overwriting TTBR registers in lab 4. Explain what do these lines mean, and why they are needed when updating a page table.\ndsb ishst tlbi vmalle1 dsb ish isb Atomic instruction is another tool that architectures provide to facilitate parallel programming. Atomic instructions guarantee that it is not possible to partially perform certain actions.\n/* thread 1 */ mov x0, #0 mov x1, addr of counter loop: ldr x2, [x1] add x2, x2, #1 str x2, [x1] add x0, x0, #1 cmp x0, #1000 ble loop /* program 2 */ mov x0, #0 mov x1, addr of counter loop: ldr x2, [x1] add x2, x2, #1 str x2, [x1] add x0, x0, #1 cmp x0, #1000 ble loop Consider the above program, where two threads are incrementing the same memory address. They both loop 1000 times, so ideally the counter should contain 2000 when the threads finish the execution. Sadly, the increment is not performed in an atomic manner, so each thread can read the \u0026ldquo;intermediate\u0026rdquo; value of each other. The program does not prevent thread 2 to load the value of the counter between the time frame of thread 1\u0026rsquo;s load and store. When it happens, both thread 1 and 2 write the same value to the counter, resulting in the counter to be only incremented by one instead of two.\nAtomic instructions help resolving this problem by either performing an action in non-preemptive manner (single instruction to perform fetch and add) or provide a mechanism to detect the data contention so that the operation can be retried. They are building blocks for higher level synchronization primitives such as mutex. Check ARMv8-A Architecture Reference Manual C3.2.12-14 and ARMv8-A Synchronization Primitives document to read more about AArch64\u0026rsquo;s atomic instructions.\nWriting atomic assembly code (atomic-handson)\nRewrite the above program with atomic instructions, so that the answer is always correct.\nCheck your architecture version\nYou should check the architecture version when writing an atomic instruction. For instance, CAS, compare and swap instruction is only available after ARMv8.1 and cannot be used in ARMv8. If you are using Rust, the LLVM backend will automatically handle this.\nExcept the case when you work at low-level inline assembly level, you should use atomic data types such as AtomicBool and AtomicUsize provided by Rust\u0026rsquo;s standard library. They provide high-level atomic operations such as fetch_and() and compare_and_swap(). Even if the architecture does not support them as a single operation, the compiler will preserve the semantics and implement them with several smaller atomic operations.\nimpl AtomicBool { pub fn fetch_and(\u0026amp;self, val: bool, order: Ordering) -\u0026gt; bool { ... } } impl AtomicUsize { pub fn compare_and_swap( \u0026amp;self, current: usize, new: usize, order: Ordering ) -\u0026gt; usize { ... } } Note the \u0026amp;self requirement in those operations. A non-mutable reference in Rust implies that the access to the value is shared. Although multiple code accesses the value at the same time, atomic operations guarantee that there will be no data race. Thus, it is valid to use \u0026amp;self instead of \u0026amp;mut self in these operations.\nA visualization of acquire and release semantics These atomic instructions accept an Ordering parameter. Rust provides five memory orderings , which are the same as those of C++20 . Relaxed ordering provides atomic operation without ordering guarantee. Acquire and release orderings are used in pair; A load with acquire ordering prevents instructions after load to be reordered before it, and a store with release ordering prevents instructions before store to be reordered after it. In result, an acquire-release pair of an address creates a critical region that prevents instructions between them from being reordered to outside of the critical region. In addition, if thread A writes to a variable with release ordering and thread B reads the same variable with acquire ordering, all subsequent loads in thread B can see stores from thread B before the write with release ordering. AcqRel is Acquire plus Release for atomic instructions that perform both load and store. SeqCst is AcqRel with the additional guarantee that all threads see sequentially consistent operations in the same order.\nFurther reading\nChapter 8 of Rustonomicon contains a summary of what we have covered.\nImplementation Now you have enough knowledge to fix the mutex code. You will primarily working in kern/src/mutex.rs, while fixing other functions in kern/src/vm.rs, kern/src/main.rs, and kern/src/init.rs.\nFirst, start with per-core MMU initialization. Add VMM.wait() to kmain() like below:\nunsafe fn kmain() -\u0026gt; ! { ALLOCATOR.initialize(); FILESYSTEM.initialize(); VMM.initialize(); SCHEDULER.initialize(); init::initialize_app_cores(); VMM.wait(); SCHEDULER.start(); } Then, modify kmain2() in kern/src/init.rs so that it calls VMM.wait() after writing 0 to its spinning address to acknowledge the core boot sequence. Once you are finished, print any message inside an infinite loop in the next line.\nNext, finish the implementation of wait() in kern/src/vm.rs. Specifically, after each core setup its MMU by calling setup() (which is included in the skeleton code), increment the ready_core_cnt by one and loop until the count reaches the number of core, pi::common::NCORES. Think about which atomic ordering should be used here.\nThe provided mutex code used relaxed load and store, which do not synchronize among cores. Therefore, if you test your code at this point, the printing result from cores can be mixed together or sometimes the kernel even deadlocks and stops making progress.\nNow, fix try_lock() and unlock() in kern/src/mutex.rs. You need to use atomic operations as well as APIs in kern/src/percore.rs to properly implement the mutex. Specifically, use is_mmu_ready() to check whether MMU is enabled or disabled, getcpu() to increment core\u0026rsquo;s preemption counter when you lock a mutex, and putcpu() to decrement core\u0026rsquo;s preemption counter when you unlock a mutex.\nHere is the description of how mutex should behave when MMU is enabled and disabled:\nWhen MMU is disabled\nA mutex in this state is used when core 0 is initializing per-kernel resources. If MMU is disabled, general atomic operations are not available except relaxed load and store. As such, we will only allow core 0 to use mutex when MMU is disabled. If only one core is accessing the mutex, these operations are sufficient to implement a mutex. In fact, it can be thought as a mutable-only version of RefCell.\nYou can reuse most of the current mutex code to implement this. Add the necessary checks to make sure your mutex follows Rust\u0026rsquo;s safety requirement.\nThe original mutex code supported reentrancy, which allows the owner of a mutex to lock it more than once. However, this design is not compatible with Rust\u0026rsquo;s one mutable owner model in general. You don\u0026rsquo;t have to implement this when you fix the mutex design.\nWhen MMU is enabled\nWhen MMU is enabled, perform correct locking and unlocking with atomic operations. The overall code structure will look similar with the MMU-disabled state, but instead of relaxed load and store, atomic operations such as compare_and_swap or swap should be used with correct ordering parameter. Check APIs of AtomicBool and choose whatever atomic operation you want.\nWhen you are done, messages from each core should be printed line by line without any overlap or deadlock. Double check your implementation even if you get the expected behavior. Parallel code is indeterministic in nature, which makes bugs in them very hard to be reproduced and fixed. It\u0026rsquo;s much better to take time now and prevent those bugs than be puzzled with inscrutable behaviors later. If you feel confident of your design, continue to the next subphase.\nBe careful not to use mutex before MMU initialization\nIf you have followed the instruction correctly, Mutex will assert that the core number is equal to 0 when MMU is not initialized for the current core. If other core tries to use mutex before MMU initialization, it will halt the core with infinite recursion, because assert!() calls kprintln!(), kprintln!() calls Mutex::lock(), and Mutex::lock() calls assert!().\nWhen this happens, the kernel becomes unresponsive without printing anything to the console. If you find your kernel behave in this way, try to find a lock usage before MMU. Note that kprintln!() and logging macros use console lock internally.\nSend and Sync variance is hard\nA trait implementation of a wrapper type sometimes depends on a trait implementation of an inner type. This is called \u0026ldquo;variance\u0026rdquo;.\nContainers like Vec\u0026lt;T\u0026gt; implements Send if T is Send and Sync if T is Sync. Mutex implements Send if T is Send and Sync if T is Send. The variance rule for Sync and Send trait is tricky and hard to reason about. When you are in doubt, check the closest type in Rust standard library and follow the design.\nExplain your mutex design (mutex-design)\nWhy did you choose such ordering requirement? Why does this design guarantee the soundness? Explain with brevity.\nState transition of Mutex (mutex-bad-state)\nWhat can go wrong if a thread calls lock(), initialize MMU, and unlock()? If you think this should be prevented, describe why it can be a problem and how to prevent it. You may add additional checks in VMManager::wait(). If you think it is okay to allow such behavior, justify your thought. Either answer can be correct depends on how you implemented mutex.\nSubphase C: Multicore Scheduling Right now, only core 0 is scheduling user programs. In this subphase, you will make other cores participate in scheduling. As a result, our kernel will have around four times more throughput to run processes compared to the single-core version.\nPer-Core IRQ Handling In this section, you\u0026rsquo;ll enable per-core IRQ handling. We have used timer register at IO_BASE + 0x3000, but the interrupt from this timer only propagates to core 0. In order to make other cores participate in scheduling, we\u0026rsquo;ll switch to another timer interrupt named CNTPNSIRQ, which stands for Counter Physical Non-Secure IRQ.\nYou\u0026rsquo;ll mainly work in pi/src/local_interrupt.rs. The overall structure is mostly similar to pi/src/timer.rs and pi/src/interrupt.rs. You will also read \u0026ldquo;AArch64 Programmer\u0026rsquo;s Guides: Generic Timer\u0026rdquo; (timer) and \u0026ldquo;Quad-A7 Control\u0026rdquo; (QA7) as references while working on per-core IRQ handling. \u0026ldquo;AArch64 Programmer\u0026rsquo;s Guides: Generic Timer\u0026rdquo; describes how generic timer works in AArch64 architecture, and \u0026ldquo;Quad-A7 Control\u0026rdquo; describes how to propagate interrupts from generic timer to each core.\nFirst, fill in the definition of Registers. See QA7 chapter 4 for register definition. You only need to define registers up to \u0026ldquo;Core3 FIQ Source\u0026rdquo;. Then, fill the definition of LocalInterrupt and implement From\u0026lt;usize\u0026gt; for LocalInterrupt. See QA7: 4.10 for the definition.\nNext, implement the following functions of LocalController:\nenable_local_timer()\nEnable CNTP timer by setting appropriate bits to CNTP_CTL_EL0 register (ref: D7.5.10). Now enable per-core CNTPNS IRQ by writing values to Core X timers interrupt control register (QA7: 4.6). Use register definition in lib/aarch64/src/regs.rs whenever possible instead of writing an inline assembly.\nis_pending()\nRead corresponding bits from Core X interrupt source register (QA7: 4.10) and convert it to a boolean value.\ntick_in()\nFinally, implement tick_in() method for generic timer as we did in lab 4. See timer: 3.1 to 3.3 to determine which register you should use. You\u0026rsquo;ll need to convert Duration to counter tick value using the frequency of the timer. Again, prefer using register definition in lib/aarch64/src/regs.rs over writing an inline assembly.\nHow de we clear CNTPNS IRQ bit? (cntpns-clear)\nWhen local timer passes the specified time, CNTPNS IRQ bit is set and the core will receive an IRQ interrupt. When using the global timer, we wrote to CS register to clear the interrupt bit. Then, how do we clear CNTPNS IRQ bit?\nFixing the Scheduler The next step is to use this per-core timer interupt in the scheduler to enable multicore scheduling. Follow these instructions step by step:\nMake the scheduler use per-core local timer interrupt\nOpen kern/src/scheduler.rs. You should have initialize_global_timer_interrupt() in GlobalScheduler::start() if you followed the merge guideline at the beginning of the lab. Add the core number check around initialize_global_timer_interrupt(), so that only core 0 invokes that function. Then, call initialize_local_timer_interrupt() in the next line (for all cores).\nNext, erase what you have in initialize_global_timer_interrupt and implement the same logic in initialize_local_timer_interrupt with local interrupt controller. Leave the content of initialize_global_timer_interrupt empty for now, but do not erase the call to this function in start(). We will use this function in the later part of the lab.\nSupport local timer interrupt handling\nImplement Index\u0026lt;LocalInterrupt\u0026gt; for LocalIrq in kern/src/traps/irq.rs. Then, add local timer interrupt handling logic in kern/src/traps.rs. Global interrupt should be handled only by core 0, and all cores should handle their local interrupts.\nFix the scheduler logic for multicore environment\nSwitching to multicore environment breaks a few characteristics of the scheduler.\nIt is no more guaranteed that the first process in the queue matches the current running process on the core. If your scheduler logic relies on this assumption, fix that now.\nIt is not guaranteed that the process found with switch_to() call in start() is in its initial state. This can happen if the number of process is smaller than the number of cores. In result, we can no longer assume that all registers are zero in the process\u0026rsquo;s context, and it is invalid to overwrite any register used by the user process after returning from context_restore.\nThere are many ways to fix this. One naive way is giving up the stack adjustment and set SP to the address of the copied trap frame in the kernel stack. This solution wastes about 300 bytes plus the size of the trap frame. Another way to fix it is to copy the content of tf to the top of the kernel stack, and rewind SP back before calling context_restore. This involves more unsafe code, but it only wastes the size of the trap frame. You can also save the value of general registers to the memory after context_restore, use them as temporary regsiters and adjust SP, and restore them back before eret.\nYou may need to insert sev() in schedule_out() if you loop with wfe() in switch_to() to notify other cores waiting.\nMake other cores participate in scheduling\nThe last step is to replace the infinite loop at the end of kmain2() in kern/src/init.rs with SCHEDULER::start().\nWhen you are finished, you should see that four cores are participating in scheduling. Print a trace!() log in local IRQ handling routine and verify that all four cores are participating in the scheduling. Recall that you need to run VERBOSE_BUILD=1 make to enable trace-level log. If everything works correctly, proceed to the next phase.\nMulticore performance experiment (multicore-performance)\nPopulate 4 fib processes in the scheduler. Run them with and without multicore, and record the running time. How much time did it take on a single core, and how much time did it take on four cores? Was it exactly four times faster on four cores? If not, what will be the overhead?\nPhase 2: TCP Networking In this phase, you\u0026rsquo;ll add TCP networking capability to our kernel by implementing a network driver and related system calls. In subphase A, you will learn how TCP over Ethernet works. Then, in subphase B, You will integrate an existing Ethernet implementation for RPi (USPi) and a minimal TCP stack (smoltcp) to our kernel. In subphase C, you will adjust process and scheduler so that sockets are properly managed as a process resource. Finally, in subphase D, you will implement several socket related system calls to expose the network driver to a user program.\nSubphase A: Networking 101 In this subphase, you\u0026rsquo;ll learn the concepts of computer networking. We will cover the basics of computer networking, network layer, how a packet routes through computer networks, and socket abstraction provided by OS.\nNetwork Layer Model Networking is a way to communcate with other computers. There are many models that describe the network structure, but the common concept among them is that they all model network as a layered structure. In this lab, we will use TCP/IP\u0026rsquo;s 4 layer model. From the lowest to the highest, 4 layers of TCP/IP model are:\nLink Layer (Ethernet, IEEE 802.11) Internet Layer (IP) Transport Layer (TCP, UDP) Application Layer (FTP, HTTP, Plain Text) The link layer provides a way to exchange packets in local (i.e., directly connected) network, such as computers connected to the same switch. Ethernet and IEEE 802.11 (Wi-Fi) are two popular link layer protocol. Typically, a Media Access Control (MAC) address is used as an identifier for link layer protocols. TCP/IP is designed to be link-layer independent, and there even exists a standard which uses homing birds as a link layer .\nThe internet layer defines an addressing system to identify a host and how packets should be routed to the destination. Internet protocol version 4 (IPv4) is the most widely used address system. Under IPv4 scheme, an endpoint is identified by an address of four octets, which is often denoted as four decimal numbers separated by dots, such as 127.0.0.1. Internet protocol only provides best-effort delivery ; it does not guarantee that a packet will actually reach the destination. This concern is handled at the transport layer level.\nThe transport layer provides an abstraction of data channels, so that multiple connections can be established between two host computers. The two most famous transport layer protocols are Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). They use ports, a 16-bit integer, as an endpoint identifier. TCP provides reliable connection between two endpoints with packet delivery acknowledgement, retransmission for error recovery, and congestion control. On the other hand, UDP is a connectionless and lossy protocol that is much lighter and faster.\nFinally, the application layer serves various application level protocols. For instance, HyperText Transfer Protocol (HTTP) for web content, File Transfer Protocol (FTP) for file sharing, or Transport Layer Security (TLS) for encrypted communcations. In our lab, we will send and receive plain text messages over TCP/IP stack, using an Ethernet cable.\nPacket Routing A route table is a data table managed by an operating system to determine which interface and gateway should be used to reach an IP address. On Ubuntu 18.04, a route table can be printed with route command (if the command is not available, install net-tools package by running sudo apt install net-tools).\nAn example route table looks like this:\nKernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.0.1 0.0.0.0 UG 0 0 0 enp11s0f1 192.168.0.0 0.0.0.0 255.255.255.0 U 0 0 0 enp11s0f1 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 An entry in a route table contains destination, gateway, subnet mask (genmask), and interface ID (Iface). Given an IP address, the operating system compares it with each entry in the route table to determine where that packet should be delivered (or forwarded). If the packet\u0026rsquo;s target IP address masked by the subnet mask matches the destination IP of an entry, that packet goes through the specified interface. If the packet\u0026rsquo;s target IP does not match any entry, it will be delivered to the default gateway, which is 192.168.0.1 in our example.\nA pair of destination address and a subnet mask is called Classless Internet-Domain Routing (CIDR) block. A subnet mask splits 32-bit address into two sections, network address and host address. The network address is a fixed part and represented as bit 1 in the subnet mask, and the host address is a variable part and represented as bit 0 in the subnet mask. A subnet mask always consists of n bits of 1 followed by 32-n bits of 0, and it is often shorthanded as \u0026ldquo;/(# of 1 bit in the subnet mask\u0026rdquo; after the IP address. For instance, destination IP 192.168.0.0 with subnet mask 255.255.255.0 is written as 192.168.0.0/24, and destination IP 172.17.0.0 with subnet mask 255.255.0.0 is written as 172.17.0.0/16.\nIf an entry does not define a gateway, then a host with that IP can be found in the local network connected through the interface. For instance, since 192.168.0.4 matches 192.168.0.0/24, a host with IP 192.168.0.4 can be found in the local network connected to enp11s0f1. If an entry defines a gateway, then a packet will be forwarded to that IP first in order to reach the target address. For instance, a packet with target IP 1.1.1.1 will be first forwarded to the default gateway 192.168.0.1 to reach the destination. Then, a machine at 192.168.0.1 (a router) routes the packet to the next host according to its own routing table.\nWhen used with IPv4 and Ethernet, hardware address in ARP packet is MAC address and protocol address in ARP packet is IPv4 address. Recall that MAC address is used as an address scheme when delivering packets through Ethernet link layer. Address Resolution Protocol (ARP) is used to discover the association of the IPv4 address and the MAC address of local neighbors. ARP packet consists of IP and MAC address pair, and each computer in the local network saves (IP, MAC) pair in its neighbor cache for future use.\nSocket Just like MAC is a link layer address and IP is a internet layer address, a port can be thought as an address for transport layer. An IP address is used to select a computer, and a port is used to select specific process running on that computer. Combined, a pair of IP address and a port forms a transport layer endpoint. Transport layer protocols such as TCP and UDP connects two different endpoints.\nA diagram of network layer hierarchy from \u0026#34;TCP/IP Illustrated, Vol. 1: The Protocols\u0026#34; Most of user applications work at the application layer level. They need to select an endpoint to connect (or listen on) and translation layer protocol (TCP/UDP) to use when initializing the connection, but after that, they only send and receive application data. The operating system and the network driver take cares of how to deliver an actual packet generated from user programs, and during the process, they wrap and unwrap a packet with various protocol headers.\nTherefore, an operating system needs to manage the list of active connections and remember which process is using which connections. A socket is a system resource that abstracts this behavior. User programs open, interact with, and close sockets with system calls; They use a socket descriptor, a handle for a socket, to address a specific socket.\nUnix operating system uses Berkeley Socket API . A few important socket APIs of Berkeley Socket APIs are as follows. These are descriptions of each API from Wikipedia:\nsocket() creates a new socket of a certain type, identified by an integer number, and allocates system resources to it. bind() is typically used on the server side, and associates a socket with a socket address structure, i.e. a specified local IP address and a port number. listen() is used on the server side, and causes a bound TCP socket to enter listening state. connect() is used on the client side, and assigns a free local port number to a socket. In case of a TCP socket, it causes an attempt to establish a new TCP connection. accept() is used on the server side. It accepts a received incoming attempt to create a new TCP connection from the remote client, and creates a new socket associated with the socket address pair of this connection. send(), recv(), sendto(), and recvfrom() are used for sending and receiving data. close() causes the system to release resources allocated to a socket. In case of TCP, the connection is terminated. In our lab, we will use a TCP/IP stack written in Rust named smoltcp . Since smoltcp\u0026rsquo;s API set provides a little different functionality compared to Berkeley Socket API, we will just use smoltcp\u0026rsquo;s API. These are notable differences compared to Berkeley Socket API:\nInstead of bind(Local Addr) and listen(), it uses single listen(Local Addr) syscall. Instead of using a blocking syscall accept() which waits for a new connection and creates a new socket, a server socket will automatically get connected when Ethernet device is polled. To accept a new connection, a new socket need to be created. send() and recv() are non-blocking by default. send() will queue the message to socket\u0026rsquo;s buffer and return immediately, and recv() will return immediately even if there\u0026rsquo;s no message. Subphase B: Network Driver In this subphase, you\u0026rsquo;ll add a network driver to our kernel. TCP/IP specification is such a complex and huge protocol with numerous extensions, so instead of writing the network stack from scratch, we are going to use existing implementations and focus on their integration to OS. Namely, we will use USPi as an Ethernet driver and smoltcp as a network stack.\nIntegrating USPi USPi is a bare metal USB driver for Raspberry Pi written in C. We are using it to communicate with Raspberry Pi\u0026rsquo;s Ethernet contoller through USB protocol. USPi consists of two parts, env and lib. lib is the main part of the library, and it expects a few API from the environment, such as memory allocation, timer, interrpt handling, and logging. env is a minimal kernel that exposes these functions, but instead of using env, we are going to replace it with our RustOS kernel.\nkern/src/net/uspi.rs is responsible for the integration of USPi and RustOS. Functions inside extern \u0026quot;C\u0026quot; block are exposed from USPi to Rust, and functions with #[no_mangle] annotation are exposed from Rust to USPi. Most of the provided functions are self-explanatory. Go ahead and read the file now. The code structure follows the usual design; USPi struct implements non-thread safe version to interact with USPi and Usb struct wraps USPi under a Mutex so that it can be accessed by multiple cores at the same time.\nTwo things noteworthy in this module are uspi_trace!() macro and start_kernel_timer(). Recall that VERBOSE_BUILD environment variable controls whether trace-level logs are printed or not. uspi_trace!() wraps trace!() macro and provides another knob to control, namely DEBUG_USPI. Set it to true when you are working on USPi related functions and turn it off when you are done. start_kernel_timer() uses USPi\u0026rsquo;s TimerStartKernelTimer() to register timer callback functions. The logic is very similar to the timer IRQ handling of our kernel, but it implements software timer so that multiple callback functions with different delays can be registered under the same interrupt.\nUSB and Timer3 interrupts need to be handled in the kernel to support USPi. Moreover, USB interrupt needs to be handled in the kernel context. Which means, our kernel needs nested handling of USB interrupt. Instead of allowing general nested interrupt handling, we are going to handle USB interrupt as FIQ and allow nested FIQ interrupt in a few limited code places.\nNetwork Driver We will use smoltcp , a minimal TCP/IP stack for bare-metal systems written in Rust, as a network stack for our kernel. Three important traits / structs in smoltcp are SocketSet , Device , and EthernetInterface .\nSocketSet manages a set of sockets. Each socket in a socket set manages its own Rx and Tx buffer, and writing to and reading from a socket only accesses these buffers. A type that implements Device trait represents a physical hardware device that is capable of send and receive a raw network packet. In our case, USPi library will be used to send and receive raw physical packets. EthernetInterface internally manages Device and uses it to send and receive packets. EthernetInterface can be poll()ed with a socket set. When polled, pending data in Tx buffer of sockets will be sent and received data will be buffered to Rx buffer of sockets. kern/src/net.rs integrates smoltcp library with our kernel design. Read the code of following structs to understand how this integration is done:\nFrameBuf and Frame: Fixed size, 8-byte aligned, and length trackable u8 buffer.\nEthernetDriver and GlobalEthernetDriver: A thread-unsafe Ethernet driver struct and its corresponding struct wrapped in a mutex.\nUsbEthernet, RxToken, and TxToken: UsbEthernet implements smoltcp::phy::Device trait, which is an interface for sending and receiving raw network frames. RxToken and TxToken structs implement smoltcp::phy::RxToken and smoltcp::phy::TxToken, which are traits that define how a single network packet should be sent and received.\nWhen administrating a network stack, an important role of an operating system is managing port numbers on the system. Typically, an operating system tracks which processes are using which port numbers and prevents other processes to interfere with the connection. To support this behavior, you need to implement port management functions in kern/src/net.rs and use them properly in the process and scheduler code.\nImplementation You\u0026rsquo;re now ready to implement a network driver to our kernel. We recommend you to implement features in the following order:\nFinish USPi environment integration in kern/src/net/uspi.rs\nFirst, implement logging functions, DoLogWrite() and uspi_assertion_failed(). When implementing DoLogWrite(), ignore _pSource and _Severity, translate pMessage as null-terminated C-style string, and print it with uspi_trace!() macro. When implementing uspi_assertion_failed(), convert pExpr and pFile to Rust string similarly and print them with line number nLine. Both functions should not panic.\nThen, implement four timer functions, TimerSimpleMsDelay(), TimerSimpleusDelay(), MsDelay(), and usDelay(). Convert milliseconds and microseconds provided as a parameter to Rust\u0026rsquo;s Duration and use pi::timer::spin_sleep().\nFinally, implement malloc() and free(). These are C-style allocation API, which means that they do not provide layout parameter like Rust allocation API. Carefully read the definition of malloc() and free() and think about what information need to be tracked in addition to what has been provided as their parameters. Allocate additional space in a chunk to save those information. Make sure all allocated pointers are 16-byte aligned.\nEnable FIQ interrupt handling\nUSPi requires handling of USB and Timer3 interrupts. Moreover, it requires to handle USB interrupt in the kernel context. We are going to handle USB interrupt as FIQ interrupt instead of normal IRQ interrupt and allow nested FIQ interrupt handling in small number of places to support this behavior.\nStart by implementing enable_fiq() in pi/src/interrupt.rs. You\u0026rsquo;ll need to revisit chapter 7 of the BCM2837 ARM Peripherals Manual to see how an interupt can be selected as a FIQ interrupt. Then, add FIQ handling routine to handle_exception() in kern/src/traps.rs and implement Index\u0026lt;()\u0026gt; for Fiq in kern/src/traps/irq.rs.\nYou may want to check FIQ control register when writing enable_fiq().\nOnce you are finished, complete ConnectInterrupt() function in kern/src/net/uspi.rs. First, assert that nIRQ is one of Interrupt::Usb or Interrupt::Timer3. Then, if the request is for USB interrupt, enable FIQ handling of USB interrupt and register a handler that invokes provided pHandler to the FIQ handler registry (crate::FIQ). Otherwise, enable IRQ handling of Timer3 interrupt and register a handler that invokes provided pHandler to the global IRQ handler registry (crate::GLOBAL_IRQ).\nRecall that all exception flags are masked by default when an exception handler is called. To handle FIQ interrupt, F flag of PSTATE register should be unmasked. enable_fiq_interrupt() and disable_fiq_interrupt() in aarch64 library give an ability to temporarily enable FIQ interrupt. Enable FIQ handling in following places with these functions:\nWhen handling system calls (kern/src/traps.rs) When handling IRQ interrupts (kern/src/traps.rs) While waiting on the very first switch_to call in start() (kern/src/process/scheduler.rs) Implement Ethernet initialization\nFinish create_interface() in kern/src/net.rs. You should use smoltcp\u0026rsquo;s EthernetInterfaceBuilder . When creating the interface, use UsbEthernet as an inner physical device and MAC address obtained from USPi as Ethernet address of the interface. Then, add an empty neighbor cache using BTreeMap. Finally, add two CIDR blocks as its IP addresses: 169.254.32.10/16 and 127.0.0.1/8. When you are done, implement EthernetDriver::new() using create_interface().\nLink-local Address\n169.254.0.0/16 is a reserved address space for local communication. If two computers are directly connected with an Ethernet cable, two different link-local addresses will be assigned to each of them. In our case, we are assigning a fixed link-local address of 169.254.32.10 to the kernel\u0026rsquo;s network interface.\nHow to setup with router\nIf you have a router, you can connect RPi to the router instead of connecting it directly to the computer.\n1. Record the MAC address of your RPi board.\n2. In router management page (check your router manual to learn how to access it), manually assign an IP to the MAC address of your RPi.\n3. In the network driver, manually assign the IP you chose.\nThe IP address range managed by your router should be in one of these three private network address range.\n- 192.168.0.0 - 192.168.255.255\n- 172.16.0.0 - 172.31.255.255\n- 10.0.0.0 - 10.255.255.255\nThe next step is to initialize USB and ETHERNET in kmain(). Write the following initialization routine after the scheduler initialization:\nEnable FIQ interrupt Initialize USB Initialize ETHERNET Assert if the Ethernet is available with is_eth_available() Poll the Ethernet with is_eth_link_up() and loop until it returns true Disable FIQ interrupt Implement port management APIs\nNext, write port management APIs in EthernetDriver. There are 65535 ports in our kernel, from 1 to 65535. We will manage port availability with port_map field of EthernetDriver, which is an arrray of u64. One u64 integer variable has 64 bits, and you\u0026rsquo;ll use each bit in as an indicator of whether a port is available or not.\nPort permission\nA port number under 1024 are reserved in Linux operating system, and you need root permission to listen on those ports. Since we don\u0026rsquo;t have \u0026ldquo;user\u0026rdquo; concept in RustOS, we will not enforce this limitation.\nNow write mark_port(), erase_port(), and get_ephemeral_port() in kern/src/net.rs. See the comments of the functions and follow the instruction.\nImplement Ethernet polling\nThe next step is to implement Ethernet interface polling. When Ethernet interface is polled with a SocketSet, it should send pending Tx packets in the socket buffer and queue pending Rx packets in the Ethernet to corresponding socket buffer.\nsmoltcp already implements this logic. Use them to implement EthernetDriver::poll() and EthernetDriver::poll_delay() in kern/src/net.rs. You may want to check EthernetInterface documentation. You also need to implement GlobalEthernetDriver::poll(), which is a wrapper function to EthernetDriver:poll(). GlobalEthernetDriver::poll() should be only executed by core 0 in Timer3 handling context. Check these assumptions in addition to the usual wrapper function semantics.\nWhen implementing the check in GlobalEthernetDriver::poll(), you will need to check the core affinity and the preemptive counter.\nRegister Ethernet polling callback\nEthernet driver should be polled regularly to provide continuous network connectivity. Since we have already dedicated Timer1 interrupt to the scheduler tick, we are going to use Usb::start_kernel_timer() for Ethernet polling.\nFirst, implement poll_ethernet() function in kern/src/process/scheduler.rs. Poll the Ethernet interface with GlobalEthernetDriver::poll(), calculate the next poll time with poll_delay(), and register the timeout with Usb::start_kernel_timer(). Once you are finished, register the first timeout in initialize_global_timer_interrupt() with Usb::start_kernel_timer().\nWhen you are done, make sure that USPi and Ethernet initialization in kmain() works correctly, Ethernet interface is polled repetitively, and existing user programs still work. If everything works as expected, continue to the next subphase.\nSubphase C: Process Resource Management In this subphase, you\u0026rsquo;ll add process resource management to our kernel. An operating system kernel needs to track which sockets and files are used by which processes, so that they can be opened, updated, and closed according to processes\u0026rsquo;s requests and their lifetime. In Unix operating systems, actual sockets or files (inode) implementation reside in the kernel, and they are exposed to user programs as a descriptor. The process makes a syscall using a descriptor, which is an index to the list of resources used by the current process. For instance, if a process opens a file with open() syscall, it returns \u0026ldquo;3\u0026rdquo; and the process uses \u0026ldquo;3\u0026rdquo; to refer that file henceforth. (0, 1, 2 are reserved for standard I/O). We will follow this design when implementing process resource management code.\nSocket descriptor design (descriptor-design)\nWhat will be the benefit and the cost of using the index as a descriptor? What will be better or worse if a random token is used instead of an index?\nImplementation Start with adding sockets field of type Vec\u0026lt;SocketHandle\u0026gt; to Process struct in kern/src/process/process.rs. Fix new() appropriately. Then, implement Scheduler::release_process_resources() in kern/src/process/scheduler.rs. Iterate through all sockets held by the current process, close all local ports attached to them, release the socket handle, and prune the Ethernet socket set. Don\u0026rsquo;t forget to call Scheduler::release_process_resources() in Scheduler::kill().\nUse mem::replace to swap out process.sockets.\nThere are two handles in our network stack\nsmoltcp uses SocketHandle to refer a socket in SocketSet. Our kernel uses a socket descriptor to refer a SocketHandle held by a process. Be careful not to confuse these two handles.\nSubphase D: Socket System Calls In this subphase, you will add a number of system calls to our kernel to allow user programs to use the network functionality. You\u0026rsquo;ll implement total 6 socket syscalls. Here is a short summary of them:\nsock_create(): creates a new socket and registers it as current process\u0026rsquo;s resource sock_status(): checks the status of the socket sock_connect(): connects to a remote endpoint with the socket sock_listen(): listens on a local endpoint with the socket sock_send(): sends a packet with a connected socket sock_recv(): receives a packet from a connected socket Implementation Start writing socket related system call handlers in kern/src/traps/syscall.rs. Refer to the comments to understand what those functions do. Except create(), they will have the similar code structure:\nFind a socket handle held by the current process Find a socket from Ethernet socket set using the handle Perform an operation with socket (such as status(), connect(), etc.) Update the scheduler and Ethernet driver accordingly; For example, you should mark the local port number as used when listen() or connect(). Convert the result according to the system call semantics. You will need to revisit scheduler and Ethernet driver code to find necessary APIs for this subphase. Also consult TcpSocket page from smoltcp document to find useful functions.\nUser address validation\nWhen reading from or writing to an address from users, you must ensure that the contents are really inside the user address space. Passing a kernel address through syscalls and trick the kernel to overwrite the kernel memory is a classic attack against operating system kernels.\nThere is no close() syscall\nWe did not include close() syscall as a requirement, but feel free to implement it if you want to.\nsmoltcp socket APIs are non-blocking\nUnlike Berkeley socket API, our socket system calls are non-blocking by default. This is because we are directly wrapping smoltcp APIs as a syscall. In result, instead of getting notified by the kernel when a new connection is established, user programs need to check the socket status in a loop to see if a connection has been established.\nImplement Berkeley-like socket APIs (berkeley-socket)\nHow can you implement Berkeley-like socket APIs in our kernel design? For instance, let\u0026rsquo;s say a user program invokes sock_listen() syscall. How can the kernel suspend a user program until a new connection is established?\nWhen you finish implementing socket related system call handlers in the kernel, write their corresponding user-side system calls in kernel_api/src/syscalls.rs. When implementing them, check the comments in their kernel side handlers for syscall argument and return value specification. Convert wrapper types (e.g., SocketDescriptor, IpAddr, etc.) to u64 and back while implementing them, so that user programs can invoke system calls with more natural types.\nPhase 3: Echo Server The last phase of our long journey is to implement a simple user program that tests the network stack. In this phase, you\u0026rsquo;ll write an echo server, a server that sends back any message that it reads from the client. It is one of the simplest network application. You will mainly work in user/echo/src/main.rs.\nImplementation We have provided a very basic error handling code in main(). Your role is to implement an echo server in main_inner().\nFirst, create a socket with sock_create(). Then make that socket listen on port number 80. Loop until a packet can be sent with the socket. You need to check can_send field in the socket status. Print a message in the loop and sleep for a short period of time (say, 1 sec). Send a welcome message to the client. Inside another loop, receive a packet and send it back through the socket. Also print the message to the console with print!(). When you are done implementing the echo server, add \u0026ldquo;/echo\u0026rdquo; to the scheduler initialization code.\nHow to Test Run make in user/echo to build the program and copy user/echo/build/echo.bin to the SD card. Then, make transmit your kernel (in kern directory). There are two connections to the kernel now: a serial connection and the Ethernet connection. If you screen to the serial connection after transmitting the kernel image, you should see repeated messages which say that the server is waiting for a client connection. The next step is to connect to the echo server with netcat (nc) command.\nDon\u0026rsquo;t forget to copy your user program to the SD card\nmake transmit only transmits the kernel image. Don\u0026rsquo;t forget to copy the user programs to the SD card to make your kernel detect them!\nIf you are using a VM, we recommend you to make transmit in VM but using netcat on your host operating system. Using a link local address is a bit uncommon way to communicate these days, and since the link-local address space is meant to be used only in local setups, there is a possibility that the network translation layer of a virtual machine doesn\u0026rsquo;t work well with it.\nYou may need to install netcat command to your host system. On Windows, netcat seems to work well. On Linux, you might need to configure your ethernet interface to be Link-Local Only in the Network Manager or using ifconfig. Netcat will mostly be pre-installed. If not, you can use sudo apt-get install netcat. On MacOS, you can install netcat with brew install nmap command. If your RPi server wakes up, you can see the light for USB 10/100/1000 LAN (or something similar name) turns to yellow from red in System Preferences - Network with its IP address.\nIf everything is ready, type nc 169.254.32.10 80 (or ncat on MacOS) on your computer to connect to the echo server. It would take a while for RPi to be recognized by the operating system, so the first few tries might fail. When connected, the echo server will print a welcome message, and anything you type into the shell will go through the Ethernet cable and come back in a while.\nSubmission Once you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Congratulations!\nYou can call make check in tut/5-multicore directory to check if you\u0026rsquo;ve answered every question.\nOnce you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Ensure you\u0026rsquo;ve committed your changes. Any uncommitted changes will not be visible to us, thus unconsidered for grading.\nWhen you\u0026rsquo;re ready, push a commit to your GitHub repository with a tag named lab5-done.\n# submit lab5 $ git tag lab5-done $ git push --tags ", 
        "url": "\/\/localhost:1313\/post\/lab5\/"
    },
    
    "\/\/localhost:1313\/tags\/": {
        "title": "Tags",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/"
    },
    
    "\/\/localhost:1313\/post\/lab4\/": {
        "title": "Lab 4: Preemptive Multitasking",
        "tags": ["Lab",],
        "content": " Handed out: Tuesday, March 3, 2020 Due: Monday, April 13, 2020 Introduction In this assignment, you will enable user-level applications by implementing processes and related infrastructure. You will write privilege level switching code, context-switching code, a simple round-robin scheduler, system call handlers, and a virtual memory subsystem. You will also write several user programs and load them to start new processes.\nPhase 0: Getting Started Fetch the update for lab 4 from our git repository to your development machine.\n$ git fetch skeleton $ git merge skeleton/lab4 This is the directory structure of our repository. The directories you will be working on this assignment are marked with *.\n. ├── bin : common binaries/utilities ├── doc : reference documents ├── ext : external files (e.g., resources for testing) ├── tut : tutorial/practices │ ├── 0-rustlings │ ├── 1-blinky │ ├── 2-shell │ ├── 3-fs │ └── 4-spawn : questions for lab4 * ├── boot : bootloader ├── kern : the main os kernel * ├── lib : required libraries │ ├── aarch * │ ├── kernel_api * │ ├── fat32 │ ├── pi │ ├── shim │ ├── stack-vec │ ├── ttywrite │ ├── volatile │ └── xmodem └── user : user level program * ├── fib * └── sleep *` You may need to resolve conflicts before continuing. For example, if you see a message that looks like:\nAuto-merging kern/src/main.rs CONFLICT (content): Merge conflict in kern/src/main.rs Automatic merge failed; fix conflicts and then commit the result. You will need to manually modify the main.rs file to resolve the conflict. Ensure you keep all of your changes from lab 3. Once all conflicts are resolved, add the resolved files with git add and commit. For more information on resolving merge conflicts, see this tutorial on githowto.com .\nARM Documentation Throughout this assignment, we will be referring to three official ARM documents. They are:\nARMv8 Reference Manual This is the official reference manual for the ARMv8 architecture. This is a wholistic manual covering the entire architecture in a general manner. For the specific implementation of the architecture for the Raspberry Pi 3, see the ARM Cortex-A53 Manual. We will be referring to sections from this manual with notes of the form (ref : C5.2) which indicates that you should refer to section C5.2 of the ARMv8 Reference Manual .\nARM Cortex-A53 Manual Manual for the specific implementation of the ARMv8 (v8.0-A) architecture as used by the Raspberry Pi 3. We will be referring to sections from this manual with notes of the form (A53 : 4.3.30) which indicates that you should refer to section 4.3.30 of the ARM Cortex-A53 Manual .\nARMv8-A Programmer Guide A high-level guide on how to program an ARMv8-A process. We will be referring to sections from this manual with notes of the form (guide : 10.1) which indicates that you should refer to section 10.1 of the ARMv8-A Programmer Guide .\nYou can find all those three documents under doc/ subdirectory of our lab repo. We recommend that you download these three documents now and maintain them within easy reach.\nPhase 1: ARM and a Leg In this phase, you will learn about the ARMv8 architecture, switch to a lower privilege level, install exception vectors, enable timer interrupts, and handle breakpoint exceptions by starting a debug shell. You will learn about exception levels in the ARM architecture and how the architecture handles exceptions, interrupts, and privilege levels.\nSubphase A: ARMv8 Overview In this subphase, you will learn about the ARMv8 architecture. You will not be writing any code, but you will be answering several questions about the architecture.\nThe ARM (Acorn RISC Machine) CPU architecture has a history spanning over 30 years. There are eight major revisions, the latest being ARMv8-A, introduced in 2011. The Broadcom BCM2837 SOC contains an ARM Cortex-A53, an ARMv8.0-A based CPU. The Cortex-A53 (and other specific CPUs) are referred to as implementations of the architecture. This is the CPU you have been programming in the last three assignments.\nARM CPUs dominate the mobile market.\nARM CPUs dominate the mobile market, appearing in over 95% of all smartphones sold worldwide and almost 100% of all flagship smartphones including Apple\u0026rsquo;s iPhone and Google\u0026rsquo;s Pixel.\nThus far, we\u0026rsquo;ve avoided the details of the underlying architecture, allowing the Rust compiler to handle them for us. To enable running processes in user-space, however, we\u0026rsquo;ll need to program the CPU directly at the lowest of levels. Programming the CPU directly requires familiarization with the CPUs native assembly language and overall concepts. We\u0026rsquo;ll start with an overview of the architecture and then proceed to describe a few key assembly instructions.\nRegisters The ARMv8 architecture includes the following registers (ref : B1.2.1):\nr0 \u0026hellip; r30 - 64-bit general purpose registers\nThese registers are accessed via aliases. The registers x0 \u0026hellip; x30 alias all 64-bits of these registers. The registers w0 \u0026hellip; w30 alias the least-significant 32-bits of these registers.\nlr - 64-bit link register; aliases x30\nUsed to store the link address. The bl \u0026lt;addr\u0026gt; instruction stores the address of the next instruction in lr and branches to addr. The ret instruction sets the PC to the address in lr.\nsp - a dedicated stack pointer\nThe lower 32 bits of the stack-pointer can be accessed via wsp. The stack pointer must always be 16-byte aligned.\npc - the program counter\nThis register can be read but not written. The pc is updated on branching instructions and exception entry/return.\nv0 \u0026hellip; v31 - 128-bit SIMD and FP point registers\nThese registers are used for vectorizing SIMD instruction and floating point operations. These registers are accessed via aliases. The registers q0 \u0026hellip; q31 alias all 128-bits of these registers. The registers d0 \u0026hellip; d31 alias the lower 64-bits of these registers. There are also alias for the lower 32, 16, and 8 bits of these registers prefixed with s, h, and b, respectively.\nxzr - read-only zero register\nThis pseudo-register, which may or may not be a hardware register, always holds the value 0.\nThere are also many special-purpose registers. We\u0026rsquo;ll describe these as needed, as in the next section.\nPSTATE At any point in time, an ARMv8 CPU captures the program state in a pseudo-register named PSTATE (ref : D1.7). PSTATE isn\u0026rsquo;t a real register; there\u0026rsquo;s no way to read or write it directly. Instead, there are special purpose registers that can be used to read or write the various fields of the PSTATE pseudo-register. On ARMv8.0, these are:\nNZCV - condition flags DAIF - exception mask bits, used to prevent exceptions from being issued CurrentEL - the current exception level (explained later) SPSel - stack pointer selector These registers belong to the class of registers known as system registers or special registers (ref : C5.2). Typically, registers can be loaded into from memory (written) using the ldr instruction and stored in memory (read) using the str instruction. System registers cannot be read/written with these instructions. Instead, the special purpose instructions mrs and msr must be used (ref : C6.2.162 - C6.2.164). For example to read NZCV into x1, you can issue the instruction:\nmrs x1, NZCV Execution State At any point in time, an ARMv8 CPU is executing in a given execution state. There are two such execution states: AArch32, corresponding to 32-bit ARMv7 compatibility mode, and AArch64, 64-bit ARMv8 mode (guide : 3.1). We\u0026rsquo;ll always be executing in AArch64.\nSecure Mode At any point in time, an ARMv8 CPU is executing in a given security state, otherwise known as a security mode or security world. There are two security states: secure, and non-secure, also known as normal. We\u0026rsquo;ll always be executing in non-secure (normal) mode.\nException Levels At any point in time, an ARMv8 CPU is executing at a given exception level (guide : 3). Each exception level corresponds to a privilege level: the higher the exception level, the greater the privileges programs running at that level have. There are 4 exception levels:\nEL0 (user) - Typically used to run untrusted user applications.\nEL1 (kernel) - Typically used to run privileged operating system kernels.\nEL2 (hypervisor) - Typically used to run virtual machine hypervisors.\nEL3 (monitor) - Typically used to run low-level firmware.\nThe Raspberry Pi\u0026rsquo;s CPU boots into EL3. At that point, the firmware provided by the Raspberry Pi foundation runs, switches to EL2, and runs our kernel8.img file. Thus, our kernel starts executing in EL2. Later, you\u0026rsquo;ll switch from EL2 to EL1 so that our kernel is running in the appropriate exception level.\nELx Registers Several system registers such as ELR, SPSR, and SP are duplicated for each exception level. The register names are suffixed with _ELn to indicate the register for exception level n. For instance, ELR_EL1 is the exception link register for EL1, while ELR_EL2 is the exception link register for EL2.\nWe use the suffix x, such as in ELR_ELx, when we refer to a register from the target exception level x. The target exception level is the exception level the CPU will switch to, if necessary, to run the exception vector. We use the suffix s, such as in SP_ELs, when we refer to a register in the source exception level s. The source exception level is the exception level in which the CPU was executing when the exception occurred.\nSwitching Exception Levels There is exactly one mechanism to increase the exception level and exactly one mechanism to decrease the exception level.\nTo switch from a higher level to a lower level (a privilege decrease), the running program must return from the exception level using the eret instruction (ref : D1.11). On executing an eret instruction when the current exception level is ELx, the CPU:\nSets the PC to the value in ELR_ELx, a special purpose system-register.\nSets the PSTATE to the values in SPSR_ELx, a special purpose system-register.\nThe SPSR_ELx register (ref : C5.2.18) also contains the exception level to return to. Note that changing exception levels also has the following implications:\nOn return to ELs, the sp is set to SP_ELs if SPSR_ELx[0] == 1 or SP_EL0 if SPSR_ELx[0] == 0. Switching from a lower level to a higher level only occurs as a result of an exception (guide : 10). Unless otherwise configured, the CPU will trap exceptions to the next exception level. For instance, if an interrupt is received while running in EL0, the CPU will switch to EL1 to handle the exception. When a switch to ELx occurs, the CPU will:\nMask all exceptions and interrupts by setting PSTATE.DAIF = 0b1111. Save PSTATE and other fields to SPSR_ELx. Save the preferred exception link address to ELR_ELx (ref : D1.10.1). Set sp to SP_ELx if SPSel was set to 1. Save the exception syndrome (described later) to ESR_ELx (ref : D1.10.4). Set pc to address corresponding to the exception vector (described later). Note that the exception syndrome register is only valid when the exception was synchronous (described next). All general purpose and SIMD/FP registers will maintain the value they had when the exception occurred.\nException Vectors When an exception occurs, the CPU jumps to the exception vector for that exception (ref : D1.10.2). There are 4 types of exceptions each with 4 possible exception sources for a total of 16 exception vectors. The four types of exceptions are:\nSynchronous - an exception resulting from an instruction like svc or brk IRQ - an asynchronous interrupt request from an external source FIQ - an asynchronous fast interrupt request from an external source SError - a \u0026ldquo;system error\u0026rdquo; interrupt The four sources are:\nSame exception level when source SP = SP_EL0 Same exception level when source SP = SP_ELx Lower exception level running on AArch64 Lower exception level running on AArch32 As described in (guide : 10.4):\nWhen an exception occurs, the processor must execute handler code which corresponds to the exception. The location in memory where [an exception] handler is stored is called the exception vector. In the ARM architecture, exception vectors are stored in a table, called the exception vector table. Each exception level has its own vector table, that is, there is one for each of EL3, EL2 and EL1. The table contains instructions to be executed, rather than a set of addresses [as in x86]. Each entry in the vector table is 16 instructions long. Vectors for individual exceptions are located at fixed offsets from the beginning of the table. The virtual address of each table base is set by the [special-purpose] Vector Based Address Registers VBAR_EL3, VBAR_EL2 and VBAR_EL1.\nThe vectors are physically laid out as follows:\nTarget and source at same exception level with source SP = SP_EL0:\nVBAR_ELx Offset Exception 0x000 Synchronous exception 0x080 IRQ 0x100 FIQ 0x180 SError Target and source at same exception level with source SP = SP_ELx:\nVBAR_ELx Offset Exception 0x200 Synchronous exception 0x280 IRQ 0x300 FIQ 0x380 SError Source is at lower exception level running on AArch64\nVBAR_ELx Offset Exception 0x400 Synchronous exception 0x480 IRQ 0x500 FIQ 0x580 SError Source is at lower exception level running on AArch32\nVBAR_ELx Offset Exception 0x600 Synchronous exception 0x680 IRQ 0x700 FIQ 0x780 SError The vector table is contiguous.\nInterface with Rust We\u0026rsquo;ve provided aarch64 library (lib/aarch64) to provide Rusty interface in order to access low-level details about the system. Before moving to next subphase, please compare your understanding with registers defined in regs.rs. The other files in the library will be revisited at the end of the next subphase.\nRecap For now, this is all of the ARMv8 architecture that you need to know. Before continuing, answer the following questions:\nWhich registers alias x30? (arm-x30)\nIf a value of 0xFFFF is written to x30, which two other registers names can be used to retrieve that value?\nHow would you set the PC to a specific address? (arm-pc)\nHow would you set the PC to the address A using the ret instruction? How would you set the PC to the address A using the eret instruction? Be specific about which registers you would set to which values.\nHow would you determine the current exception level? (arm-el)\nWhich instructions, exactly, would you run to determine the current exception level?\nHow would you change the stack pointer on exception return? (arm-sp-el)\nThe stack pointer of a running program is A when an exception occurs. After handling the exception, you\u0026rsquo;d like to return back to where the program was executing but want to change its stack pointer to B. How would you do so?\nWhich vector is used for system calls from a lower EL? (arm-svc)\nA process is running in EL0 when it issues an svc instruction. To which address, exactly, does the CPU jump to?\nWhich vector is used for interrupts from a lower EL? (arm-int)\nA process is running in EL0 when a timer interrupt occurs. To which address, exactly, does the CPU jump to?\nHow do you unmask IRQ exceptions? (arm-mask)\nWhich values would you write to which register to unmask IRQ interrupts only?\nHow would you eret into an AArch32 execution state? (arm-aarch32)\nAn exception has occurred with the source running in the AArch64 state. The target is also running in AArch64. Which values in which registers would you change so that on return from the exception via eret, the CPU switches to the AArch32 execution state?\nSee (guide : 10.1).\nSubphase B: Instructions In this subphase, you will learn about the ARMv8 instruction set. You will not be writing any code, but you will be answering several questions about the instruction set.\nAccessing Memory ARMv8 is a load/store RISC (reduced instruction set computer) instruction set. Perhaps the defining feature of such an instruction set is that memory can only be accessed through specific instructions. In particular, memory can only be read by reading into a register with a load instruction and written to memory by storing from a register using a store instruction.\nThere are many load and store instructions and variations of particular instructions. We\u0026rsquo;ll start with the simplest:\nldr \u0026lt;ra\u0026gt;, [\u0026lt;rb\u0026gt;]: load value from address in \u0026lt;rb\u0026gt; into \u0026lt;ra\u0026gt; str \u0026lt;ra\u0026gt;, [\u0026lt;rb\u0026gt;]: store value in \u0026lt;ra\u0026gt; to address in \u0026lt;rb\u0026gt; The register \u0026lt;rb\u0026gt; is known as the base register. Thus, if r3 = 0x1234, then:\nldr r0, [r3] // r0 = *r3 (i.e., r0 = *(0x1234)) str r0, [r3] // *r3 = r0 (i.e., *(0x1234) = r0) You can also provide an offset in the range [-256, 255]:\nldr r0, [r3, #64] // r0 = *(r3 + 64) str r0, [r3, #-12] // *(r3 - 12) = r0 You can also provide a post-index that changes the value in the base register after the load or store has been applied:\nldr r0, [r3], #30 // r0 = *r3; r3 += 30 str r0, [r3], #-12 // *r3 = r0; r3 -= 12 You can also provide a pre-index that changes the value in the base register before the load or store has been applied:\nldr r0, [r3, #30]! // r3 += 30; r0 = *r3 str r0, [r3, #-12]! // r3 -= 12; *r3 = r0 Offset, post-index, and pre-index are known as addressing modes.\nFinally, you can load and store from two registers at once using the ldp and stp (load pair, store pair) instructions. These instructions can be used with all of the same addressing modes as ldr and str:\n// push `x0` and `x1` onto the stack. after this operation the stack is: // // |------| \u0026lt;x (original SP) // | x1 | // |------| // | x0 | // |------| \u0026lt;- SP // stp x0, x1, [SP, #-16]! // pop `x0` and `x1` from the stack. after this operation, the stack is: // // |------| \u0026lt;- SP // | x1 | // |------| // | x0 | // |------| \u0026lt;x (original SP) // ldp x0, x1, [SP], #16 // these four operations perform the same thing as the previous two sub SP, SP, #16 stp x0, x1, [SP] ldp x0, x1, [SP] add SP, SP, #16 // same as before, but we are saving and restoring all of x0, x1, x2, and x3. sub SP, SP, #32 stp x0, x1, [SP] stp x2, x3, [SP, #16] ldp x0, x1, [SP] ldp x2, x3, [SP, #16] add SP, SP, #32 Loading Immediates An immediate is another name for an integer whose value is known without any computation. To load a 16-bit immediate into a register, optionally shifted to the left by a multiple of 16-bits, use mov (move). To load a 16-bit immediate shifted by left some number of bits without replacing any of the other bits, use the movk (move/keep) instruction. An example of their usage:\nmov x0, #0xABCD, LSL #32 // x0 = 0xABCD00000000 mov x0, #0x1234, LSL #16 // x0 = 0x12340000 mov x1, #0xBEEF // x1 = 0xBEEF movk x1, #0xDEAD, LSL #16 // x1 = 0xDEADBEEF movk x1, #0xF00D, LSL #32 // x1 = 0xF00DDEADBEEF movk x1, #0xFEED, LSL #48 // x1 = 0xFEEDF00DDEADBEEF Note that immediates are prefixed with a #, that the destination register appears to the left, and that LSL specifies the left shift.\nOnly 16-bit immediates with optional shifts can be loaded into a register. The assembler is able to figure out the right shift value in many cases. For instance, the assembler is able to convert mov x12, #(1 \u0026lt;\u0026lt; 21) into a mov x12, 0x20, LSL #16 automatically.\nLoading Addresses from Labels Sections of assembly code can be labled using \u0026lt;label\u0026gt;::\nadd_30: add x1, x1, #10 add x1, x1, #20 To load the address of the first instruction after the label, you can either use the adr or ldr instructions:\nadr x0, add_30 // x0 = address of first instruction of add_30 ldr x0, =add_30 // x0 = address of first instruction of add_30 You must use ldr if the label is not within the same linker section as the instruction. If the label is within the same section, you should use adr.\nMoving Between Registers You can move values between registers with the mov instruction as well:\nmov x13, #23 // x13 = 23 mov sp, x13 // sp = 23, x13 = 23 Loading from Special Registers Special and system registers such as ELR_EL1 can only be loaded/stored from other registers using the mrs and msr instruction.\nTo write to a special register from another register, use msr:\nmsr ELR_EL1, x1 // ELR_EL1 = x1 To read from a special register into another register, use mrs:\nmrs x0, CurrentEL // x0 = CurrentEL Arithmetic The add and sub instruction can be used to perform arithmetic. The syntax is:\nadd \u0026lt;dest\u0026gt; \u0026lt;a\u0026gt; \u0026lt;b\u0026gt; // dest = a + b sub \u0026lt;dest\u0026gt; \u0026lt;a\u0026gt; \u0026lt;b\u0026gt; // dest = a - b For example:\nmov x2, #24 mov x3, #36 add x1, x2, x3 // x1 = 24 + 36 = 60 sub x4, x3, x2 // x4 = 36 - 24 = 12 The parameter \u0026lt;b\u0026gt; can also be an immediate:\nsub sp, sp, #120 // sp -= 120 add x3, x1, #120 // x3 = x1 + 120 add x3, x3, #88 // x3 += 88 Logical Instructions The and and orr instruction perform bitwise AND and OR. Their usage is identical to that of add and sub:\nmov x1, 0b11001 mov x2, 0b10101 and x3, x1, x2 // x3 = x1 \u0026amp; x2 = 0b10001 orr x3, x1, x2 // x3 = x1 | x2 = 0b11101 orr x1, x1, x2 // x1 |= x2 and x2, x2, x1 // x2 \u0026amp;= x1 and x1, x1, #0b110 // x1 \u0026amp;= 0b110 orr x1, x1, #0b101 // x1 |= 0b101 Branching Branching is another term for jumping to an address. A branch changes the PC to a given address or address of a label. To unconditionally jump to a label, use the b instruction:\nb label // jump to label To jump to a label while storing the next address in the link register, use bl. The ret instruction jumps to the address in lr:\nmy_function: add x0, x0, x1 ret mov x0, #4 mov x1, #30 bl my_function // lr = address of `mov x3, x0` mov x3, x0 // x3 = x0 = 4 + 30 = 34 The br and blr instruction are the same as b and bl, respectively, but jump to an address contained in a register:\nldr x0, =label blr x0 // identical to bl label br x0 // identical to b label Conditional Branching The cmp instruction compares values in two registers or a register and an immediate and sets flags for future conditional branching instructions such as bne (branch not equal), beq (branch if equal), blt (branch if less than), and so on (ref : C1.2.4):\n// add 1 to x0 until it equals x1, then call `function_when_eq`, then exit not_equal: add x0, x0, #1 cmp x0, x1 bne not_equal bl function_when_eq exit: ... // called when x0 == x1 function_when_eq: ret Using an immediate:\ncmp x1, #0 beq x1_is_eq_to_zero Note that if the branch is not taken, execution simply continues forward.\nInterface with Rust Now go to the aarch64 subdirectory again and start reviewing the remaining files. Here is brief description of each files in the library.\nasm.rs - Internally use of inline assembly to wrap as a function. macros.rs - Define macros used in this library regs.rs - Define registers and provide their interface using macros sp.rs - Access to the stack pointer vmsa.rs - Support virtual memory (described later) lib.rs - Include above modules; Has some useful functions such as current_el(). As an example, the get() and set() function in sp.rs allows you retrieve and change the current stack pointer at any point in time. Similarly, the current_el() function in lib.rs returns the exception level the CPU is currently executing in, otherwise known as the current exception level.\nRecap There are many more instructions in the ARMv8 instruction set. With these as a basis, you should be able to pick up most of the remaining instructions with ease. The instructions are documented in (ref : C3). For a concise reference of the instructions presented above, see this ISA cheat sheet by Griffin Dietz. Before continuing, answer the following questions:\nHow would you write memcpy in ARMv8 assembly? (asm-memcpy)\nAssuming that the source address is in x0, the destination address is in x1, and the number of bytes to copy is in x2, which is guaranteed to be a non-zero multiple of 8, how would you implement memcpy in ARMv8 assembly? Ensure you ret.\nYou can implement this in just 6 or 7 lines.\nHow would you write 0xABCDE to ELR_EL1? (asm-movk)\nAssume you\u0026rsquo;re running in EL1, how would you write the immediate 0xABCDE to ELR_EL1 using ARMv8 assembly?\nYou\u0026rsquo;ll need three instruction.\nWhat does the cbz instruction do? (asm-cbz)\nRead the documentation for the cbz instruction in (ref : C6.2.36). What does the instruction do? How would you use it?\nWhat does init.rs do? (init)\nUp until lab 3, we used kern/src/init/init.s as the first piece of code that runs in our kernel. kern/src/init.rs now replaced that role. In particular, the _start function will be at address 0x80000 when the Raspberry Pi boots, and the firmware will jump to this address as soon as it is done initializing the system. Soon, you\u0026rsquo;ll modify this file to switch to EL1 and setup exception vectors.\nRead all of the code in kern/src/init.rs. Then, for every function in the file, explain what the code is doing. For example, to explain the _start function, we might say:\n\u0026ldquo;The [7:0] bits of the MPIDR_EL1 register (ref : D7.2.74) are read (Aff0), yielding the core number that\u0026rsquo;s currently executing code. If the number is zero, KERN_STACK_BASE(0x80_000) is set to stack pointer and kinit() is called.\nRefer to the manual for any instruction/register you\u0026rsquo;re not familiar with yet.\naarch64 library and kern/src/param.rs will be needed while reviewing init.rs\nSubphase C: Switching to EL1 In this subphase, you will write the Rust code to switch from EL2 to EL1. You will be working primarily in kern/src/init.rs and kern/src/main.rs. We recommend that you only proceed to this subphase after you have answered the questions from the previous subphases.\nCurrent Exception Level As mentioned before, the CPU should be running in EL2 when our kernel is called. Confirm this now by printing the current exception level in kmain(). Note that you\u0026rsquo;ll need to use unsafe to call current_el(); we\u0026rsquo;ll remove this call once we\u0026rsquo;ve confirmed that we\u0026rsquo;ve switched to EL1 successfully. current_el() is in the lib/aarch64 library and you need to add it to solve dependency issue.\nSwitching Now, you\u0026rsquo;ll finish writing the assembly to switch to EL1. Find the line in kern/src/init.rs marked:\n// FIXME: eret to itself, expecting current_el() == 1 this time Above this line, you\u0026rsquo;ll see following code.\nSPSR_EL2.set( (SPSR_EL2::M \u0026amp; 0b0101) | SPSR_EL2::F | SPSR_EL2::I | SPSR_EL2::D | SPSR_EL2::A, ); From the previous subphase, you should know what these do. In particular, you should know which bits are being set in SPSR_EL2 and what the implications will be if an eret occurs thereafter.\nComplete the switching routine now by replacing the FIXME with the proper code. Ensure that on the switch to EL1, the CPU jumps to switch_to_el1 recursively with the proper exception level to bypass the internal check current_el() == 2 and just move on to kmain(). You\u0026rsquo;ll need exactly two lines of code to complete the routine. Recall that the only way to decrease the exception level is via an eret. Once you have completed the routine, ensure that current_el() now returns 1.\nWhich register is used to set the PC on exception return?\nSubphase D: Exception Vectors In this subphase, you\u0026rsquo;ll setup and install exception vectors and an exception handler. This will be the first step towards enabling your kernel to handle arbitrary exceptions and interrupts. You\u0026rsquo;ll test your exception vector and exception handling code by implementing a tiny debugger that starts in response to a brk #n instruction. You will be working primarily in kernel/src/init/vectors.s, kernel/src/traps.rs and the kernel/src/traps directory.\nOverview Recall that the vector table consists of 16 vectors, where each vector is a series of at most 16 instructions. We\u0026rsquo;ve set apart space in vectors.s for these vectors and have placed the vectors label at the base of the table. This vectors.s file will be included to init.rs with global_asm! macro. Your task will be to populate the table with the 16 vectors such that, ultimately, the handle_exception Rust function in kernel/src/traps.rs is called with the proper arguments when an exception occurs. All exceptions will be routed to the handle_exception function. The function will determine why an exception has occurred and dispatch the exception to higher-level handlers as needed.\nCalling Convention In order to properly call the handle_exception function declared in Rust, we must know how the function expects to be called. In particular, we must know where the function should expect to find the values for its parameters info, esr, and tf, what it promises about the state of the machine after the function is called, and how it will return to the site of the function call.\nThis problem of knowing how to call foreign functions arises whenever one language calls into another (as in lab 3 between C and Rust). Instead of having to know how every language expects its functions to be called, calling conventions are established. A calling convention, or procedure call standard, is a set of rules that dictates the following:\nHow to pass parameters to a function.\nOn AArch64, the first 8 parameters are passed in registers r0 \u0026hellip; r7, in that order from left-to-right.\nHow to return values from a function.\nOn AArch64, the first 8 return values are passed in registers r0 \u0026hellip; r7.\nWhich state (registers, stack, etc.) the function must preserve.\nRegisters are usually categorized as either caller-saved or callee-saved.\nA caller-saved register is not guaranteed to be preserved across a function call. Thus, if the caller requires the value in the register to be preserved, it must save the register\u0026rsquo;s value before calling the function.\nOn the contrary, a callee-saved register is guaranteed to be preserved across a function call. Thus, if a callee wishes to use the register during the function call, it must save the register\u0026rsquo;s value before doing so and restore it before returning.\nRegister values are typically saved and restored by pushing and popping from the stack.\nOn AArch64, registers r19 \u0026hellip; r29 and SP are callee-saved. The remaining general purpose registers are caller-saved. Note that this includes lr (x30). SIMD/FP registers have complicated saving rules. For our purposes, it suffices to say that they are all caller-saved.\nHow to return to the caller.\nOn AArch64, the lr register holds the link address: the address the callee should jump to when it returns. The ret instruction branches to lr, so it often terminates a function.\nThe AArch64 calling convention is described in (guide : 9) as well as in the official procedure call standard documentation. When you call the handle_exception Rust function from assembly, you\u0026rsquo;ll need to ensure that you adhere to this calling convention.\nHow does Rust know which convention to use?\nStrictly adhering to a calling convention precludes all kinds of optimizations on function calls and bodies. As a result, Rust\u0026rsquo;s functions are not guaranteed to abide to a particular calling convention by default. To force Rust to compile a function exactly according to the calling convention of the target platform, the extern function qualifier can be used. We\u0026rsquo;ve already declared handle_exception as extern, so we can be sure that Rust will compile the function appropriately.\nVector Table To help you populate the vector table, we\u0026rsquo;ve provided the HANDLER source, kind macro which expands to a series of 8 instructions aligned to the next valid vector entry. When HANDLER a, b is used as an \u0026ldquo;instruction\u0026rdquo;, it expands to the lines that reside between .macro and .endm. In other words, this:\nvectors: HANDLER 32, 39 Expands to the following:\nvectors: .align 7 stp lr, xzr, [SP, #-16]! stp x28, x29, [SP, #-16]! mov x29, #32 movk x29, #39, LSL #16 bl context_save ldp x28, x29, [SP], #16 ldp lr, xzr, [SP], #16 eret The expanded code pushes lr, xzr, x28 and x29 to the stack, creates a 32-bit value in x29 where the lower 16-bits are source and the upper 16-bits are kind, and calls the context_save assembly function (declared above vectors in vectors.s). When that function returns, it restores saved four registers from the stack and finally returns from the exception.\nThe context_save function currently does nothing: it simply falls through to a ret from context_restore below. Soon, you will modify the context_save function so that it correctly calls the handle_exception Rust function.\nSyndrome When a synchronous exception occurs (an exception caused by the execution or attempted execution of an instruction), the CPU sets a value in a syndrome register (ESR_ELx) that describes the cause of the exception (ref : D1.10.4). We\u0026rsquo;ve set up structures in kernel/src/traps/syndrome.rs that should parse the syndrome value into a meaningful Syndrome enum. You will soon write code that passes the ESR_ELx value to the Rust function as the esr parameter. You\u0026rsquo;ll then use Syndrome::from(esr) to parse the syndrome value which determines what to do next.\nInfo The handle_exception Rust function takes an Info structure as the first parameter. The structure has two 16-bit fields: the first corresponds to the source, and the second corresponds to the kind of exception. As you may have guessed, this is exactly the 32-bit value that the HANDLE macro sets up in x29. You\u0026rsquo;ll need to move this x29 to x0 within context_save code block in order to pass it as the first parameter. In addition, please ensure that you use the correct HANDLE invocations for the correct entries so that the Info structure is correctly created.\nImplementation You\u0026rsquo;re now ready to implement preliminary exception handling code. The first exception you will handle is the brk exception (a software breakpoint). When such an exception occurs, you\u0026rsquo;ll start up a shell that would theoretically allow you explore the state of the machine at that point in its execution.\nStart by inserting a call to brk in main.rs. Instead of using inline assembly, you can call brk macro defined in asm.rs of aarch64 library.\nThen, proceed as follows:\nPopulate the vectors table using the HANDLE macro.\nEnsure that your entries would correctly create the Info structure. Refer to (guide : 10.4) to check the entry order. The source and kind of handler should be matched with Source and Kind enum in src/traps.rs.\nCall the handle_exception function in context_save.\nEnsure that you save/restore any caller-saved registers as needed and that you pass the appropriate parameters. For now, you can pass in 0 for the tf parameter; we\u0026rsquo;ll be using this later. Refer src/traps.rs to see what to pass for arguments.\nAArch64 requires the SP register to be 16-byte aligned whenever it is used as part of a load or store. Ensure that you keep SP 16-byte aligned at all times.\nSetup the correct VBAR register at the comment marked in init.rs.\n// FIXME: load `vectors` addr into appropriate register (guide: 10.4) At this point, your handle_exception function should be called whenever an exception occurs.\nIn handle_exception, print the value of the info and esr parameters and ensure that they are what you expect. Then, loop endlessly in the handler. You\u0026rsquo;ll want to call aarch64::nop() in the loop to ensure it doesn\u0026rsquo;t get optimized away. We will need to write more code to properly return from the exception handler, so we\u0026rsquo;ll simply loop for now. We will fix this in the next subphase.\nImplement the Syndrome::from() and the Fault::from() methods.\nThe former should call the latter. You\u0026rsquo;ll need to refer to (ref : D1.10.4, ref : Table D1-8) to implement these correctly. Clicking on the \u0026ldquo;ISS encoding description\u0026rdquo; in the table gives you details about how to decode the syndrome for a particular exception class as well as decode for Fault. You should ensure, for example, that a brk 12 is decoded as Syndrome::Brk(12). Similarly, a svc 77 should be parsed as a Syndrome::Svc(77). Note that we have excluded the 32-bit variants of some exceptions and coalesced exceptions when they are identical but occur with differing exception classes.\nUse aarch64 library!\nInstead of using inline assembly and raw bit operation for each register, use pre-defined registers and functions in aarch64 library to your advantage. You will find REG_NAME::get_value(raw_value, REG_NAME::MASK) function useful.\nStart a shell when a brk exception occurs.\nUse your Syndrome::from() method in handle_exception to detect a brk exception. When such an exception occurs, start a shell. You may wish to use a different shell prefix to differentiate between shells. Note that you should only call Syndrome::from() for synchronous exceptions. The ESR_ELx register is not guaranteed to hold a valid value otherwise.\nAt this point, you\u0026rsquo;ll also need to modify your shell to implement a new command: exit. When exit is called, your shell should end its loop and return. This will allow us to exit from a brk exception later. Because of this change, you\u0026rsquo;ll also need to wrap your invocation to shell() in kmain in a loop { } to prevent your kernel from exiting and crashing.\nOnce you are finished, the brk 2 instruction in kmain should result in an exception with syndrome Brk(2), source CurrentSpElx, and kind Synchronous being routed to the handle_exception function. At that point, a debug shell should start. When exit is called from the shell, the shell should terminate, and the exception handler should begin to loop endlessly.\nBefore proceeding, you should ensure that you detect other synchronous exceptions correctly. You should try calling other exception-causing instructions such as svc 3. You should also try purposefully causing a data or instruction abort by jumping to an address outside of the physical memory range.\nOnce everything works as expected, you\u0026rsquo;re ready to proceed to the next phase.\nSubphase E: Exception Return In this subphase, you will write the code to enable correct returns from an exception of any kind. You will be working primarily in kern/init/vectors.s, kern/src/traps.rs and the kern/src/traps directory.\nOverview If you try removing the endless loop in handle_exception now, your Raspberry Pi will likely enter an exception loop, where it repeatedly enters the exception handler, or crash entirely when you exit from your debug shell. This is because when your exception handler returns to whatever code was running previously, the state of the processor (its registers, primarily) has changed without the code accounting for it.\nAs an example, consider the following assembly:\n1: mov x3, #127 2: mov x4, #127 3: brk 10 4: cmp x3, x4 5: beq safety 6: b oh_no When the brk exception occurs, your exception vector will be called, eventually calling handle_exception. The handle_exception function, as compiled by Rust, will make use of the x3 and x4 registers (among others) for processing. If your exception handler returns to the site of the brk call, the state of x3 and x4 is unknown, and the beq safety instruction on line 5 is not guaranteed to branch to safety.\nAs a result, in order for our exception handler to be able to use the machine as it desires, we\u0026rsquo;ll need to ensure that we save all of the processing context (the registers, etc.) before we call our exception handler. Then, when the handler returns, we\u0026rsquo;ll need to restore the processing context so that the previously executing code continues to execute flawlessly. This process of saving and restoring a processing context is known as a context switch.\nWhat makes it a context switch?\nThe inclusion of the word switch can be a bit deceiving. After all, aren\u0026rsquo;t we simply returning to the same context?\nIn fact, we rarely want to return to the same context. Instead, we typically want to modify the context before we return so that the CPU executes things just a little bit differently. For example, when we implement process switching, we\u0026rsquo;ll swap out the context of one process for another\u0026rsquo;s, multiplexing CPU time. When we implement system calls, we\u0026rsquo;ll modify the values of registers to inject return values. And finally, when we return from a breakpoint exception, we\u0026rsquo;ll need to modify the return address in ELR so that the next instruction executes instead of the same one.\nSoon, you\u0026rsquo;ll write the code to save all of the processing context into a structure known as a trap frame. You\u0026rsquo;ll finish the definition of the TrapFrame structure in kern/src/traps/frame.rs so that you can access and modify the trap frame from Rust, and you\u0026rsquo;ll write the assembly to save and restore the trap frame as well as pass a pointer to the trap frame to the handle_exception function in the tf parameter.\nTrap Frame The trap frame is the name we give to the structure that holds all of the processing context. The name \u0026ldquo;trap frame\u0026rdquo; comes from the term \u0026ldquo;trap\u0026rdquo; which is a generic term used to describe the mechanism by which a processor invokes a higher privilege level when an event occurs. We say that the processor traps to the higher privilege level.\nThere are many ways to create a trap frame, but all approaches are effectively the same: they save all of the state necessary for execution in memory. Most implementations push all of the state onto the stack. After pushing all of the state, the stack pointer itself becomes a pointer to the trap frame. We\u0026rsquo;ll be taking exactly this approach.\nFor now, the complete execution state of our Cortex-A53 consists of:\nx0 \u0026hellip; x30 - all 64-bits of all 31 general purpose registers\nq0 \u0026hellip; q31 - all 128-bits of all SIMD/FP registers\nTPIDR - the 64-bit \u0026ldquo;thread ID\u0026rdquo; register\nThis is stored in TPIDR_ELs when the source of the exception is at level s.\nsp - the stack pointer\nThis is stored in SP_ELs when the source of the exception is at level s.\nPSTATE - the program state\nRecall that this is stored in SPSR_ELx when an exception is taken to ELx.\npc - the program counter\nThe register ELR_ELx stores the preferred link address, which may or may not be the PC that the CPU had when the exception is taken. Typically, the ELR_ELx is either the PC when the exception is taken, or PC + 4.\nWe\u0026rsquo;ll need to save all of this context in the trap frame by pushing the relevant registers onto the stack before calling the exception handler and then restore the trap frame by popping from the stack when the handler returns. After saving all of the state, the stack should look as follows:\nPlease double check each register whether it uses _ELx or _ELs. Note that SP and TPIDR in the trap frame should be the stack pointer and thread ID of the source, not the target. Since the only eventual source of exception will be EL0, you should save/restore the SP_EL0 and TPIDR_EL0 registers. When all state has been pushed, the CPU\u0026rsquo;s true SP (the one used by the exception vector) will point to the beginning of the trap frame.\nFinally, you\u0026rsquo;ll pass a pointer to the trap frame as the third argument to handle_exception. The type of the argument is \u0026amp;mut TrapFrame; TrapFrame is declared in kern/src/traps/frame.rs. You\u0026rsquo;ll need to define the TrapFrame struct so that it exactly matches the trap frame\u0026rsquo;s layout.\nWhat\u0026rsquo;s a thread ID?\nThe TPIDR register (TPIDR_ELx) allows the operating system to store some identifying information about what\u0026rsquo;s currently executing. Later, when we implement process, we\u0026rsquo;ll store the process\u0026rsquo;s ID in this register. For now, we\u0026rsquo;ll save and restore this register for posterity.\nPreferred Exception Return Address When an exception is taken to ELx, the CPU stores a preferred link address, or preferred exception return address in ELR_ELx. This value is defined in (ref : D1.10.1) as follows:\nFor asynchronous exceptions, it is the address of the first instruction that did not execute, or did not complete execution, as a result of taking the interrupt.\nFor synchronous exceptions other than system calls, it is the address of the instruction that generates the exception.\nFor exception generating instructions, it is the address of the instruction that follows the exception generating instruction.\nA brk instruction falls into the second category. As such, if we want to continue execution after a brk instruction, we\u0026rsquo;ll need to ensure that ELR_ELx contains the address of the next instruction before returning. Since all instructions are 32-bits wide on AArch64, this is simply ELR_ELx + 4.\nImplementation Start by implementing the context_save and context_restore routines in kern/init/vectors.s. The context_save routine should push all of the relevant registers onto the stack and then call handle_exception, passing a pointer to the trap frame as the third argument. Then implement context_restore, which should do nothing more than restore the context.\nNote that the instructions generated by the HANDLER macro already save and restore x28, x29, x30(lr), and x31(xzr). You should not save and restore these registers in your context_{save,restore} routines, but your trap frame must still contain these registers. Technically there is no need to save and restore xzr register since it always contains zero. We save it just to make SP 16-byte aligned.\nTo minimize the impact on performance for the context switch, you should push/pop registers from the stack as follows:\n// pushing registers `x1`, `x5`, `x12`, and `x13` stp x1, x5, [SP, #-16]! stp x12, x13, [SP, #-16]! // popping registers `x1`, `x5`, `x12`, and `x13` ldp x1, x5, [SP], #16 ldp x12, x13, [SP], #16 Once you have implemented these routines, finish defining TrapFrame in kern/src/traps/frame.rs. Ensure that the order and size of the fields exactly match the trap frame you create and pass a pointer to in context_save.\nFinally, in handle_exception, remove the endless loop and increment the ELR in the trap frame by 4 before returning from a brk exception. Once you have successfully implemented the context switch, your kernel should continue to run as normal after exiting from the debug shell. When you are ready, proceed to the next phase.\nTrap Frame Layout\nYour trap frame doesn\u0026rsquo;t need to exactly match the diagram, but it should contain all of the same data.\nq0 .. q31\nDon\u0026rsquo;t forget that the qn registers are 128-bits wide!\nTo call handle_exception, you\u0026rsquo;ll need to save/restore a register that\u0026rsquo;s not part of the trap frame.\nRust has two 128-bit integer types: u128 and i128.\nUse the mrs and msr instruction to read/write special registers.\nOur context_save routine is exactly 42 instructions.\nOur context_restore routine is exactly 37 instructions.\nOur TrapFrame contains 6 fields (two of them are arrays) and is 792 bytes in size without xzr register.\nHow could you lazy-load floating point registers? (lazy-float)\nSaving and restoring the 128-bit SIMD/FP registers is very expensive; they account for 512 of the 792 bytes in the TrapFrame! It would be ideal if we saved/restored these registers only if they were actually in use by the source of the exception or the target of a context switch.\nThe AArch64 architecture allows the use of these registers to be selectively enabled and disabled. When SIMD/FP is disabled, an instruction that uses the registers traps. How could you use this functionality to implement lazy-loading of SIMD/FP registers so that they\u0026rsquo;re only saved/restored on context switches if they\u0026rsquo;re being used while continuing to allow the registers and SIMD/FP instructions to be used freely? Be specific about what you would do when specific exceptions occur, whether you would need to modify the TrapFrame struct, and what additional state you\u0026rsquo;d need to maintain.\nPhase 2: It\u0026rsquo;s a Process In this phase, you will implement user-level processes. You\u0026rsquo;ll start by implementing a Process struct that will maintain a process\u0026rsquo;s state. You\u0026rsquo;ll then bootstrap the system by starting the first process. Then, you\u0026rsquo;ll implement a tick-based, round-robin scheduler. To do so, you\u0026rsquo;ll first implement an interrupt controller driver and enable timer interrupts. Then, you\u0026rsquo;ll invoke your scheduler when a timer interrupt occurs, performing a context switch to the next process. Finally, you\u0026rsquo;ll implement your first system call: sleep.\nAfter completing this subphase, you\u0026rsquo;ll have built a minimal but complete multitasking operating system. For now, processes will be sharing physical memory with the kernel and other processes. In the next phase, we will enable virtual memory to isolate processes from one another and protect the kernel\u0026rsquo;s memory from untrusted processes.\nSubphase A: Processes In this subphase, you\u0026rsquo;ll complete the implementation of the Process structure in kern/src/process/process.rs. You\u0026rsquo;ll use your implementation to start the first process in the next subphase.\nWhat\u0026rsquo;s a Process? A process is a container for code and data that\u0026rsquo;s executed, managed, and protected by the kernel. They are the singular unit by which non-kernel code executes: if code is executing, it is either executing as part of a process or executing as part of the kernel. There are many operating system architectures, especially in the research world, but they all have a concept that largely mirrors that of a process.\nProcesses typically run with a reduced set of privileges (EL0 for our OS) so that the kernel can ensure system stability and security. If one process crashes, we don\u0026rsquo;t want other processes to crash or for the entire machine to crash with it. We also don\u0026rsquo;t want processes to be able to interfere with one another. If one process hangs, we don\u0026rsquo;t want other processes to be unable to make progress. Processes provide isolation: they operate largely independently of one another. You likely see these properties of processes every day: when your web browser crashes or hangs, does the rest of the machine crash or hang as well?\nImplementing processes, then, is about creating the structures and algorithms to protect, isolate, execute, and manage untrusted code and data.\nWhat\u0026rsquo;s in a Process? To implement processes, we\u0026rsquo;ll need to keep track of a process\u0026rsquo;s code and data as well as auxiliary information to allow us to properly manage and isolate multiple processes. This means keeping track of a process\u0026rsquo;s:\nStack\nEach process needs a unique stack to execute on. When you implement processes, you'll need to allocate a section of memory suitable for use as the process's stack. You'll then need to bootstrap the process's stack pointer to point to this region of memory. Heap\nTo enable disjoint dynamic memory allocation, each process will also have its own heap. The heap will start empty but can be expanded on request via a system call. In lab 4, we won't implement the heap and will support user programs that only require the stack. Code\nA process isn't very useful unless it's executing code, so the kernel will need to load the process's code into memory and execute it when appropriate. Virtual address space\nBecause we don't want processes to have access to the kernel's memory or the memory of other processes, each process will be confined to a separate virtual address space using virtual memory. Scheduling state\nThere are typically many more processes than there are CPU cores. The CPU can only execute one instruction stream at a time, so the kernel will need to multiplex the CPUs time (and thus, instruction stream) to execute processes concurrently. It is the scheduler's job to determine which process gets to run when and where. To do so correctly, the scheduler needs to know if a process is ready to be scheduled. The *scheduling state* keeps track of this. Execution state\nTo correctly multiplex the CPUs time amongst several processes, we'll need to ensure that we save a process's execution state when we switch it off the CPU and restore it when we switch it back on. You've already seen the structure we use to maintain execution state: the trap frame. Each process maintains a trap frame to properly maintain its execution state. A process\u0026rsquo;s stack, heap, and code make up all of the physical state of a process. The rest of the state is necessary for the isolation, management, and protection of processes.\nThe Process structure in kernel/src/process/process.rs will maintain all of this information. Because all processes will be sharing memory for the time being, you won\u0026rsquo;t see any fields for the process\u0026rsquo;s heap, code, or virtual address space; you\u0026rsquo;ll handle these later in the assignment.\nDoes a process have to trust the kernel? (kernel-distrust)\nIt should be clear that a kernel is distinctively distrustful of processes, but does a process have to trust the kernel? If so, what is it expecting from the kernel?\nWhat could go wrong if two processes shared stacks? (isolated-stacks)\nImagine that two processes are executing concurrently and are sharing a stack. First: what would it mean for two processes to share a stack? Second: why would it be very likely that the processes would crash fairly quickly into their lives? Third: define a property of processes that, even if they were sharing a stack, would never crash as a result of sharing a stack. In other words, what would two processes that run concurrently and share a stack but never crash as a result of this sharing look like?\nImplementation You\u0026rsquo;ll now start the implementation of the Process structure in kern/src/process/process.rs. Before you begin, read the implementation of the Stack structure that we provided for you in kern/src/process/stack.rs. Ensure that you know how to use the structure to allocate a new stack and retrieve a pointer to the stack for a new process. Then, read the implementation of the State structure, which will be used to keep track of the scheduling state, that we have provided for you in kern/src/process/state.rs. Try to reason about how you\u0026rsquo;d interpret the different variants when scheduling processes.\nFinally, only implement the Process::new() method. The implementation will be simple; there\u0026rsquo;s nothing complex about keeping track of state! You will finish the implementation of the process structure later on. When you\u0026rsquo;re ready, proceed to the next subphase.\nHow is the stack\u0026rsquo;s memory reclaimed? (stack-drop)\nThe Stack structure allocates a 16-byte aligned 1MiB block of memory when it is created. What ensures that this memory is freed when the Process that owns it is no longer around?\nHow could you lazily allocate memory for the stack? (lazy-stacks)\nThe Stack structure allocates 1MiB of memory for the stack regardless of whether or how much of the stack the process actually uses. Thinking ahead to virtual memory, how might we use virtual memory to lazily allocate memory for the stack so that no or minimal memory is used by the stack until it\u0026rsquo;s needed?\nHow could a process increase its stack size? (stack-size)\nSome processes will require significantly more stack space than 1MiB, but our simple design allocates exactly 1MiB of stack space for all processes. Assuming processes have access to dynamic memory allocation, how could a process increase its stack size? Be specific about which instructions the process would execute.\nSubphase B: The First Process In this subphase, we\u0026rsquo;ll start the first user-space (EL0) process. You will be working primarily in kern/src/process/scheduler.rs and kern/src/main.rs.\nContext Switching Processes You\u0026rsquo;ve already done most of the work that will allow you to context switch between processes. To context switch between processes in response to an exception, you will:\nSave the trap frame as the current process\u0026rsquo;s trap frame in its context field.\nRestore the trap frame of the next process to execute from its context field.\nModify the scheduling state to keep track of which process is executing.\nUnfortunately, to context switch into the first process, we\u0026rsquo;ll need to deviate from this plan a bit. It would be incorrect to execute one of the steps above before the first process. Can you tell which?\nLet\u0026rsquo;s see what would happen if we followed these steps before the first process. First, an exception occurs which prompts a context switch. We\u0026rsquo;ll later see that this will be a timer interrupt which drives the process scheduler. Then we follow step 1: in response to the exception, we store the current trap frame in the current process\u0026rsquo;s context field. Note, however, that there is no current process yet! And so Later, as part of step 2, we restore the next process\u0026rsquo;s context and return.\nBecause the exception wasn\u0026rsquo;t taken while the process running, the trap frame we save, and later restore, will have little in relation to the process itself. In other words, we\u0026rsquo;ve clobbered the process\u0026rsquo;s trap frame with an unrelated one. Thus, we can\u0026rsquo;t possibly run step 1 without a valid process\u0026rsquo;s trap frame first. In other words, to properly context switch into the first process, it seems that the process needs to already be running. Said yet another way, we can\u0026rsquo;t properly context switch until after the first context switch has occurred: catch-22!\nTo work around this, we\u0026rsquo;re going to bootstrap context switching by faking the first context switch. Instead of the trap frame for the first process coming from the context_save routine you wrote previously, we will manually create the trap frame on the new process\u0026rsquo;s stack and call context_restore ourselves, avoiding step 1 above entirely. Once the first process is running, all other context switching will work normally.\nKernel Threads We haven\u0026rsquo;t yet built a mechanism to load code from the disk into memory. Once we enable virtual memory, we\u0026rsquo;ll need to implement the procedures to do so. For now, while we\u0026rsquo;re sharing memory with the kernel, we can simply reuse the kernel\u0026rsquo;s code and data. As long as the kernel and the processes don\u0026rsquo;t share local data (the stack), which we\u0026rsquo;ve ensured they don\u0026rsquo;t by allocating a new stack for each process, they will be able to execute concurrently without issue. What\u0026rsquo;s more, Rust ensures that there is no possibility of a data race between the processes.\nSharing memory and other resources between processes is such a common occurrence that these types of processes have a special name: threads. Indeed, a thread is nothing more than a process that shares memory and other resources with another process.\nSoon, you\u0026rsquo;ll start the first process. Because that process will be sharing memory with the kernel, it will be a kernel thread. As such, the extent of the work required to start this first process is minimal since all of the code and data is already in memory:\nBootstrap context switching by setting up the \u0026ldquo;fake\u0026rdquo; saved trap frame. Call context_restore Switch to EL0. While requiring very few lines of code, you\u0026rsquo;ll find that it requires careful implementation for correctness.\nThe term kernel thread is overloaded.\nThe term kernel thread is used to refer both to threads implemented by the kernel (as opposed to threads implemented in user space) and threads running in the kernel. It\u0026rsquo;s an unfortunate name clash, but context typically clarifies which is meant. As a quick heuristic, unless the discussion is about OS development, you should assume that the discussion is about threads implemented by the kernel.\nImplementation There is a new global variable in kern/src/main.rs, SCHEDULER, of type GlobalScheduler, which is simply a wrapper around a Scheduler. Both of these types are defined in kern/src/process/scheduler.rs. The SCHEDULER variable will serve as the handle to the scheduler for the entire system.\nTo initialize the scheduler and start executing the first process, the start() method on GlobalScheduler should be called. Your task is to implement the start() method.\nTo do so, you will need to:\nWrite an extern function that takes no parameters and starts a shell.\nYou will arrange for this function to be called when the process first executes. You can write this function wherever you\u0026rsquo;d like. We\u0026rsquo;ll remove it once we\u0026rsquo;re able to start processes backed by binaries on the disk.\nIn start() , create a new Process and set-up the saved trap-frame.\nYou\u0026rsquo;ll need to set up the process\u0026rsquo;s trap frame so that when it is restored to the CPU by context_restore later, your extern function executes, the process\u0026rsquo;s stack pointer points to the top of the process\u0026rsquo;s stack, the process is executing in EL0 in the AAarch64 execution state, and IRQ interrupts are unmasked for current EL1 so that we can handle timer interrupts from EL0 in the next section.\nSetup the necessary registers, call context_restore , and eret into EL0.\nOnce you\u0026rsquo;ve set up the trap frame, you can bootstrap a context switch to that process by:\nCalling context_restore with the appropriate register(s) set to the appropriate values.\nNote: we are being vague here on purpose! If this feels opaque, consider what context_restore does, what you want it to do, and how you can make it do that.\nSetting the current stack pointer (sp) to its initial value (the address of _start). This is necessary so that we can use the entire EL1 stack when we take exceptions later. Note: You cannot ldr or adr into sp directly. You must first load into a different register and then mov from that register into sp.\nResetting any registers that may no longer contain 0. You should not leak any information to user-level processes.\nReturning to EL0 via eret.\nYou\u0026rsquo;ll need to use inline assembly to implement this. As an example, if a variable tf is a pointer to the trap frame, the following sets the value of x0 to that address and then copies it to x1:\nunsafe { asm!(\u0026#34;mov x0, $0 mov x1, x0\u0026#34; :: \u0026#34;r\u0026#34;(tf) :: \u0026#34;volatile\u0026#34;); } You may wish to add an infinite loop at the end of the start() function to satisfy compiler since the function should not be returned. Once you\u0026rsquo;ve implemented the method, add a call to SCHEDULER.start() in kmain and remove any shell or breakpoint invocations. You don\u0026rsquo;t need to call SCHEDULER.initialize() yet which will be implemented in Subphase D. Your kmain should now simply be a series of two initialization calls and a scheduler starting call such as below.\nunsafe fn kmain() -\u0026gt; ! { ALLOCATOR.initialize(); FILESYSTEM.initialize(); SCHEDULER.start() } If all is well, your extern function will be called from EL0 when the kernel starts, running the shell as a user-level process.\nBefore continuing, you should also ensure that a context switch back to the same process works correctly at this point. Try adding a few calls to brk in your extern function before and after you start a shell:\nextern fn run_shell() { unsafe { asm!(\u0026#34;brk 1\u0026#34; :::: \u0026#34;volatile\u0026#34;); } unsafe { asm!(\u0026#34;brk 2\u0026#34; :::: \u0026#34;volatile\u0026#34;); } shell::shell(\u0026#34;user0\u0026gt; \u0026#34;); unsafe { asm!(\u0026#34;brk 3\u0026#34; :::: \u0026#34;volatile\u0026#34;); } loop { shell::shell(\u0026#34;user1\u0026gt; \u0026#34;); } } You should be able to return from each of the break point exceptions successfully. The source for each of the breakpoint exception should be LowerAArch64, indicating a successful switch to user-space. Once everything works as you expect, proceed to the next subphase.\nOur inline assembly consists of exactly 6 instructions.\nBesides the inline assembly, you do not need unsafe.\nSubphase C: Timer Interrupts In this subphase, you will implement a driver for the interrupt controller on the BCM2837. You\u0026rsquo;ll also modify your existing system timer driver to enable timer interrupts to be configured. Finally, you\u0026rsquo;ll enable periodic timer interrupts to act as the spring-board for scheduling based context switches. You will be working primarily in lib/pi/src/interrupt.rs, lib/pi/src/timer.rs, and kern/src/traps.\nInterrupt Handling On AArch64, interrupts are nothing more than exceptions of a particular class. The key differentiator between the two is that interrupts occur asynchronously: they are generated by an external source in response to external events.\nThe diagram below illustrates the path an interrupt takes from the source, an external device, to the sink, an exception vector:\nInterrupts can be selectively disabled at each point along the path. In order for an interrupt to be delivered to an exception vector, the external device, the interrupt controller, and the CPU must all be configured to accept the interrupt.\nWhat is an interrupt controller?\nAn interrupt controller as another external device that acts as a proxy and gate between interrupt generating devices, like the system timer, and the CPU. The interrupt controller is physically connected to the CPU\u0026rsquo;s interrupt pins. When an input pin on the interrupt controller is signaled, the interrupt controller forwards the signal to the CPU.\nThe extra layer of indirection allows for interrupts to be selectively enabled and disabled. It also allows CPU manufacturers to choose which, if any, interrupt controller they want to bundle with the CPU.\nExternal Device You\u0026rsquo;ve already written a device driver for the system timer. In this subphase, you will extend your driver to enable configuration of the timer\u0026rsquo;s compare registers. The system timer continuously compares the current time to the values in the compare registers and generates an interrupt when the values equal.\nInterrupt Controller The system timer delivers interrupts to the interrupt controller, which must then be configured to deliver interrupts to the CPU. You will write a device driver for the interrupt controller to do exactly this.\nWhen the interrupt controller receives an interrupt, it marks the interrupt as pending and forwards it to the CPU by holding a physical interrupt pin on the CPU logically high. For some interrupts, including system timer interrupts, the pin is held high until the interrupt is acknowledged. This means that the interrupt will be continuously delivered until it is acknowledged. Once the interrupt is acknowledged, the interrupt pin is released, and the pending flag is unset.\nCPU Interrupts must be unmasked for the CPU to deliver them to exception vectors. By default, interrupts are masked by the CPU, so they will not be delivered. The CPU may deliver interrupts that were received while interrupts were masked as soon as interrupts are unmasked. When the CPU invokes an exception vector, it also automatically masks all interrupts. This is so that interrupts which are held high until they are handled, like system timer interrupts, don\u0026rsquo;t immediately result in an exception loop.\nIn the previous subphase, you configured interrupts to be delivered when processes are executing in EL0, so there\u0026rsquo;s no additional work to do on this front.\nWhen would you unmask IRQs while handling an IRQ? (reentrant-irq)\nAlthough our kernel keeps IRQs masked in exception handling routine, thus does not support nested interrupt handling, it turns out that unmasking IRQs while handling IRQs is a fairly common occurrence in commodity operating systems. Can you come up with a scenario in which you\u0026rsquo;d want to do this? Further, would doing so without first acknowledging pending IRQs result in an exception loop? Why or why not?\nException Vector You\u0026rsquo;ve already configured exception vectors. As such, all that\u0026rsquo;s left is to properly handle IRQ (interrupt request) exceptions. To handle interrupts, we have a global Irq struct which holds a list of handler functions for corresponding interrupt. You can set a handler function for each interrupt by calling register() function and execute the registered handler with invoke() function in kern/src/traps/irq.rs. You\u0026rsquo;ll modify your handle_exception function in kern/src/traps.rs so that it forwards all known interrupt requests to the invoke() function.\nTo determine which interrupt has occurred, you will need to check which interrupts are pending at the interrupt controller. The handle_irq function will then acknowledge the interrupt and process it.\nImplementation Start by implementing the interrupt controller driver in lib/pi/src/interrupt.rs. The documentation for the interrupt controller is on chapter 7 of the BCM2837 ARM Peripherals Manual . You only need to handle enabling, disabling, and checking the status of the regular IRQs described by the Interrupt enum; you needn\u0026rsquo;t worry about FIQs or Basic IRQs. We are providing four methods for Interrupt struct. iter() methods can be used when iterating interrupts to check which interrupts are pending. to_index() and from_index() would be useful when implementing register() and invoke() method.\nThen, implement the tick_in() method and function for your system timer driver in lib/pi/src/timer.rs. The documentation for the system timer is on chapter 12 of the BCM2837 ARM Peripherals Manual . You will need write to two registers to implement tick_in() correctly.\nNow go to the src/traps/irq.rs and implement register() and invoke() methods. The Irq struct internally holds a list of IrqHandler which is a smart pointer for a handler function. You can set a new handler function for an interrupt with register() method, and executes it with invoke() method.\nThen, enable timer interrupts and set a timer interrupt to occur in TICK microseconds just before you start the first process in GlobalScheduler::start() in kern/src/process/scheduler.rs. The TICK variable is declared in kern/src/param.rs. In addition, register an handler function for the timer. The handler function should set a new timer interrupt to occur in TICK microseconds, ensuring that timer interrupts occur every TICK microseconds indefinitely. To test your implementation, you might want to print a message in here.\nModify your handle_exception function in kern/src/traps.rs so that it forwards known interrupts to the invoke() function in kern/src/traps/irq.rs.\nFinally, add IRQ.initialize() in your kern/src/main.rs before calling SCHEDULER.start().\nWhen you are finished, you should see a timer interrupt occur every TICK microseconds with a source of LowerAArch64 and kind of Irq. You should be able to interact with the process normally between timer interrupts. When everything works as you expect, proceed to the next subphase.\nWe\u0026rsquo;ll change the TICK setting later on!\nWe\u0026rsquo;re currently using an absurdly slow TICK setting of 2 seconds to ensure that everything works as we expect. Typically, this number is between 1 and 10 milliseconds. We\u0026rsquo;ll decrease the TICK to a more reasonable 10 ms later on.\nSubphase D: Scheduler In this subphase, you will implement a simple round-robin preemptive scheduler. You will be working primarily in kern/src/process/scheduler.rs and kern/src/process/process.rs.\nScheduling The scheduler\u0026rsquo;s primary responsibility is to determine which task to execute next, where a task is defined as anything that requires execution on the CPU. Our operating system is relatively simple, and so the scheduler\u0026rsquo;s idea of a task will be constrained to processes. As such, our scheduler will be responsible for determining which process to run next, if any.\nThere are many scheduling algorithms with a myriad of properties. One of the simplest is known as \u0026ldquo;round-robin\u0026rdquo; scheduling. A round-robin scheduler maintains a queue of tasks. The next task to execute is chosen from the front of the queue. The scheduler executes the task for a fixed time slice (the TICK), also known as a quantum. When the task has executed for at most its full quantum, the scheduler moves it to the back of the queue. Thus, a round-robin scheduler simply cycles through a queue of tasks.\nIn our operating system, the scheduler marks a task as being in one of four states:\nReady\nA task that is ready to be executed. The scheduler will execute the task when its turn comes up. Running\nA task that is currently executing. Waiting\nA task that is waiting on an event and is not ready to be executed until that event occurs. The scheduler will check if the event has occurred when the task's turn comes up. If the event has occurred, the task is executed. Otherwise, the task loses its turn and is checked again in the future. Dead\nA task is currently dead (not running nor eligible to run) and ready to be reclaimed. The State enum in kern/src/process/state.rs represents these states. Each process struct is associated with a State which the scheduler will manage. Note that the Waiting state contains a function that the scheduler can use to determine if the event being waited on has occurred.\nThe diagram below depicts four scheduling rounds of a round-robin scheduler. The task C is waiting on an event that occurs some time between rounds 3 and 4.\nThe rounds are:\nIn round 1, there are four tasks in the queue, A, B, C, and D. A, the process at the head of the queue is currently running on the CPU. C is in a waiting state, while the others are ready or running. When A\u0026rsquo;s quantum is used up, it is moved to the back of the queue.\nThe task from the front of the queue, B, is executed. It is moved to the back of the queue when its quantum expires.\nSince C is waiting for an event, the scheduler checks to see if the event being waited on has occurred. At this point it has not, so C is skipped and D is chosen to run next. D is moved to the front and executed. At the end of the quantum, D is moved to the back of the queue.\nSince C is still in waiting state, the scheduler checks to see if the event has occurred. At this point it has, so C is scheduled. After its time quantum, C is moved to the back of the queue.\nWould separating ready and waiting tasks be beneficial? (wait-queue)\nAn alternative implementation of a round-robin scheduler maintains two queues: a ready queue, consisting of only ready tasks, and a wait queue, consisting only of waiting tasks. How would you make use of the queues in the round-robin scheduler? Do you expect performance (average task latency/throughput) to be better or worse?\nCode Structure The Scheduler structure in kern/src/process/scheduler.rs maintains a queue of processes to execute. Processes are added to the queue via the the Scheduler::add() method. The method is also responsible for assigning unique IDs to processes. IDs are stored in the process\u0026rsquo;s TPIDR register.\nWhen a scheduling change is required, the Scheduler::schedule_out() and Scheduler::switch_to() method is invoked. As their name suggests, schedule_out() method changes the current process\u0026rsquo;s state to new_state, saves the current trap frame in the current process, and push the current process back to the end of scheduling queue. Recalling that our Raspberry Pi board has 4 cores, there might exist multiple process with Running state up to the number of cores. Thus, you have to find current process based on the process id within current trapframe. switch_to() method finds the next process to execute, moves the next process to the front of the queue, restores the next process\u0026rsquo;s trap frame, and marks the next process as Running.\nTo determine if a process is ready to execute, the scheduler should call the process.is_ready() method, defined in kern/src/process/process.rs. The method returns true if either the state is Ready or if an event being waited on has occurred.\nThe scheduler should be invoked every TICK microseconds. Timer interrupts, set up in the previous subphase, will be one of the primary sources of a scheduling change. The GlobalScheduler type provides thread-safe method switch() as well as wrappers around the add() and kill() methods of Scheduler. switch() method first gets the lock and schedules out the current process. Then, it repeatedly tries to switch to a next process. If switch_to() method fails to find a next process to switch to, such as when all processes are in the Waiting state, it executes wfe instruction to enter into a low-power state and wait until an event comes without executing further instruction.\nFinally, to kill a currently running process, Scheduler::kill() method is invoked. This method will be called by system call in the next subphase. The method internally calls Scheduler::schedule_out() method with Dead state as a parameter to schedule out the current process. Then, it removes the dead process from the end of the queue and drop the dead process`s instance. Since GlobalScheduler type provides thread-safe wrappers around the Scheduler::kill(), it is guaranteed that the dead process is the last entry of the processes queue.\nWhy doesn\u0026rsquo;t the scheduler know the new state? (new-state)\nThe scheduler.switch() method requires the caller to pass in the new state of the current process. This implies that the scheduler does not know what the new state of the process should be. Why might it not?\nImplementation You\u0026rsquo;re now ready to implement the round-robin scheduler. We recommend the following approach:\nImplement the Process::is_ready() method in kernel/src/process/process.rs.\nThe mem::replace() function will prove useful here. Please note that the Waiting state contains a function to check if the event being waited on has occurred. Only if it returns true, Waiting process can be scheduled.\nImplement the Scheduler struct in kern/src/process/scheduler.rs.\nThere are five functions you need to implement; new(), add(), schedule_out(), switch_to() and kill().\nInitialize the scheduler in GlobalScheduler::initialize().\nThe global scheduler should be created and initialized before the first process executes. The first process should be present in scheduler\u0026rsquo;s queue before it executes.\nModify the GlobalScheduler::start().\nInstead of starting a process from an address of a extern function, now make use of switch_to function to start a process from the scheduling queue.\nInvoke the scheduler when a timer interrupt occurs.\nInvoke SCHEDULER.switch() on a timer interrupt to context switch between the current process and the next process.\nTest your scheduler by adding more than one process in GlobalScheduler::initialize(). You\u0026rsquo;ll need to allocate new processes and set up their trap frames appropriately. You\u0026rsquo;ll likely want to create a new extern function for each new process so that you can differentiate between them. Ensure that you add the processes to the scheduler\u0026rsquo;s queue in the correct order.\nWhen you are finished, you should see a different process execute every TICK microseconds. You should be able to interact with each process normally between timer interrupts. When everything works as you expect, proceed to the next subphase.\nOverflow\nDon\u0026rsquo;t overflow when generating a process ID!\nUnsafe\nYou should not use unsafe to implement any of these routines!\nUse mem::replace() to get an owned version of the process\u0026rsquo;s state.\nWhy is it correct to wait for events when no process is ready? (wfe)\nUsing the wfe instruction to wait when no process is ready means that the CPU stalls until an event arrives. If no event arrives after a wfe is executed, scheduling never resumes. Why is this the correct behavior?\nThink about the scenarios in which a process is in the waiting state.\nNote about wfe\nWhen there are no processes ready to be executed, the processor will go into Wait-For-Event wfe state. Event != interrupt in this context. Meaning that, when our processor goes into wfe, it will not be waken up by our timer interrupt, and so the scheduling will not resume. If you wish to have the processor waiting for interrupts instead, change wfe instruction to wfi instruction.\nSubphase E: Sleep In this subphase, you will implement the sleep system call and shell command. You will be working primarily in kern/src/shell.rs and kern/src/traps.\nSystem Calls A system call is nothing more than a particular kind of exception. When the svc #n instruction is executed, a synchronous exception with syndrome Svc(n) will be generated corresponding to system call n. This is similar to how brk #n generates a Brk(n) exception except that the preferred link address is the instruction after the svc instruction instead of the instruction itself. System calls are the mechanism that user processes use to request services from the operating system that they would otherwise have insufficient permissions to carry out.\nA typical operating system exposes 100s of system calls ranging from file system operations to getting information about the underlying hardware. In this subphase you will implement the sleep system call. The sleep system call asks the scheduler not to schedule the process for some amount of time. In other words, it asks the operating system to put the process to sleep.\nSyscall Convention Just as we need a convention for function calls, we require a convention for system calls. Our operating system will adopt a modified version of the system call convention used by other Unix-based operating systems. The rules are:\nSystem call n is invoked with svc #n.\nUp to 7 parameters can be passed to a system call in registers x0 \u0026hellip; x6.\nUp to 7 parameters can be returned from a system call in registers x0 \u0026hellip; x6.\nRegister x7 is used to indicate an error. See lib/kernel_api/src/lib.rs for possible values.\nIf x7 is 1, there was no error.\nIf x7 is 0, unknown error has occurred.\nIf x7 is any other value, it represents an error code specific to the system call.\nAll other registers and program state are preserved by the kernel.\nAs such, to invoke an imaginary system call 7 that takes two parameters, a u32 and a u64, and returns two values, two u64s, we might write the following using Rust\u0026rsquo;s inline assembly fn syscall_7(a: u32, b: u64) -\u0026gt; Result\u0026lt;(u64, u64), Error\u0026gt; { let ecode: u64; let result_one: u64; let result_two: u64; unsafe { asm!(\u0026#34;mov w0, $3 mov x1, $4 svc 7 mov $0, x0 mov $1, x1 mov $2, x7\u0026#34; : \u0026#34;=r\u0026#34;(result_one), \u0026#34;=r\u0026#34;(result_two), \u0026#34;=r\u0026#34;(ecode) : \u0026#34;r\u0026#34;(a), \u0026#34;r\u0026#34;(b) : \u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x7\u0026#34;) } let e = OsError::from(ecode); if let OsError::Ok = e { Ok((result_one, result_two)) } else { Err(e) } } Notice that the wrapper around the system call checks the error value before returning the result value. You can find pre-defined system call number and enum of OsError in lib/kernel-api/src/lib.rs.\nWhy do we use a separate register to pass the error value? (syscall-error)\nMost Unix operating systems, including Linux, overload the first result register (x0, in our case) as the error value register. In these conventions, negative values with a certain range represent error codes; all other values are interpreted as successful return values. What is the advantage to the approach that we have taken? What is the disadvantage?\nSleep Syscall The sleep system call will be system call number 1 in our operating system. The call takes one parameter: a u32 corresponding to the number of milliseconds that the calling process should be suspended for. Besides the possible error value, it returns one parameter: a u32 corresponding to the number of milliseconds that elapsed between the process\u0026rsquo;s initial request to sleep and the process being woken up. Its pseudocode signature would be:\nfn sleep(t: u32) -\u0026gt; u32 When does the elapsed time differ from the requested time? (sleep-elapsed)\nIn which situations, if any, will the return value from sleep differ from the input value? In which situations, if any, will they be identical? What do you think the relative probability of each case is?\nImplementation Implement the sleep system call now. Start by modifying your handle_exception function in kern/src/traps.rs so that it recognizes system call exceptions and forwards them to the handle_syscall function in kern/src/traps/syscalls.rs.\nThen implement the handle_syscall function. The function should recognize the sleep system call and calls its handler function sys_sleep(). Inside the sys_sleep(), modify the currently executing process as required. You will likely need to create a Box\u0026lt;FnMut\u0026gt; using a closure to complete your implementation. This should look as follows:\nlet boxed_fnmut = Box::new(move |p| { // use `p` }); You can read more about closures in TRPLv2 .\nFinally, add a sleep \u0026lt;ms\u0026gt; command to your shell that invokes the sleep system call, passing in ms milliseconds as the sleep time.\nTest your implementation by calling sleep in user-level shells. Ensure that a process is not scheduled while it is sleeping. All other processes should continue to be scheduled correctly. Then, ensure that no process is scheduled if all processes are sleeping. Once your implementation works as you expect, proceed to the next subphase.\nThe sleep system call handler will need to interact with the scheduler.\nRecall that closures can capture values from their environment.\nWe are providing kernel_api::syscall::sleep(span: Duration) function. Feel free to use it in your shell to invoke sleep systemcall. System call number and error code can also be found in kernel_api library.\nThe u32 type implement FromStr .\nPhase 3: Memory Management Unit In this phase, you\u0026rsquo;ll enable the support of the memory management unit so that our system can run multiple processes independently with each running in their own private virtual memory space. You\u0026rsquo;ll start by reviewing VirtualAddr and PhysicalAddr structs and several traits to support translation between addresses. Then, you\u0026rsquo;ll implement a two level page table indexing 64KB aligned page. With this PageTable struct, you\u0026rsquo;ll implement kernel page table and user page table. Finally, you\u0026rsquo;ll revisit code you have partially built in the previous phases, such as init.rs or scheduler.rs, in order to support context switch between processes having virtual memory space.\nSubphase A: Virtual Memory In this subphase, you\u0026rsquo;ll review VirtualAddr and PhysicalAddr structs to represent 64bit address values under kern/src/vm/address.rs. Then, you\u0026rsquo;ll add two more registers in your TrapFrame under kern/src/traps/frame.rs and kern/src/init/vectors.s\nWhy Do We Need Virtual Memory? Instead of sharing physical memory between kernel and user processes, each user process will have its own virtual memory space. This will isolate processes from each other and protect the kernel\u0026rsquo;s memory from untrusted processes.\nBy supporting virtual memory space, user processes do not need any knowledge of the physical memory space anymore. That is, processes will be unaware of the actual hardware addresses, or about other processes\u0026rsquo; virtual spaces that might execute at the same time.\nBecause of the isolation between virtual spaces of each process, two different processes might use the exact same virtual address, but each will translate into two different physical addresses.\nAlso, because of the isolation between virtual and physical space, we can translate contiguous virtual memory pages into fragmented physical memory pages. Hence, we can utilize fragmented pages in the physical space to form one contiguous virtual space. User processes can write, compile, and link applications to run in the virtual memory space, and the operating system will take care of mapping all memory accesses to the physical memory addresses.\nFor the operating system to achieve this, it needs the memory management unit, commonly referred to as MMU, that handle all translations between virtual spaces and physical space.\nHow To Work With The MMU? The memory management unit is a hardware unit that makes the translation between virtual space addresses and physical addresses. To make use of it, we need to enable it first, and provide the page mapping table which it will use for translations. That is, the MMU will perform all the translations, but the operating system is responsible for creating and upkeeping of the page tables.\nWith the MMU enabled, all processor instruction fetch or memory access will go through the MMU for translation, and so we will need to provide it with the proper page tables for both the kernel and user space.\nA consequence of turning on the MMU is that the kernel now have to be prepared to use the virtual memory and maintain its own set of page tables. We will see how to set that up in later parts of this phase.\nSeparation of Kernel And User Process Operating systems typically have a number of processes that are running concurrently. Since each of those processes have its own virtual memory space, the operating system needs to keep track of each process translation table. The translation table of each process will contain information that maps each virtual page address into a physical page address. Since these page tables are used for translation between user processes and physical addresses, they are hence called User Page Tables.\nIn the same way, since now we are enabling the MMU, we need to provide a Kernel Page Table that the MMU will use to continue to map the kernel addresses correctly. The MMU will also enforce the permissions of the kernel pages stated by the kernel page table we provide. Hence, we will have a permission isolation between kernel memory pages and processes memory pages.\nOur ARM architecture have two registers to keep track of the page table the MMU should use for address translations: Translation Table Base Register TTBR0_EL1 and TTBR1_EL1. In our design, we will use TTBR1_EL1 to point to the User Page Table base address, and TTBR0_EL1 to point to the Kernel Page Table base address. We will need to add both registers to the TrapFrame for bookkeeping of each process and the kernel page tables.\nSince now we have the kernel and user level using the virtual space address, we will need to split the virtual address space, so that we have a clear address isolation between kernel and user addresses in the virtual space. To do so, will use addresses starting from 0xffff_ffff_c000_0000 for user level virtual address, and addresses starting from 0x0 for kernel space virtual address.\nThus, the maximum size of the virtual memory space for user level would be 0x4000_0000, which is 1GB memory. You can find these variables defined in kern/src/param.rs.\nHow does the MMU know which table to use? (which-table)\nGiven a virtual address, the MMU will use either the user page table based at TTBR1_EL1, or the kernel page table based at TTBR0_EL1 in order to correctly translate to the physical address. But how does the MMU know which of the two tables to use for a given address?\nWe did not split the kernel and user level virtual space at arbitrary address! What happens if we make the kernel level at the upper virtual space part? The following reference might help: ARMv8-A Address Translation\nImplementation Review the VirtualAddr and PhysicalAddr in kern/src/vm/address.rs. Although they have different name and different purpose, there is no difference in their structure, required traits or functions.\nNow add TTBR0 and TTBR1 registers to the trap frame. You need to modify kern/src/traps/frame.rs and kern/src/init/vectors.s to do so. You should add instructions to save and restore TTBR0 and TTBR1, just like we did with other system registers: saved in context_save before calling handle_exception and restored in context_restore after returning back. Also, you should add TTBR0 and TTBR1 in TrapFrame structs accordingly. Note that your TrapFrame struct should exactly match the trap frame\u0026rsquo;s layout.\nIn addition, insert the following four lines after restoring TTBR0 and TTBR1 from the stack in vectors.s. This code block is required to ensure that memory accesses that occur before the dsb have completed before the completion of the dsb instruction.\ndsb ishst tlbi vmalle1 dsb ish isb Test that your modification doesn\u0026rsquo;t affect your trap frame and context switch functionality. If everything still works well, you can proceed to the next subphase.\nSubphase B: Page Table When the processor issues a 64-bit virtual address for an instruction fetch, or data access, the memory management unit(MMU) translates the virtual address to the corresponding physical address.\nThe above diagram depicts an example of virtual to physical address translation for a 64KB page. This assumes a 64KB granule and 42-bit virtual address space and we are going to stick to this setting. You could find more detail diagram with bits specified in Figure K6-13 in (ref : K6.1.2).\nLets walk through what is happening in the diagram above. The 64-bit virtual address -shown on the top- is split into 4 regions:\nBits [63-42] are used to decide which page table to use, the one based at TTBR0 or TTBR1, this is the first step of translation. Bits [41-29] are used to index into the L2 table based at the chosen TTBRx. Since 13 bits are used, this level of the page table can have 8192 entries, with each having 64-bit. We will later discover how each bit of the 64-bits is used, for now, we will just state that bits [47-16] of these entries are used to provide the base of the L3 page table. Bits [28-16] are used to index into the chosen L3 page table. Since we are using 13 bits, we can have 8192 entries for each L3 page table, with each entry being 64-bits. Out of these 64-bits, we will use the bits [47-16] in the final physical address. Bits [15-0] are used as a byte offset within the page addressed by bits [47-16] from L3 entry. Since we are having 64KB pages, 16 bits is enough to address each byte within. Having known the different parts of the virtual address, lets detail the steps taken by the MMU to perform the address translation:\nIf it is a user page mapping, TTBR1 is used for the base address for the first page table. When it is a mapping for kernel or I/O peripherals, TTBR0 is used for the base address for the first page table.\nThe page table contains 8192 64-bit page table entries, and [41:29] bits of virtual address are used as an index to find an entry in the table.\nCheck the validity of the L2 page table entry and whether or not the requested memory access is allowed.\nIf the entry is valid and allowed, the [47:16] bits of L2 page table entry is used to find the address of the level 3 page table.\nBits [28:16] of the virtual address are used to index the level 3 page table entry.\nCheck the validity of the L3 page table entry and whether or not the requested memory access is allowed.\nIf the entry is valid and allowed, the L3 page table entry refers to a 64KB physical page. Bits [47:16] are taken from the level 3 page table entry and used to form bits [47:16] of a physical address.\nBecause we have 64KB page, virtual address [15:0] is taken to form physical address [15:0].\nThe number of page tables within each level is up to the operating system, and how much memory it needs to map. The fact is, if we fully utilize this hierarchical model, we will end up with much bigger virtual space than our need. In our design, however, we are sufficed with only 1GB of virtual memory in the user level. Hence, for the user page table, we will have only one L2 page table, with two entries, each pointing to different L3 page table. Each L3 table of the two will have 8192 entries, each pointing to a 64KB page. So the calculation becomes:\n1 (L2 table) * 2 (L3 tables) * 8192 (L3 entries) * 64KB (page size) = 1GB\nHow is our design different? (translation-control)\nThe figure you see above does not exactly match our design. Particularly, the number of bits of TTBR select and Level 2 index are not the same in our design. Can you figure out the number of bits we allocated for each? and how exactly did we configure the MMU to apply our design?\nExactly the same hint of the previous question: how did we enforce the address split between the kernel and the user virtual spaces? Where in our code did we configure the MMU such that addresses above 0xffff_ffff_c000_0000 are for user virtual space?\nPage Table Entry The Page table entry PDF contains the specific details about L2 and L3 page table entry including their bit locations, names, and field descriptions. You will be referring to this document when you implement your page table. For more details, see the (ref : D4.3)\nADDR field contains 32-bit output address. That address is used differently depending on if it is a L2 table entry or a L3 table entry. For L2 table, the output address will be combined with Level 3 index that is located in [28:16] bits of virtual address to point the L3 page table. For L3 table, the output address will be the [47:16] bits of the translated physical address.\nThe low 10 bits of each entry represent the memory attributes.\nVALID : Identifies whether the descriptor is valid. You should set the entry as valid to use it.\nTYPE : L2 entry can be interpreted as a descriptor pointing a block of memory or a descriptor pointing next level of translation table (L3). Note that our L2 entries needs to point the L3 table, while our L3 entries will point to the memory pages.\nATTR : Describes memory region attributes. When user page table allocates a new page, its L3 entry should be normal memory. Likewise, when kernel page table sets L3 entries, they should be normal memory. On the other hand, for the memory range from IO_BASE to IO_BASE_END, they should have device-memory entries in L3 page table.\nNS : We don\u0026rsquo;t consider this for our system\nAP : Sets data access permission of the entry. The kernel page table should have KERN_RW permission while the user page table should have USER_RW permission.\nSH : Shareability field. Normal memory space should set their entries as inner shareable while device memory should set theirs as outer shareable.\nAF : Should be set when first accessed. Make sure you set this bit whenever you make a page table entry; in our implementation, we will assume all pages are being used.\nNow open the lib/aarch64/src/vmsa.rs file and review the pre-defined attributes, RawL2Entry bits and RawL3Entry bits. You can find defbit! macro defined in lib/aarch64/src/macros.rs file. Match your understanding with the library codes and learn how to use methods implemented for each entry.\nOur OS Memory Space Layout Now that you have knowledge about the small details of the MMU and the page tables. It is time to discuss how we are designing our operating system memory layout.\nAs a quick recap, in the virtual space, the kernel addresses will start from virtual address 0x0, while the user addresses will start from virtual address 0xffff_ffff_c000_0000. This is how we divided the virtual space, but what about the physical space?\nWe will design our operating system as follows: Once our kernel start running, we will create the kernel page table, and map all available physical memory to the kernel virtual memory. That is, we will use our memory_map function, that we implemented in Lab 3, to give us the end of the available physical memory. Then, we will start at address 0x0 in both address spaces, virtual and physical, and insert a page table entry for every page until the end limit we got from memory_map. This way, the kernel can continue to use the physical addresses we have been using without change, and we made the MMU happy by providing the table it needs!\nWe will also do the same for memory mapped IO devices. We will map the region between IO_BASE and IO_BASE_END to the same respective virtual address for every page. That way, we can continue to use the IO devices with the same addresses we have. You may find the definition of IO_BASE and IO_BASE_END in pi/src/common.rs.\nNote that since our rpi memory is 1GB, we can design the kernel page table exactly as we did in the user page table: One L2 table and two L3 tables.\nBut what about the user virtual memory? If we map all physical memory to the kernel virtual memory then what will we map to the user virtual space?\nActually, mapping the pages to the kernel virtual memory does NOT mean they are allocated! We are only creating this mapping so that the kernel can manage the physical memory using the virtual space.\nSo now, how does the memory allocation look like? If you remember, our allocator is managing the memory given back by memory_map, which uses a physical address.\nIn case the kernel needs to allocate memory, we can continue to use the allocator as usual, since the physical address space the allocator uses and the kernel\u0026rsquo;s virtual space are identical; thanks to the kernel page table we built in the beginning.\nIn case the user needs to allocate memory, we will use our allocator too, but then we will need to insert a proper page table entry in that context\u0026rsquo;s user page table, so that the translation happen properly.\nWith this design, the allocator ensures that the kernel and the user have their demand of memory, without the kernel accidentally using a page that is allocated to the user. The MMU ensures that the user does not have access to the kernel pages since we are flagging the kernel permission for kernel pages, and we do not need to change any of the physical addresses we have been using in the kernel since the virtual address for the kernel is no different, pretty cool!\nImplementation You\u0026rsquo;re now ready to implement a two level page table with 64KB granule starting at level 2. You\u0026rsquo;ll be primarily working in kern/src/vm/pagetable.rs , kern/src/process.rs and kern/src/vm.rs.\nWe recommend the following approach:\nImplement L2PageTable, L3Entry and L3PageTable struct.\nFeel free to add useful traits to convert {usize, u64, i32,.. } types to PhysicalAddr and VirtualAddr, or some functions to convert PhysicalAddr and VirtualAddr to {usize, u64, *mut u8,\u0026hellip;} such as as_usize(), as_u64(), and as_mut_ptr().\nImplement PageTable struct.\nThere are six methods needs to be implemented for PageTable. The requirements for each methods are specified in doc-comments within a skeleton code. Note that we are supporting virtual memory space up to 1GB memory. Thus, two L3 page table is enough to cover the space and L2 page table should have entries no more than 2. Whenever you make a page table entry, please make sure you set the proper bits value.\nImplement IntoIterator for \u0026amp;PageTable.\nThe returned iterator should iterates from the first entry of the first L3 page table and moves on to the second L3 page table. The chain() method will be useful.\nImplement KernPageTable::new() method.\nKernPageTable is a struct that internally has smart pointer of PageTable. First, create a PageTable with proper permission for the kernel page table.\nThen, load all the memory space residing in SDRAM starting from 0x0000_0000. You can get ending address by calling allocator::memory_map() function implemented in the lab3. Note that this space is normal memory that is internally shared. Set proper settings for L3 entries and set those entries in the page table.\nThen, set L3 entries for I/O memory range from IO_BASE to IO_BASE_END as device memory. The range variables are defined in pi/src/common.rs. The entries should have same settings with the one for normal memory space, except that its attribute should be set as device memory and it is outer sharable. Also, make sure to set the proper bits value for each page table entry you create.\nNote that we are using 64KB page granularity for both cases.\nImplement UserPageTable struct.\nFor the user page table, you don\u0026rsquo;t need to set any pages into the pagetable in creation time.\nWhen alloc() function is called, allocates a 64KB page, creates an L3 entry with proper settings, set the physical address of the allocated page to L3 entry\u0026rsquo;s ADDR field, and set the entry in the page table. Before looking up the page table with virtual address, note that the virtual address for user process starts from USER_IMG_BASE, which is 0xffff_ffff_c000_0000. You have to subtract this base address from the virtual address to lookup the page table and set entries. Make sure to set the proper bits value for each page table entry you create.\nImplement Drop traits for UserPageTable.\nTo implement Drop traits, iterate the internal pagetable and dealloc() each entry that exists.\nLet a process struct have a UserPageTable.\nYou can simple do this by uncommenting pub vmap: Box\u0026lt;UserPageTable\u0026gt;, line within the Process struct in process.rs. Modify Process::new() method to return the Process struct holding vmap.\nFinally, implement VMManager in kern/src/vm.rs.\nThe virtual memory manager is a thread-safe wrapper around a kernel page table. Before its first use, it must be initialized by calling initialize() which internally calls setup() method. Setting up the virtual machine manager includes configuring some proper values to the several relevant registers.\nHere is the registers used by the setup() method :\nID_AA64MMFR0_EL1 : AArch64 Memory Model Feature Register 0 (ref : D7.2.43)\nTgran64 field is used to check whether the current system support 64KB memory translation granule size or not. startup() panics if it\u0026rsquo;s not supported. PARange field is used to check the range supported for physical address.\nMAIR_EL1 : Memory Attribute Indirection Register (EL1) (ref : D7.2.70)\nProvides the memory attribute encoding corresponding to the possible AttrIndx.\nTCR_EL1 : Translation Control Register (ref : D7.2.91)\nIt controls other memory management features at EL1 and EL0. (guide : 12.2)\nTTBR0_EL1 and TTBR1_EL1\nSpecifies the base address of the translation table.\nSCTLR_EL1 : System Control Register (ref : D7.2.88)\nProvides top level control of the system including memory system.\nNow complete the implementation for VMManager by writing codes for unimplemented methods.\nInstead of using raw bit and bit operation, use wrapper functions and variables from aarch64 library to prevent potential typos and mistakes.\nWe also have interface for registers in aarch64 library.\nWhy do we need Deref and DerefMut traits? (Deref)\nIn pagetable.rs, we have Deref and DerefMut traits for KernPageTable and UserPageTable. What are the roles of these traits and why do we need them?\nWe do not support page access rights permissions\nEven though in the implementation of UserPageTable::alloc you might see _perm for memory page access rights permission (i.e. RW pages or RWX), we do not actually use it. It is there just as a stub for future support of this feature. Note, however, that we do support MMU page permissions isolating the kernel and user space pages. We do this by setting the proper values in the page table entries, AP.\nTesting Virtual Memory First, we\u0026rsquo;ll add one line of code in kern/src/main.rs. Call initialize() method for VMM before calling scheduler initialization. Thus, your kmain() will now have six lines like below:\nunsafe fn kmain() -\u0026gt; ! { ALLOCATOR.initialize(); FILESYSTEM.initialize(); IRQ.initialize(); VMM.initialize(); SCHEDULER.initialize(); SCHEDULER.start(); } As your kernel now enables virtual memory, your process should have a user page table. We\u0026rsquo;re going to implement several user programs as well as loading those programs as a process in the next phase. Before moving to the next phase, let\u0026rsquo;s simply test whether your user process can be run with user page table.\nIn initialize() function in kern/src/process/scheduler.rs. We can start using the virtual address instead of the physical address.\nSince we do not have any process in the virtual space, yet. We have provided a test function test_phase_3. This function, given a process, will allocate a new page at address USER_IMG_BASE in the process\u0026rsquo;s virtual space, and will copy the function test_user_process to that page. So that now if we set elr to USER_IMG_BASE, it should run test_user_process.\nSet ttbr0 and ttbr1 appropriately for each process. ttbr0 should have the base address of the kernel page table while ttbr1 should have the base address for the user page table. Then, call test_phase_3() function for each process, so that they have something to execute at USER_IMG_BASE, and set elr to USER_IMG_BASE. test_user_process is a simple function that invokes a sleep system call. Finally, add the processes to the scheduler queue.\nIf everything works well, you can see your each process sleeps for 10 seconds falling to the Waiting status right after it is scheduled. You may want to print timer interrupts and scheduling queue to check it.\nPhase 4: Programs In The Disk In this phase, you will further modify Process structure implementation to load programs binaries from the filesystem image to create a process.\nSubphase A: Load A Program In the previous phase, we generated user-level processes using extern function and started them by passing the address of the extern function to the ELR register of trap frame. However, implementing all the user programs in the kernel code is not practical nor safe. Instead, we\u0026rsquo;ll implement features to load programs written by the user outside of the kernel, compiled and stored in the disk as binary.\nIn this subphase, you will be working in kern/src/process/process.rs.\nTo convert a program binary from the disk to a Process struct, you need the following steps:\nOpen the binary file, and create a process object.\nAllocate a 64KB page within the process\u0026rsquo;s user page table for user process\u0026rsquo;s stack. This page should have read and write permission.\nAllocate a 64KB page within the process\u0026rsquo;s user page table with virtual address starting from USER_IMG_BASE. This page should have read, write and execute permissions. Start reading the binary file and store it in the allocated page. Keep allocating additional pages until you are done reading the whole binary file.\nSet the trap frame for the process with the proper values.\nImplementation First, complete the helper functions. There are four functions returning VirtualAddr in kern/src/process/process.rs. You can find relevant constant variables defined in kern/src/param.rs\nget_max_va() : Returns the highest virtual address for user process.\nget_image_base() : Returns the base address for the user virtual memory space.\nget_stack_base() : Returns the base address for the user process\u0026rsquo;s stack in the virtual space. Since stacks grow from higher memory address to lower memory addresses, a good stack base would be the last page in the user\u0026rsquo;s virtual space. Note that the address should be aligned by PAGE_SIZE.\nget_stack_top() : Returns the top of the user process\u0026rsquo;s stack. This will be set to the stack pointer once the process is run. It should be the maximum stack pointer possible. Remember, in ARM, stack pointers are 16 byte aligned.\nThen, complete load() and do_load() methods.\ndo_load() method gets a path to the file as a parameter and returns a wrapped Process struct. do_load needs to create a new process struct, allocate the stack in process virtual space, opens a file at the given path and read its content into the process virtual space starting at address USER_IMG_BASE.\nload() method internally call do_load() method. Then, it should sets the trap frame for the process with the proper virtual addresses in order to make the process run with user page table. Finally, it returns the process object ready to be run.\nCompiling User Programs Now that your operating systems support loading and running user programs for the file system, we should give it a run. But first, we need to compile the user programs to our OS needs.\nIn commodity OSes, like Linux, executables are usually compiled into an executable file format such as elf. The process of loading user programs into memory and getting them to run is usually more complicated than what we do in our operating system. You may read about how it is done in the Linux kernel How programs get run: ELF binaries .\nFor our operating system, however, we do not have an elf interpreter, and so we will need to compile our programs into a plain binary file that expect to be loaded at address USER_IMG_BASE.\nWe have provided two user programs ready for you to compile at user directory. fib actually needs the system call write which we have not implemented yet. So, we will base our example on sleep, which have the same behavior as the test_user_process function you have seen before.\nTo compile sleep, run make at user/sleep. This will generate two files at user/sleep/build: sleep.bin and sleep.elf.\nsleep.elf is the elf version of the sleep program. Since we do not have an elf interpreter, we cannot use it in our operating system. Instead, we will use sleep.bin, which is literally just a stream of the executable bytes.\nOnce you have compiled sleep, copy sleep.bin into the SD card, and move on to the testing phase.\nTest your implementation We provides two user programs fib and sleep under user directory. As the name suggests, sleep is a program that calls the sleep system call. It has the exact same codes as the previous test_user_process() function. The behaviour of fib is also very simple. It just calculates 40th fibonacci number with recursive fib() function. You can find the source code in user/fib/src/main.rs.\nThen, revisit the kern/src/process/scheduler.rs. Now, we don\u0026rsquo;t have to manually set the registers and add the processes created from an extern function, but we can load multiple programs from our disk using Process::load function. Load four sleep programs and add them in the scheduling queue in initialize() method.\nFinally, modify inline assembly in GlobalScheduler::start() function. Instead of the address of _start function, calculate the address of the next page and store it to the sp register. This way, we will have a clean page for the stack when we return to the kernel level. Before calling eret, don\u0026rsquo;t forget to clean any register that may leak information or addresses from the kernel.\nAfter boot up, if the four sleep programs starts, you can continue to the next subphase. Note that you can\u0026rsquo;t test fib program yet because we haven\u0026rsquo;t implemented write system call. We will implement it and some more system calls in the next subphase.\nIf you wish to use qemu, and need a file system image that have the user programs inside of it, you can use user/build.sh script. This script will generates fs.img that contains compiled binary of sleep and fib program.\nSubphase B: User Processes In this subphase, we will extend our system call library to make it more useful to user programs. We will also write and compile an example user program in Rust, which should be run on top of our operating system. Finally, we will apply changes in our system call handler to add the new system calls. You will be working primarily in lib/kernel_api/src/syscall.rs and kern/src/traps/syscall.rs.\nUser Programs First, we will build the kernel api library for our operating system. This library will be imported by user level programs to initiate a system call to our operating system.\nStart by implementing functions in lib/kernel_api/src/syscall.rs. Each function should be implemented to invoke system call corresponding to the name of the function.\nTo give you an example, lets walk through how does system call are handled in our system. The user program import the library kernel_api, and use its functions. The functions in kernel_api will implement the required assembly to issue the system call to the operating system, with the appropriate system call number and arguments. This will trap into the operating system handle_exception, which will forward the request to handle_syscall. Similarly, handle_syscall will categorize the request based on the syscall number, and forward it to the responsible handler. The handler will execute the required functionality of the system call, and return back through the chain of calls to the user level.\nStart implementing the library functions at lib/kernel_api/src/syscall.rs. All you have to do in these function is to prepare the argument for the required system call, and issue the svc instruction. We have provided sleep as an example. The system call numbers are defined in lib/kernel_api/src/lib.rs. Please refer to lib/kernel_api/src/lib.rs to find pre-defined variables for system call number.\nThen, for each system call you implemented, make sure you complete the full chain of handling it just as we described above, i.e. add it to handle_syscall and write the respective sys_{SYSCALL} function in kern/src/traps/syscall.rs.\nWe recommend you to start from implementing write system call. When you are done implementing it, you can now run multiple fib programs with the same way you did for the sleep program.\nIf all is well, you can see fib processes running and their output is printed.\nNote that you have to recompile the user programs whenever you are making changes to kernel_api.\nWhen you are done with implementing every functions in the kernel_api user library as well as its system call handler in the kernel, try to call and test every system call in the fib program. Also, if you feel adventurous, write your own programs. You can copy the same build system from fib and use it in your program to generate a working binary.\nCheck your code before submit!\nBefore submitting your code, please change your TICK variable in kern/src/param.rs to 10 ms. In addition, please check you are not leaving any system calls nor exceptions you used for debugging. Your main should have all 6 lines: 5 initialize() calls and 1 start() call.\nSubmission Once you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Congratulations!\nYou can call make check in tut/4-spawn directory to check if you\u0026rsquo;ve answered every question.\nOnce you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Ensure you\u0026rsquo;ve committed your changes. Any uncommitted changes will not be visible to us, thus unconsidered for grading.\nWhen you\u0026rsquo;re ready, push a commit to your GitHub repository with a tag named lab4-done.\n# submit lab4 $ git tag lab4-done $ git push --tags ", 
        "url": "\/\/localhost:1313\/post\/lab4\/"
    },
    
    "\/\/localhost:1313\/post\/lab3\/": {
        "title": "Lab 3: FAT32 Filesystem",
        "tags": ["Lab",],
        "content": " Handed out: Tuesday, February 11, 2020 Due: Monday, March 2, 2020 Introduction In this assignment, you will enable the use of Rust\u0026rsquo;s collections module (Vec, String, HashMap, and friends) by writing a memory allocator, implement the FAT32 file system, implement a Rust interface for a driver for the Raspberry Pi\u0026rsquo;s EMMC (SD card controller), and extend your shell with cd, ls, pwd, and cat, commands.\nPhase 0: Getting Started Fetch the update for lab 3 from our git repository to your development machine.\n$ git fetch skeleton $ git merge skeleton/lab3 This is the directory structure of our repository. The directories you will be working on this assignment are marked with *.\n. ├── bin : common binaries/utilities ├── doc : reference documents ├── ext : external files (e.g., resources for testing) ├── tut : tutorial/practices │ ├── 0-rustlings │ ├── 1-blinky │ ├── 2-shell │ └── 3-fs : questions for lab3 * ├── boot : bootloader ├── kern : the main os kernel * └── lib : required libraries ├── fat32 * ├── pi * ├── shim ├── stack-vec ├── ttywrite ├── volatile └── xmodem You may need to resolve conflicts before continuing. For example, if you see a message that looks like:\nAuto-merging kern/src/main.rs CONFLICT (content): Merge conflict in kern/src/main.rs Automatic merge failed; fix conflicts and then commit the result. You will need to manually modify the main.rs file to resolve the conflict. Ensure you keep all of your changes from lab 2. Once all conflicts are resolved, add the resolved files with git add and commit. For more information on resolving merge conflicts, see this tutorial on githowto.com .\nmake transmit command Since you\u0026rsquo;ve finished writing the bootloader in the previous lab, you are ready to use the command make transmit that builds the kernel binary and calls ttywrite to send it to the Raspberry Pi for the bootloader to load. As a result, assuming the bootloader is installed as kernel8.img, you will be able to test new binaries simply by resetting your Raspberry Pi and running make transmit.\nYou should have installed ttywrite utility in the previous lab. If you didn\u0026rsquo;t for some reason, install it now by running cargo install --path . in the lib/ttywrite directory. Ensure that the utility was properly installed by running ttywrite --help.\nThe make transmit target is configured to write to /dev/ttyUSB0 by default. If your TTY device differs, modify the TTY_PATH declaration on line 7 of kern/Makefile appropriately.\nAdd your user to dialout group\nIf you are experiencing a permission issue when accessing the TTY, please try adding your user to the dialout group.\nsudo usermod -a -G dialout $USER sudo reboot Compilation errors after merging lab3\nWe provide a template code for the final phase of lab3, which contains lots of uncompleted code. When you try to compile your code while working on this lab, you will see the compiler complains about some of the code you just merged. You can comment the offensive lines of the code until we fix them later. For instance, you may want to disable the file system component when working in phase 1. Make sure you only comment out minimum amount of the code!\nThe ALLOCATOR.initialize() call panics!\nYour shell should continue to function as before. If you test the make install target now, however, you\u0026rsquo;ll likely find that you shell appears to no longer work. The likely culprit is an ALLOCATOR.initialize() call preceding your shell() call. Because there is no memory allocator yet, the call will lead to a panic!(), halting your system without warning. We\u0026rsquo;ll fix this soon. Feel free to comment out the line temporarily to ensure everything is working as expected.\nPhase 1: Memory Lane In this phase you will implement two memory allocators: a simple bump allocator and a more fully-featured bin allocator. These will immediately enable the use of heap allocating structures such as Vec, Box, and String. To determine the available memory on the system for allocation, you will read ARM tags (ATAGS ). You will also implement the panic handler to properly handle panic! calls.\nSubphase A: Panic! In this subphase you will implement the panic handler. You will be working in kern/src/init/panic.rs.\nError Handling in Rust Rust has two major categories of errors: recoverable and unrecoverable. Rust represents recoverable errors with Result\u0026lt;T, E\u0026gt; type. On the other hand, when a Rust program encounters an unrecoverable error, it stops the program execution altogether. This behavior is called panic! in Rust terminology.\nWhen targeting standard operating systems, the Rust compiler will generate a program that prints the backtrace and sets the process exit code on panic. However, when the Rust compiler is instructed to compile a Rust program for a target without operating system support, such as we do for our Raspberry Pi, the compiler requires the manual implementation of the panic handler.\nA panic handler is a function that is called when a panic! occurs. It has a type of fn panic(info: PanicInfo) -\u0026gt; !, which means it takes a PanicInfo as an argument and never returns. PanicInfo struct contains the information of the file name, line number, and column where the panic! occurred.\nWe\u0026rsquo;ve provided the panic handler that loops indefinitely in kern/src/init/panic.rs. You will extend this panic implementation so that it logs useful information to the console.\nImplement the panic handler Implement the panic function now. Your implementation should print the passed in information to the console and then allow the loop already in place to run. You\u0026rsquo;re free to implement the function as you like. As an example, our implementation takes inspiration from Linux kernel oops messages:\n( ( ) ) ) ( ( ( ` .-\u0026#34;\u0026#34;^\u0026#34;\u0026#34;\u0026#34;^\u0026#34;\u0026#34;^\u0026#34;\u0026#34;\u0026#34;^\u0026#34;\u0026#34;-. (//\\\\//\\\\//\\\\//\\\\//\\\\//) ~\\^^^^^^^^^^^^^^^^^^/~ `================` The pi is overdone. ---------- PANIC ---------- FILE: src/kmain.rs LINE: 40 COL: 5 index out of bounds: the len is 3 but the index is 4 Test your new panic implementation by having your kernel panic. Recall that you can use the new make install target to compile and send the kernel to your Raspberry Pi. Note that the ALLOCATOR.initialize() call already panic! s, so you shouldn\u0026rsquo;t need to make any changes. Ensure this function is called before your shell().\nThen, try making your kernel panic in other ways: a rogue unwrap(), an explicit panic!(), or an unreachable!(): ensure they all work as expected. When you\u0026rsquo;re satisfied with your implementation, continue to next the subphase.\nSubphase B: ATAGS In this subphase, you will implement an iterator over the ARM tags (ATAGS) loaded by the Raspberry Pi\u0026rsquo;s firmware. You will use your iterator to find the ATAG that specifies how much memory is available on the system. You will be working in the lib/pi/src/atags directory and kern/src/allocator.rs.\nARM Tags ATAGS , or ARM tags, are a mechanism used by ARM bootloaders and firmware to pass information about the system to the kernel. Linux, for example, can use ATAGS when configured for the ARM architecture.\nThe Raspberry Pi places an array of ATAG structures at address 0x100. This is the structure of ATAGS, in Rust syntax:\n#[repr(C)] struct Atag { dwords: u32, tag: u32, kind: Kind } Each ATAG begins with an 8 byte header, dwords and tag. The dwords field specifies the size of the complete ATAG in double words (32-bit words) and includes the header. Thus the minimum size is 2. The tag field specifies the type of the ATAG. There are 10 different types of specified tags, all documented in the ATAGS reference . The Raspberry Pi only makes use of four. These are documented below:\nName Type (tag) Size Description CORE 0x54410001 5 or 2 if empty First tag used to start list NONE 0x00000000 2 Empty tag used to end list MEM 0x54410002 4 Describes a physical area of memory CMDLINE 0x54410009 variable Command line to pass to kernel The type of tag determines how the data after the header should be interpreted. In our skeleton code, the data following the header is represented as a field named kind which is a union of different kind of tags. Clicking on the name of the tag in the table above directs you to the reference for that particular tag which includes the layout of the tag\u0026rsquo;s data. The MEM tag data, for instance, is structured as below:\nstruct Mem { size: u32, start: u32 } Tags are laid out sequentially in memory with zero padding between each tag. The first tag is specified to be a CORE tag while the final tag is indicated by the NONE tag. Other tags can appear in any order. The dwords field is used to determine the address of the adjacent ATAG. The diagram below depicts the general layout.\nUnions \u0026amp; Safety The raw ATAG data structures are declared in lib/pi/src/atags/raw.rs. The main declaration, copied below, makes use of a Rust union. Rust\u0026rsquo;s unions are identical to C unions: they define a structure where all fields share common storage.\npub struct Atag { dwords: u32, tag: u32, kind: Kind } pub union Kind { core: Core, mem: Mem, cmd: Cmd } In effect, unions allow memory to be cast into arbitrary structures without regard for whether the cast is correct. As a result, accessing union fields in Rust is unsafe.\nWe\u0026rsquo;ve already handled most of the unsafe in the atags module for you, so you don\u0026rsquo;t need to worry about handling unions yourself. Nonetheless, exposing unions to end-users of our pi library is a bad idea. Because of this, we\u0026rsquo;ve declared a second Atag structure in lib/pi/src/atags/atag.rs. This structure is entirely safe to use and access. This is the structure that the pi library will expose. When you finish the implementation of the atag module later in this subphase, you\u0026rsquo;ll write conversions from the raw structures to the safe structures.\nWhy is it a bad idea to expose unions to end-users? (enduser-unsafe)\nWe\u0026rsquo;re going through a lot of effort to expose a safe interface to unsafe data structures. You\u0026rsquo;ll see this over and over again in Rust, with the standard library as a prime example. What benefit is there to exposing safe interfaces to unsafe structures or operations in Rust? Could we yield the same benefits in a language like C?\nCommand Line Arguments The CMDLINE tag deserves special attention. Its declaration is:\nstruct Cmd { /// The first byte of the command line string. cmd: u8 } As indicated by the comment, the cmd field holds the first byte of the command line string. In other words, \u0026amp;cmd is a pointer to a null-terminated, C-like string. The safe version of the Cmd tag is Cmd(\u0026amp;'static str). When you write the conversion from the raw to safe version of the Cmd tag, you\u0026rsquo;ll need to determine the size of the C-like string by searching for the null terminator in the string. You\u0026rsquo;ll then need to cast the address and size into a slice using slice::from_raw_parts() and finally cast the slice into a string using str::from_utf8() or str::from_utf8_unchecked(). You used both of these functions before in lab 2.\nImplement atags You\u0026rsquo;re ready to implement the atags module in lib/pi/src/atags. Start by implementing the raw::Atag::next() method in atags/raw.rs. The method determines the address of the ATAG following self and returns a reference to it. You\u0026rsquo;ll need to use unsafe in your implementation. Then implement the helper methods and conversion traits from raw structures to safe structures in atags/atag.rs. You should only need to use unsafe when implementing From\u0026lt;\u0026amp;'a raw::Cmd\u0026gt; for Atag. Finally, finish the implementation of the Iterator trait for Atags in atags/mod.rs. This requires no unsafe.\nYou can convert from x: \u0026amp;T to *const u32 using x as *const T as *const u32.\nYou can convert from x: *const T to \u0026amp;T using \u0026amp;*x. However, this conversion is extremely unsafe. Make sure that you don\u0026rsquo;t violate the alias rule of Rust references.\nYou can perform pointer arithmetic with add() , sub() , or offset() .\nTesting atags Test your implementation by running cargo test command in lib/pi directory. Then, test your ATAGS implementation with the RPi board by iterating over all of the ATAGS and debug printing them to your console in kern/src/main.rs. You should see at least one of each of the three non-NONE tags. Verify that the value of each ATAG matches your expectations. Once your implementation performs as expected, proceed to the next subphase.\nThe {:#?} format specifier prettifies the debug output of a structure.\nWhat does the CMDLINE ATAG contain? (atag-cmdline)\nWhat is the value of the command line string in the CMDLINE ATAG found on your Raspberry Pi? What do you think the parameters control?\nHow much memory is reported by the MEM tag? (atag-mem)\nWhat is the exact start address and size of the available memory reported by the MEM ATAG? How close is this to the Raspberry Pi\u0026rsquo;s purported 1GB of RAM?\nSubphase C: Warming Up In this subphase, we\u0026rsquo;ll set the stage to write our two memory allocators in the next subphases. You\u0026rsquo;ll implement two utility functions, align_up and align_down, that align addresses to a power of two. You\u0026rsquo;ll also implement the memory_map function that returns the start and end address of the available memory on the system. Your memory_map function will be used by both memory allocators to determine the available memory for allocation.\nAlignment A memory address is n-byte aligned if it is a multiple of n. Said another way, a memory address k is n-byte aligned if k % n == 0. We don\u0026rsquo;t usually need to be concerned about the alignment of our memory addresses, but as budding system\u0026rsquo;s programmers, we do! This is because hardware, protocols, and other external forces enjoin alignment properties. For example, the ARM 32-bit architecture requires the stack pointer to be 8-byte aligned. The AArch64 architecture, our operating system\u0026rsquo;s architecture of choice, requires the stack pointer to be 16-byte aligned; x86-64 requires the same alignment. Page addresses used for virtual memory typically need to be 4k-byte aligned. And there are many more examples, but it suffices to say that alignment of memory addresses is important.\nIn C, the alignment of a memory address returned from a libC allocator is guaranteed to be 8-byte aligned on 32-bit systems and 16-byte aligned on 64-bit systems. Beyond this, the caller has no control over the alignment of the returned memory address and must fend for themselves (POSIX functions like posix_memalign later corrected for this).\nWhy did C choose these alignments? (libc-align)\nThe choice to guarantee 8 or 16-byte alignment from libC\u0026rsquo;s malloc is not without reason. Why did libC choose these particular alignment guarantees?\nRecall the signatures for malloc() and free() in C:\nvoid *malloc(size_t size); void free(void *pointer); In contrast, Rust\u0026rsquo;s low-level, unsafe alloc and dealloc methods in GlobalAlloc trait have the following signatures:\n// `layout.size()` is the requested size, `layout.align()` the requested alignment unsafe fn alloc(\u0026amp;self, layout: Layout) -\u0026gt; *mut u8; // `layout` should be the same as was used for the call that returned `ptr` unsafe fn dealloc(\u0026amp;self, ptr: *mut u8, layout: Layout) Note that the caller can specify the alignment with the layout argument, which is defined by two parameters, size and align. As a result, the onus is on the allocator, not the caller, to return a properly aligned memory address. When you implement memory allocators in the next phase, you\u0026rsquo;ll need to ensure that the address you return satisfies the condition specified by the layout parameter.\nThe second thing to note is that the dealloc function, analogous to C\u0026rsquo;s free, requires the caller to pass in the Layout used for the original call to alloc. As a result, the onus is on the caller, not the allocator, to remember the requested size and alignment of an allocation.\nSize and alignment guarantee in Rust\nIn Rust, all layouts must have non-negative size and a power-of-two alignment; These conditions are checked when a layout is created.\nWhy do you think Rust split responsibilities in this way? (onus)\nIn C, the allocator has fewer restrictions on the alignment of memory addresses it returns but must record the size of an allocation for later use. The inverse is true in Rust. Why do you think Rust chose the opposite path here? What advantages does it have for the allocator and for the caller?\nUtilities: align_up and align_down When you implement your allocators in the next subphases, you\u0026rsquo;ll find it useful to, given a memory address u, be able to determine the first address \u0026gt;= or \u0026lt;= u that is aligned to a power of two. The (unimplemented) align_up and align_down functions in kernel/src/allocator/util.rs do exactly this:\n/// Align `addr` downwards to the nearest multiple of `align`. /// Panics if `align` is not a power of 2. fn align_down(addr: usize, align: usize) -\u0026gt; usize; /// Align `addr` upwards to the nearest multiple of `align`. /// Panics if `align` is not a power of 2 /// or aligning up overflows the address. fn align_up(addr: usize, align: usize) -\u0026gt; usize; Implement these functions now. You can unit test your implementations by calling make test or cargo test in the kernel directory. This will run the tests in kern/src/allocator/tests.rs. All of the align_util unit tests should pass.\nTesting\nDuring testing, calls to kprint{ln}! become calls to print{ln}!.\nThread Safety Memory allocators like libC\u0026rsquo;s malloc() and the two you will soon implement are global: they can be called by any thread at any point in time. As such, the allocator needs to be thread safe, and that\u0026rsquo;s why alloc() and dealloc() method take shared (aliasable) reference \u0026amp;self``, like other synchronization primitives such as Mutex and RwLock . Rust takes thread safety very seriously, and so it is difficult to implement an allocator that isn\u0026rsquo;t thread-safe even if our system doesn\u0026rsquo;t have any concurrency mechanisms like threads just yet.\nThe topic of thread-safe memory allocators is extensive, and many research papers have been published on exactly this topic. To avoid a deep tangent, we\u0026rsquo;ll ignore the topic altogether and wrap our allocator in a Mutex ensuring that it is thread-safe by virtue of exclusion. We\u0026rsquo;ve provided the code that will wrap your allocators in kern/src/allocator.rs. Read through the code now. Notice how it implements Rust\u0026rsquo;s GlobalAlloc trait; this is how Rust knows that it is a valid allocator. An implementation of this trait is required to register an instance of the struct as a #[global_allocator], which we\u0026rsquo;ve done for you in main.rs. Once an instance is registered via the #[global_allocator] annotation, we can use structures like Vec``, ``String``, and ``Box`` via the alloc crate and Rust will forward the alloc() and dealloc() calls to our registered instance.\nUtility: memory_map The final item in the kern/src/allocator.rs file is the memory_map function. This function is called by the Allocator::initialize() method which in-turn is called in kmain(). The initialize() method constructs an instance of the internal imp::Allocator structure for use in later allocations and deallocations.\nThe memory_map function is responsible for returning the start and end address of all of the free memory on the system. Note that the amount of free memory is unlikely to be equal to the total amount of memory on the system, the latter of which is identified by ATAGS. This is because memory is already being used by data like the kernel\u0026rsquo;s binary. memory_map should take care not to mark used memory as free. To assist you with this, we\u0026rsquo;ve declared the binary_end variable which holds the first address after the kernel\u0026rsquo;s binary.\nImplement the memory_map function now by using your Atags implementation from Subphase B and the binary_end variable. Ensure that the function returns the expected values. Then add a call to String::from(\u0026quot;Hi!\u0026quot;) (or any other allocating call) and ensure that a panic!() occurs because of an unimplemented bump allocator. If memory_map() returns what you expect and a call to AllocatorImpl::new() panics because the bump allocator hasn\u0026rsquo;t been implemented yet, proceed to the next subphase.\nSubphase D: Bump Allocator In this subphase, you will implement the simplest of allocators: the bump allocator. You will be working in kern/src/allocator/bump.rs.\nSwitching Implementations\nThe GlobalAlloc implementation for Allocator in kernel/src/allocator.rs simply forwards calls to an internal AllocatorImpl after taking a lock. We\u0026rsquo;ll start with the bump::Allocator in bump.rs and later switch to the bin::Allocator in bin.rs.\nA bump allocator works like this: on alloc, the allocator returns a current pointer, modified as necessary to guarantee the requested alignment, and bumps the current pointer up by the size of the requested allocation plus whatever was necessary to fulfill the alignment request. If the allocator runs out of memory, it returns an error. On dealloc, the allocator does nothing.\nThe diagram below depicts what happens to the current pointer after a 1k byte allocation and a subsequent 512 byte allocation. Note that alignment concerns are absent in the diagram.\nYour task is to implement a bump allocator in kernel/src/allocator/bump.rs. In particular, implement the new(), alloc(), and dealloc() methods of bump::Allocator. Use your align_up and align_down utility functions as necessary to guarantee the proper alignment of the returned addresses. We\u0026rsquo;ve provided unit tests that check the basic correctness of your implementation. You can run them with make test or cargo test in the kernel directory. You should pass all of the allocator::bump_ unit tests.\nEnsure that you don\u0026rsquo;t perform any potentially overflowing operations!\nUse the saturating_add and saturating_sub methods as necessary to prevent arithmetic overflow.\nOnce all of the unit tests pass, try allocating memory in kmain() to \u0026ldquo;see\u0026rdquo; your allocator in action. Here\u0026rsquo;s a simple test:\nuse alloc::vec::Vec; let mut v = Vec::new(); for i in 0..50 { v.push(i); kprintln!(\u0026#34;{:?}\u0026#34;, v); } Once your implementation works as expected, proceed to the next subphase.\nWhat does the alloc call chain look like? (bump-chain)\nIf you paused execution when bump::Allocator::alloc() gets called, what would the backtrace look like? Asked another way: explain in detail how a call like v.push(i) leads to a call to your bump::Allocator::alloc() method.\nSubphase E: Bin Allocator In this subphase, you will implement a more complete allocator: the bin allocator. You will be working in kern/src/allocator/bin.rs.\nA bin allocator segments memory allocations into size classes, or bins. The specific size classes are decided arbitrarily by the allocator. Each bin holds a linked-list of pointers to memory of the bin\u0026rsquo;s size class. Allocations are rounded up to the nearest bin: if there is an item in the bin\u0026rsquo;s linked list, it is popped and returned. If there is no free memory in that bin, new memory is allocated from the global pool and returned. Deallocation pushes an item to the linked list in the corresponding bin.\nOne popular approach is to divide bins into powers of two. For example, an allocator might choose to divide memory allocations into k - 2 bins with sizes 2^n for n from 3 to k (2^3, 2^4, \u0026hellip;, 2^k). Any allocation or deallocation request for less than or equal to 2^3 bytes would be handled by the 2^3 bin, requests between 2^3 and 2^4 bytes from the 2^4 bin, and so on:\nbin 0 (2^3 bytes): handles allocations in (0, 2^3] bin 1 (2^4 bytes): handles allocations in (2^3, 2^4] \u0026hellip; bin 29 (2^32 bytes): handles allocations in (2^31, 2^32] Linked List We\u0026rsquo;ve provided an implementation of an intrusive linked list of memory addresses in kern/src/allocator/linked_list.rs. We\u0026rsquo;ve also imported the LinkedList struct in kern/src/allocator/bin.rs.\nWhat\u0026rsquo;s an instrusive linked list?\nIn an intrusive linked list, next and previous pointers, if any, are stored in the pushed items themselves. An intrusive linked list requires no additional memory, beyond the item, to manage an item. On the other hand, the user must provide valid storage in the item for these pointers.\nA new, empty list is created using LinkedList::new(). A new address can be prepended to the list using push(). The first address in the list, if any, can be removed and returned using pop() or returned (but not removed) using peek():\nlet mut list = LinkedList::new(); unsafe { list.push(address_1); list.push(address_2); } assert_eq!(list.peek(), Some(address_2)); assert_eq!(list.pop(), Some(address_2)); assert_eq!(list.pop(), Some(address_1)); assert_eq!(list.pop(), None); LinkedList exposes two iterators. The first, obtained via iter(), iterates over all of the addresses in the list. The second, returned from iter_mut(), returns Node s that refer to each address in the list. The value() and pop() methods of Node can be used to read the value or pop the value from the list, respectively.\nlet mut list = LinkedList::new(); unsafe { list.push(address_1); list.push(address_2); list.push(address_3); } for node in list.iter_mut() { if node.value() == address_2 { node.pop(); } } assert_eq!(list.pop(), Some(address_3)); assert_eq!(list.pop(), Some(address_1)); assert_eq!(list.pop(), None); Read through the code for LinkedList now. Pay special attention to the safety properties required to call push() safely. You\u0026rsquo;ll likely want to use LinkedList to manage the bins in your memory allocator.\nWhy is it convenient to use an intrusive linked list? (ll-alloc)\nUsing an intrusive linked list for our memory allocators turns out to be a very convenient decision. What issues would arise if we had instead decided to use a regular, allocate-additional-memory-on-push, linked list?\nFragmentation The concept of fragmentation refers to memory that is unused but unallocatable. An allocator incurs or creates high fragmentation if it creates a lot of unusable memory throughout the course of handling allocations. An ideal allocator has zero fragmentation: it never uses more memory than necessary to handle a request and it can always use available memory to handle new requests. In practice, this is neither desired nor achievable given other design constraints. But striving for low fragmentation is a key quality of good memory allocators.\nWe typically define two kinds of fragmentation:\ninternal fragmentation\nThe amount of memory wasted by an allocator to due to rounding up allocations. For a bin allocator, this is the difference between a request\u0026rsquo;s allocation size and the size class of the bin it is handled from.\nexternal fragmentation\nThe amount of memory wasted by an allocator due to being unable to use free memory for new allocations. For a bin allocator, this is equivalent to the amount of free space in every bin that can\u0026rsquo;t be used to handle an allocation for a larger request even though the sum of all of the free space meets or exceeds the requested size.\nYour allocator should try to keep fragmentation down within reason.\nImplementation Implement a bin allocator in kern/src/allocator/bin.rs. Besides being a bin-like allocator, the design of the allocator is entirely up to you. The allocator must be able to reuse freed memory. The allocator must also not incur excessive internal or external fragmentation. Our unit tests, which you can run with make test to check these properties. Remember to change AllocatorImpl to bin::Allocator in kern/src/allocator.rs so that your bin allocator is used for global allocations.\nOnce your allocator passes all tests and is set as the global allocator, proceed to the next phase.\nWhat does your allocator look like? (bin-about)\nBriefly explain the design of your allocator. In particular answer the following questions:\nWhich size classes did you choose and why? How does your allocator handle alignment? What are the bounds on internal and external fragmentation for your design choices? How could you decrease your allocator\u0026rsquo;s fragmentation? (bin-frag)\nYour allocator probably creates more fragmentation that it needs to, and that\u0026rsquo;s okay! How could you do better? Sketch (only in writing) two brief design ideas for improving your allocator\u0026rsquo;s fragmentation.\nPhase 2: 32-bit Lipids In this phase, you will implement a read-only FAT32 file system. You will be working primarily in the lib/fat32 directory.\nDisks and File Systems Data on a disk is managed by one or more file systems. Much like a memory allocator, a file system is responsible for managing, allocating, and deallocating free disk space. Unlike the memory managed by an allocator, the disk is persistent: barring disk failure, a write to allocated disk space is visible at any point in the future, including after machine reboots. Common file systems include EXT4 on Linux, HFS+ and APFS on macOS, and NTFS on Windows. FAT32 is another file system that is implemented by most operating systems, including Linux, macOS, and Windows, and was used in older versions of Windows and later versions of DOS. Its main advantage is its ubiquity: no other file system sees such cross-platform support.\nTo allow more than one file system to reside on a physical disk, a disk can be partitioned. Each partition can formatted for a different file system. To partition the disk, a table is written out to a known location on the disk that indicates where each partition begins and ends and the type of file system the partition uses. One commonly used partitioning scheme uses a master boot record, or MBR, that contains a table of four partition entries, each potentially unused, marking the start and size of a partition. GPT is a more modern partitioning scheme that, among other things, allows for more than four partitions.\nIn this assignment you will be writing the code to interpret an MBR partitioned disk that includes a FAT32 partition. This is the combination used by the Raspberry Pi: the SD card uses the MBR scheme with one partition formatted to FAT32.\nDisk Layout The following diagram shows the physical layout of an MBR-partitioned disk with a FAT32 file system:\nThe FAT structures PDF contains the specific details about all of these structures including their sizes, field locations, and field descriptions. You will be referring to this document when you implement your file system. You may also find the FAT32 design Wikipedia entry useful while implementing your file system.\nMaster Boot Record The MBR is always located on sector 0 of the disk. The MBR contains four partition entries, each indicating the partition type (the file system on the partition), the offset in sectors of the partition from the start of the disk, and a boot/active indicator that dictates whether the partition is being used by a bootable system. Note that the CHS (cylinder, header, sector) fields are typically ignored by modern implementations; your should ignore these fields as well. FAT32 partitions have a partition type of 0xB or 0xC.\nExtended Bios Parameter Block The first sector of a FAT32 partition contains the extended BIOS parameter block, or EBPB. The EBPB itself starts with a BIOS parameter block, or BPB. Together, these structures define the layout of the FAT file system.\nOne particularly important field in the EBPB indicates the \u0026ldquo;number of reserved sectors\u0026rdquo;. This is an offset from the start of the FAT32 partition, in sectors, where the FATs (described next) can be found. Immediately after the last FAT is the data region which holds the data for clusters. FATs, the data region, and clusters are explained next.\nClusters All data stored in a FAT file system in separated into clusters. The size of a cluster is determined by the \u0026ldquo;number of sectors per cluster\u0026rdquo; field of the EBPB. Clusters are numbered starting at 2. As seen in the diagram, the data for cluster 2 is located at the start of the data region, the data for cluster 3 is located immediately after cluster 2, and so on.\nFile Allocation Table FAT stands for \u0026ldquo;file allocation table\u0026rdquo;. As the name implies, a FAT is a table (an array) of FAT entries. In FAT32, each entry is 32-bits wide; this is where the name comes from. The size of a complete FAT is determined by the \u0026ldquo;sectors per FAT\u0026rdquo; and \u0026ldquo;bytes per sectors\u0026rdquo; fields of the EBPB. For redundancy, there can be more than one FAT in a FAT32 file system. The number of FATs is determined by a field of the same name in the EBPB.\nBesides entries 0 and 1, each entry in the FAT determines the status of a cluster. Entry 2 determines the status of cluster 2, entry 3 the status of cluster 3, and so on. Every cluster has an associated FAT entry in the FAT.\nFAT entries 0 and 1 are special:\nEntry 0: 0xFFFFFFFN, an ID. Entry 1: The end of chain marker. Aside from these two entries, all other entries correspond to a cluster whose data is in the data region. While FAT entries are physically 32-bits wide, only 28-bits are actually used; the upper 4 bits are ignored. The value is one of:\n0x?0000000: A free, unused cluster. 0x?0000001: Reserved. 0x?0000002-0x?FFFFFEF: A data cluster; value points to next cluster in chain. 0x?FFFFFF0-0x?FFFFFF6: Reserved. 0x?FFFFFF7: Bad sector in cluster or reserved cluster. 0x?FFFFFF8-0x?FFFFFFF: Last cluster in chain. Should be, but may not be, the EOC marker. Cluster Chains Clusters form chains, or linked lists of clusters. If a cluster is being used for data, its corresponding FAT entry value either points to the next cluster in the chain or is the EOC marker indicating it is the final cluster in the chain.\nAs an example, consider the diagram below which depicts a FAT with 8 entries.\nThe clusters are color coded to indicate which chain they belong to. The first two entries are the ID and EOC marker, respectively. Entry 2 indicates that cluster 2 is a data cluster; its chain is 1 cluster long. Entry 3 indicates that cluster 3 is a data cluster; the next cluster in the chain is cluster 5 followed by the final cluster in the chain, cluster 6. Similarly, clusters 7 and 5 form a chain. Cluster 8 is free and unused.\nDirectories and Entries A chain of clusters makes up the data for a file or directory. Directories are special files that map file names and associated metadata to the starting cluster for a file\u0026rsquo;s date. Specifically, a directory is an array of directory entries. Each entry indicates, among other things, the name of the entry, whether the entry is a file or directory, and its starting cluster.\nThe root directory is the only file or directory that is not linked to via a directory entry. The starting cluster for the root directory is instead recorded in the EBPB. From there, the location of all other files can be determined.\nFor historical reasons, every physical directory entry can be interpreted in two different ways. The attributes field of an entry is overloaded to indicate which way an entry should be interpreted. An entry is either:\nA regular directory entry. A long file name entry. Long file name (LFN) entries were added to FAT32 to allow for filenames greater than 11 characters in length. If an entry has a name greater than 11 characters in length, then its regular directory entry is preceded by as many LFN entries as needed to store the bytes for the entry\u0026rsquo;s name. LFN entries are not ordered physically. Instead, they contain a field that indicates their sequence. As such, you cannot rely on the physical order of LFN entries to determine how the individual components are joined together.\nWrap Up Before continuing, cross-reference your understanding with the FAT structures PDF. Then, answer the following questions:\nHow do you determine if the first sector is an MBR? (mbr-magic)\nThe first sector of a disk may not necessarily contain an MBR. How would you determine if the first sector contains a valid MBR?\nWhat is the maximum number of FAT32 clusters? (max-clusters)\nThe FAT32 design enjoins several file limitations. What is the maximum number of clusters that a FAT32 file system can contain, and what dictates this limitation? Would you expect this limitation to be the same or different in a file system named FAT16?\nWhat is the maximum size of one file? (max-file-size)\nIs there a limit to the size of a file? If so, what is the maximum size, in bytes, of a file, and what determines it?\nTake a close look at the structure of a directory entry.\nHow do you determine if an entry is an LFN? (lfn-identity)\nGiven the bytes for a directory entry, how, precisely, do you determine whether the entry is an LFN entry or a regular directory entry? Be specific about which bytes you read and what their values should be.\nHow would you lookup /a/b/c.txt? (manual-lookup)\nGiven an EBPB, describe the series of steps you would take to find the starting cluster for the file /a/b/c.txt.\nCode Structure Writing a file system of any kind is a serious undertaking, and a read-only FAT32 file system is no exception. The code that we\u0026rsquo;ve provided for you in the lib/fat32 project provides a basic structure for implementation, but many of the design decisions and the majority of the implementation are up to you.\nWe\u0026rsquo;ll describe this structure now. You should read the relevant code in the fat32/src directory as we describe the various components and how they fit together.\nFile System Traits The traits module, rooted at traits/mod.rs, provides 7 trait declarations and 1 struct declaration. Your file system implementation will largely be centered on implementing these seven traits.\nThe single struct, Dummy, is a type that provides a dummy implementation of five of the seven traits. The type is useful as a place-holder. You\u0026rsquo;ll see that we\u0026rsquo;ve used this type already in several places in the code. You may find this type useful while you work on the assignment as well.\nYou should read the code in the traits/ directory in the following order:\nRead the BlockDevice trait documentation in traits/block_device.rs. The file system will be written generic to the physical or virtual backing storage. In other words, the file system will work on any device as long as the device implements the BlockDevice trait. When we test your file system, the BlockDevice will generally be backed by a file on your local file system. When your run the file system on the Raspberry Pi, the BlockDevice will be backed by a physical SD card and EMMC controller.\nRead the File, Dir, and Entry traits in traits/fs.rs. These traits define what it (minimally) means to be a file, directory, or directory entry in the file system. You\u0026rsquo;ll notice that the associated types of the trait depend on each other. For example, the Entry trait requires its associated type File to implement the File trait.\nRead the FileSystem traits in traits/fs.rs. This trait defines what it means to be a file system and unifies the rest of the traits through its associated types. In particular, it requires a File that implements the File trait, a Dir that implements the Dir trait whose Entry associated type is the same as the associated type of file system\u0026rsquo;s Entry associated type, and finally an Entry associated type that implements Entry with the same File and Dir associated types as the file system. These constraints together ensure that there is only one concrete File, Dir, and Entry type.\nRead the Metadata and Timestamp traits in traits/metadata.rs. Every Entry must be associated with Metadata which allows access to details about a file or directory. The Timestamp trait defines the operations requires by a type that specifies a point in time.\nCached Partition CachedPartition struct in vfat/cache.rs wraps BlockDevice and Partition and translates logical sectors, as specified by the EBPB, to physical sectors, as specified by the disk. We have provided an implementation of a method that does exactly this: virtual_to_physical(). You should use this method when determining which physical sectors to read from the disk. CachedPartition also provides a caching layer, which reduces the expensive cost of direct access to a physical disk. The get() and get_mut() methods of it allow for a sector to be referenced from the cache directly.\nActual disk cache implementations in commodity operating systems manage the disk cache very smartly. They predict the disk access pattern and preload disk contents, and they write the cache back to the disk if it is not accessed recently. For simplicity, our implementation will not implement such features. It will hold the disk content in the memory indefinitely.\nUtilities The util.rs file contains two declarations and implementations of extension traits for slices (\u0026amp;[T]) and vectors (Vec\u0026lt;T\u0026gt;). These traits can be used to cast a vector or slice of one type into a vector or slice of another type as long as certain conditions hold on the two types. For instance, to cast from an \u0026amp;[u32] to an \u0026amp;[u8], you might write:\nuse util::SliceExt; let x: \u0026amp;[u32] = \u0026amp;[1, 2, 3, 4]; assert_eq!(x.len(), 4); let y: \u0026amp;[u8] = unsafe { x.cast() }; assert_eq!(y.len(), 16); MBR and EBPB The MasterBootRecord structure in mbr.rs is responsible for reading and parsing an MBR from a BlockDevice. Similarly, the BiosParameterBlock structure in vfat/ebpb.rs is responsible for reading and parsing the BPB and EBPB of a FAT32 partition.\nFilesystem The vfat/vfat.rs file contains the VFat structure, the file system itself. You\u0026rsquo;ll note that the structure contains a CachedPartition: your implementation must wrap the provided BlockDevice in a CachedPartition.\nWhat is VFAT?\nVFAT is another file system from Microsoft that is a precursor to FAT32. The name has unfortunately become synonymous with FAT32, and we continue this poor tradition here.\nThe vfat/vfat.rs file also provides VFatHandle trait, which defines a way to share mutable access to VFat instance in a thread-safe way. When implementing your file system, you\u0026rsquo;ll likely need to share mutable access to the file system itself among your file and directory structures. You\u0026rsquo;ll rely on this trait to do so. Use clone() method for replicating the handle and lock() method for entering the critical section where the code can access \u0026amp;mut VFat.\nVFat and a few other types in our file system such as File and Dir are generic over HANDLE type parameter that implements VFatHandle trait. This design allows the user of the library to inject lock implementation by implementing VFatHandle trait on their own type. Our kernel uses PiVFatHandle struct which internally uses its custom Mutex implementation, while the test code uses StdVFatHandle struct which is implemented with types in the standard library.\nVFat is generic over VFatHandle, but VFat doesn\u0026rsquo;t physically own VFatHandle. The relationship is reverse; implementors of VFatHandle will manage VFat as their field. To represent such relationship, zero-sized marker type PhantomData has been added to VFat.\nWe\u0026rsquo;ve started an implementation of the FileSystem trait for \u0026amp;'a HANDLE already. You\u0026rsquo;ll also note that the from() method of FileSystem returns a HANDLE. Your main task will be to complete the implementation of the from() method and of the FileSystem trait for \u0026amp;'a HANDLE. This will require you to implement structures that implement the remainder of the file system traits.\nWe\u0026rsquo;ve provided the following code in vfat/ to assist you with this:\nerror.rs Contains an Error enum indicating the possible FAT32 initialization errors.\nfile.rs Contains an incomplete File struct with an incomplete traits::File implementation.\ndir.rs Contains an incomplete Dir struct which you will implement trait::Dir for. Also contains incomplete definitions for raw, on-disk directory entry structures.\nentry.rs Contains an incomplete Entry struct which you will implement traits::Entry for.\nmetadata.rs Contains structures (Date, Time, Attributes) that map to raw, on-disk entry metadata as well as incomplete structures (Timestamp, Metadata) which you should implement the appropriate file system traits for.\nfat.rs Contains the FatEntry structure which wraps a value for a FAT entry and which can be used to easily read the status of the cluster corresponding to the FAT entry.\ncluster.rs Contains the Cluster structure which wraps a raw cluster number and can be used to read the logical cluster number.\nWhen you implement your file system, you should complete and use each of these structures and types. Don\u0026rsquo;t be afraid to add extra helper methods to any of these structure. Do not, however, change any of the trait definitions or existing method signatures that we have provided for you.\nRead through all of the code now, starting with vfat.rs, and ensure you understand how everything fits together.\nImplementation You\u0026rsquo;re now ready to implement a read-only FAT32 file system. You may approach the implementation in any order you see fit.\nWe have provided a somewhat rigorous set of tests to check your implementation. Our tests use files in ext/fat32-imgs. In this directory you will find several real MBR, EBPB, and FAT32 file system images as well as hash values for file system traversals as run against our reference implementation. You may find it useful to analyze and check your understanding again the raw binaries by using a hex editor such as Bless (Linux), Hex Fiend (macOS), or HxD (Windows).\nExtract Fat32 test images first!\nThe images we provided in ext/fat32-imgs are compressed. You need to un-archive them first before testing. You can use bin/extract-fat.sh to do that for you.\nYou can run the tests with cargo test. While debugging, you may wish to run the tests with cargo test -- --nocapture to prevent Cargo from capturing output to stdout or stderr. You may also find it useful to add new tests as you progress. To prevent future merge conflicts, you should add new tests in a file different from tests.rs.\nYour implementation should adhere to the following guidelines:\nUse meaningful types where you can. For instance, instead of using a u16 to represent a raw time field, use the Time struct.\nAvoid unsafe code as much as possible. Our implementation uses a total of four non-union lines of unsafe. Additionally, our implementation uses three lines of unsafe related to accessing unions. The number of unsafe code in your implementation should be comparable to this.\nAvoid duplication by using helpers methods as necessary. It\u0026rsquo;s often useful to abstract common behavior into helper methods. You should do so when it makes sense.\nEnsure your implementation is cluster size and sector size agnostic. Do not hard-code or assume any particular values for sector sizes or cluster sizes. Your implementation must function with any cluster and sector sizes that are integer multiples of 512 as recorded in the EBPB.\nDon\u0026rsquo;t double buffer unnecessarily. Ensure that you don\u0026rsquo;t read a sector into memory that is already held in the sector cache to conserve memory.\nOur recommended implementation approach is as follows:\nImplement MBR parsing in mbr.rs.\nYour implementation will likely require the use of an unsafe method, but no more than one line. Possible candidates are slice::from_raw_parts_mut() or mem::transmute() . mem::transmute() is an incredibly powerful method. You should avoid it if you can. Otherwise, you should understand its implications thoroughly before using it.\nWhen you implement Debug, use the debug_struct() method on Formatter. You can use the Debug implementation we have provided for CachedPartition as a reference.\nPacked struct in Rust\nRust is very strict about the address alignment. All Rust references should respect the alignment of the underlying type. Because of this requirement, borrowing a field of a packed struct is sometimes illegal. You can workaround this limitation by copying the value to a temporary variable and borrowing the local variable with a syntax \u0026amp;{ struct.field }.\nImplement EBPB parsing in vfat/ebpb.rs.\nAs with the MBR, your implementation will likely require the use of an unsafe method, but no more than one line.\nTest your MBR and EBPB implementation.\nMock-up MBRs and EBPBs and ensure that you parse the values successfully. Note that we have provided an implementation of BlockDevice for Cursor\u0026lt;\u0026amp;mut [u8]\u0026gt;. Remember that you can pretty-print a structure using:\nprintln!(\u0026#34;{:#?}\u0026#34;, x); Implement CachedPartition in vfat/cache.rs.\nImplement VFat::from() in vfat/vfat.rs.\nUse your MasterBootRecord, BiosParameterBlock, and CachedPartition implementations to implement VFat::from(). Test your implementation as you did your MBR and EBPB implementations.\nImplement FatEntry in vfat/fat.rs.\nImplement VFat::fat_entry, VFat::read_cluster(), and VFat::read_chain().\nThese helpers methods abstract reading from a Cluster or a chain starting from a Cluster into a buffer. You\u0026rsquo;ll likely need other helper methods, like one to calculate the disk sector from a cluster number, to implement these methods. You may wish to add helper methods to the Cluster type. You should use the VFat::fat_entry() method when implementing read_cluster() and read_chain().\nComplete the vfat/metadata.rs file.\nThe Date, Time, and Attributes types should map directly to fields in the on-disk directory entry. Refer to the FAT structures PDF when implementing them. The Timestamp and Metadata types do not have an analogous on-disk structure, but they serve as nicer abstractions over the raw, on-disk structures and will be useful when implementing the Entry, File, and Dir traits.\nImplement Dir in vfat/dir.rs and Entry in vfat/entry.rs.\nStart by adding fields that store the directory\u0026rsquo;s first Cluster and a file system handle to Dir. Then implement the trait::Dir trait for Dir. You may wish to provide dummy trait implementations for the File type in vfat/file.rs while implementing Dir. You\u0026rsquo;ll want to create a secondary struct that implements Iterator\u0026lt;Item = Entry\u0026gt; and return this struct from your entries() method. You will likely need to use at-most one line of unsafe when implementing entries(); you may find the VecExt and SliceExt trait implementations we have provided particularly useful here. Note that you will frequently need to refer to the FAT structures PDF while implementing Dir.\nParsing an Entry\nBecause the on-disk entry may be either an LFN entry or a regular entry, you must use a union to represent an on-disk entry. We have provided such a union for you: VFatDirEntry. You can read about unions in Rust in the Rust reference and about unions in general in the union type Wikipedia entry .\nYou should first interpret a directory entry as an unknown entry, use that structure to determine whether there is an entry, and if so, the true kind of entry, and finally interpret the entry as that structure. Working with union s will require using unsafe. Do so sparingly. Our implementation uses one line of unsafe three times, one to access each variant.\nWhen parsing a directory entry\u0026rsquo;s name, you must manually add a . to the non-LFN based directory entries to demarcate the file\u0026rsquo;s extension. You should only add a . if the file\u0026rsquo;s extension is non-empty.\nFinally, you\u0026rsquo;ll need to decode UTF-16 characters when parsing LFN entries. Use the decode_utf16() function to do so. You will find it useful to store UTF-16 characters in one or more Vec\u0026lt;u16\u0026gt; while parsing a long filename.\nDir::find()\nYou should implement Dir::find() after you implement the traits::Dir trait for Dir. Note that Dir::find() must be case-insensitive. Your implementation should be relatively short. You can use the eq_ignore_ascii_case() method to perform case-insensitive comparisons.\nImplement File in vfat/file.rs. Start by adding a fields that store the file\u0026rsquo;s first Cluster and a file system handle to File. Then implement the trait::File trait and any required supertraits. Modify the iterator you return from entries() as necessary.\nImplement VFat::open() in vfat/vfat.rs. Finally, implement the VFat::open() method. Use the components() method to iterate over a Path\u0026rsquo;s components. Note that the Path implementation we have provided for you in the shim library does not contain any of the methods that require a file system. These include read_dir(), is_file(), is_dir(), and others.\nUse your Dir::find() method in your implementation. You may find it useful to add a helper method to Dir.\nOnce your implementation passes all of the unit tests and works as you expect, you may once again revel; you have implemented a real file system! After sufficient reveling, proceed to the next phase.\nDid you find any undefined behavior in the skeleton code? (undefined-behavior)\nThis is an optional extra credit question. While doing the assignment, did you notice any undefined behavior or unsound API in our skeleton code except those justified with comments? What type of Rust requirements do they violate? Why they seem to behave well in practice? How can we fix them?\nPhase 3: Saddle Up In this phase, you will interface with an existing SD card controller driver for the Raspberry Pi 3 using Rust\u0026rsquo;s foreign function interface , or FFI. You can read more about Rust\u0026rsquo;s FFI in TRPL . You will also create a global handle the file system for your operating system to use. You will be working primarily in kernel/src/fs.\nSubphase A: SD Driver FFI Rust\u0026rsquo;s foreign function interface allows Rust code to interact with software written in other programming languages and vice-versa. Foreign items are declared in an extern block:\nextern { static outside_global: u32; fn outside_function(param: i16) -\u0026gt; i32; } This declares an external outside_function as well as an outside_global. The function and global be used as follows:\nunsafe { let y = outside_function(10); let global = outside_global; } Note the required use of unsafe. Rust requires the use of unsafe because it cannot ensure that the signatures you have specified are correct. The Rust compiler will blindly emit function calls and variable reads as requested. In other words, as with every other use of unsafe, the compiler assumes that what you\u0026rsquo;ve done is correct. At link-time, symbols named outside_function and outside_global must exist for the program to successfully link.\nFor a Rust function to be called from a foreign program, the function\u0026rsquo;s location (its memory address) must be exported with a known symbol. Typically, Rust mangles function symbols for versioning and namespacing reasons in an unspecified manner. As such, by default, it is not possible to know the symbol that Rust will generate for a given function and thus not possible to call that function from an external program. To prevent Rust from mangling symbols, you can use the #[no_mangle] attribute:\n#[no_mangle] fn call_me_maybe(ptr: *mut u8) { .. } A C program would then be able to call this function as follows:\nvoid call_me_maybe(unsigned char *); call_me_maybe(...); Why can\u0026rsquo;t Rust ensure that using foreign code is safe? (foreign-safety)\nExplain why Rust cannot ensure that using foreign code is safe. In particular, explain why Rust can ensure that other Rust code is safe, even when it lives outside of the current crate, but it cannot do the same for non-Rust code.\nWhy does Rust mangle symbols? (mangling)\nC does not mangle symbols. C++ and Rust, on the other hand, do. What\u0026rsquo;s different about these languages that necessitates name mangling? Provide a concrete example of what would go wrong if Rust didn\u0026rsquo;t name mangle.\nSD Driver We have provided a precompiled SD card driver library in kern/.cargo/libsd.a. We\u0026rsquo;ve also modified the build process so that the library is linked into the kernel. We\u0026rsquo;ve provided the definitions for the items exported from the library in an extern block in kern/src/fs/sd.rs.\nThe library depends on a wait_micros function which it expects to find in your kernel. The function should sleep for the number of microseconds passed in. You will need to create and export this function for your kernel to successfully link. The C signature for the function is:\n/* * Sleep for `us` microseconds. */ void wait_micros(unsigned int us); Your task is to wrap the unsafe external API in a safe, Rusty API. Implement an Sd struct that initializes the SD card controller in its new() method. Then, implement the BlockDevice trait for Sd. You will need to use unsafe to interact with the foreign items. Test your implementation by manually reading the card\u0026rsquo;s MBR in kmain. Ensure that the bytes read match what you expect. When everything works as expected, proceed to the next subphase.\nOn 64-bit ARM, an unsigned int in C is a u32 in Rust.\nIs your implementation thread-safe? (foreign-sync)\nThe precompiled SD driver we\u0026rsquo;ve provided you uses a global variable (sd_err) to keep track of error states without any kind of synchronization. As such, it has no hope of being thread-safe. How does this affect the correctness of your bindings? Recall that you must uphold Rust\u0026rsquo;s data race guarantees in any unsafe code. Assuming your kernel called sd_init correctly, is your BlockDevice implementation for Sd thread-safe as required? Why or why not?\nSubphase B: File System In this subphase you will expose and initialize a global file system for use by your kernel. You will be working primarily in kern/src/fs.rs.\nLike the memory allocator, the file system is a global resource: we want it to always be available so that we can access the data on the disk at any point. To enable this, we\u0026rsquo;ve created a global static FILE_SYSTEM: FileSystem in main.rs; it will serve as the global handle to your file system. Like the allocator, the file system begins uninitialized.\nTying the Knot You\u0026rsquo;ve now implemented both a disk driver and a file system: it\u0026rsquo;s time to tie them together. Finish the implementation of the FileSystem struct in kernel/src/fs.rs by using your FAT32 file-system and your Rusty bindings to the foreign SD card driver. You should initialize your file-system using the Sd BlockDevice in the initialize() function. Then, implement the FileSystem trait for the structure, deferring all calls to the internal VFat. Finally, ensure that you initialize the file system in kmain, just after the allocator.\nTest your implementation by printing the files at the root (\u0026quot;/\u0026quot;) of your SD card in kmain. Once everything works as your expect, proceed to the next phase.\nPhase 4: Mo\u0026rsquo;sh In this phase, you will implement the cd, ls, pwd, and cat shell commands. You will be working primarily in kern/src/shell.rs.\n\u0026#39;Finished Product\u0026#39; Working Directory You\u0026rsquo;re likely familiar with the notion of a working directory already. The current working directory (or cwd) is the directory under which relative file accesses are rooted under. For example, if the cwd is /a, then accessing hello will result in accessing the file /a/hello. If the cwd is switched to /a/b/c, accessing hello will access /a/b/c/hello, and so on. The / character can be prepended to any path to make it absolute so that it is not relative to the current working directory. As such, /hello will always refer to the file named hello in the root directory regardless of the current working directory.\nIn a shell, the current working directory can be changed to dir with the cd \u0026lt;dir\u0026gt; command. For example, running cd /hello/there will change the cwd to /hello/there. Running cd you after this will result in the cwd being /hello/there/you.\nMost operating systems provide a system call that changes a process\u0026rsquo;s working directory. Because our operating system has neither processes nor system calls yet, you\u0026rsquo;ll be keeping track of the cwd directly in the shell.\nCommands You will implement four commands that expose expose the file system through your operating system\u0026rsquo;s primary interface: the shell. These are cd, ls, pwd, and cat. For the purposes of this assignment, they are specified as follows:\npwd - print the working directory Prints the full path of the current working directory.\ncd \u0026lt;directory\u0026gt; - change (working) directory Changes the current working directory to directory. The directory argument is required.\nls [-a] [directory] - list the files in a directory Lists the entries of a directory. Both -a and directory are optional arguments. If -a is passed in, hidden files are displayed. Otherwise, hidden files are not displayed. If directory is not passed in, the entries in the current working directory are displayed. Otherwise, the entries in directory are displayed. The arguments may be used together, but -a must be provided before directory.\nInvalid arguments results in an error. It is also an error if ``directory`` does not correspond to a valid, existing directory. cat \u0026lt;path..\u0026gt; - concatenate files Prints the contents of the files at the provided paths, one after the other. At least one path argument is required.\nIt is an error if a ``path`` does not point to a valid, existing file. It is an error if an otherwise valid file contains invalid UTF-8. All non-absolute paths must be must be treated as relative to the current working directory if they are not absolute. For an example of these commands in action, see the GIF above. When you implement these commands yourself, you are free to display directory entries and errors in any way that you\u0026rsquo;d like as long as all of the information is present.\nImplementation Extend your shell in kern/src/shell.rs with these four commands. Use a mutable PathBuf to keep track of the current working directory; this PathBuf should be modified by the cd command. You will find it useful to create functions with a common signature for each of your commands. For an extra level of type-safety, you can abstract the concept of an executable command into a trait that is implemented for each of your commands.\nOnce you have implemented, tested, and verified your four commands against the specifications above, you\u0026rsquo;re ready to submit your assignment. Congratulations!\nEnsure you\u0026rsquo;re using your bin allocator!\nYour file system is likely very memory intensive. To avoid running out of memory, ensure you\u0026rsquo;re using your bin allocator.\nUse the existing methods of PathBuf and Path to your advantage.\nYou\u0026rsquo;ll need to handle .. and . specially in cd.\nSubmission Once you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Congratulations!\nYou can call make check in tut/3-fs directory to check if you\u0026rsquo;ve answered every question and cargo test in lib/fat32 directory to run the unit tests for your FAT32 implementation. Note that there are no unit tests for some tasks in os. You\u0026rsquo;re responsible for ensuring that they work as expected.\nOnce you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Ensure you\u0026rsquo;ve committed your changes. Any uncommitted changes will not be visible to us, thus unconsidered for grading.\nWhen you\u0026rsquo;re ready, push a commit to your GitHub repository with a tag named lab3-done.\n# submit lab3 $ git tag lab3-done $ git push --tags ", 
        "url": "\/\/localhost:1313\/post\/lab3\/"
    },
    
    "\/\/localhost:1313\/post\/lab2\/": {
        "title": "Lab 2: Shell and Bootloader",
        "tags": ["Lab",],
        "content": " Handed out: Tuesday, January 28, 2020 Due: Monday, February 10, 2020 Introduction In this assignment, you will write useful utilities, libraries, and a simple shell for your Raspberry Pi. You\u0026rsquo;ll also write generic drivers for GPIO, UART , and the built-in timer. Finally, you\u0026rsquo;ll write a \u0026ldquo;bootloader\u0026rdquo; using your new drivers that loads program binaries over UART using the XMODEM protocol and executes them.\nGetting the Skeleton Code To get the skeleton code for lab2, fetch the updates from our git repository to your development machine.\n$ git fetch skeleton $ git merge skeleton/lab2 This is the directory structure of our repository. The directories you will be working on this assignment are marked with *.\n. ├── bin : common binaries/utilities ├── doc : reference documents ├── ext : external files (e.g., resources for testing) ├── tut : tutorial/practices │ ├── 0-rustlings │ ├── 1-blinky │ └── 2-shell : questions for lab2 * ├── boot : bootloader * ├── kern : the main os kernel * └── lib : required libraries ├── pi * ├── shim ├── stack-vec * ├── ttywrite * ├── volatile * └── xmodem * Please resolve conflict if you have and proceed to the next phase.\nWe recommend the following directory structure for your assignments. Confirm that your directories are properly laid out by running make inside the kern directory now. If all is well, the command will return successfully. If everything is good, feel free to explore the contents of the repository.\nThis and future assignments include writing questions that you must respond to. Here\u0026rsquo;s an example of such question:\nHow do you set other GPIO pins? (1-blinky)\nIn assignment 1-blinky, you enabled GPIO pin 16 as an output and then repeatedly set and cleared it by writing to registers GPFSEL1, GPSET0, and GPCLR0. Which three registers would you write to to do the same for GPIO pin 27? Which physical pin on the Raspberry Pi maps to GPIO pin 27?\nThe word in a parenthesis of the question indicates the name of a file located inside questions/ directory relative to the lab name in which the question is being asked. For instance, every questions in this Lab2: Shell and Bootloader should be answered in tut/2-shell/questions/ subdirectory. Note that we have pre-generated empty files for every question.\nPractice responding to questions now by answering the 1-blinky question above.\nPhase 1: Oxidation In this phase, you will write two libraries, one command-line utility, and review one library. You will be working in the stack-vec, volatile, ttywrite, and xmodem skeleton subdirectories located in lib directory.\nAll projects are being managed with Cargo. You will find the following cargo commands useful:\ncargo build - build an application or library cargo test - test an application or library cargo run - run an application cargo run -- $flags - run an application and pass arbitrary flags to it For more information on using Cargo and how Cargo works, see the Cargo Book .\nSubphase A: StackVec One important facility that operating systems provide is memory allocation. When a C, Rust, Java, Python, or just about any application calls malloc() and malloc() has run out of memory from the operating system, a system call is eventually made to request additional memory. The operating system determines if there is memory available, and if so, fulfills the request for memory.\nMemory allocation is a complicated story.\nIn practice, modern operating systems like Linux have a complicated relationship with memory allocation. For instance, as an optimization, most requests for memory allocation are only \u0026ldquo;virtually\u0026rdquo; handled: no physical memory is actually allocated until the application tries to use the newly allocated memory. Nonetheless, most operating systems aim to provide the illusion that they are allocating memory in the simplistic manner we\u0026rsquo;ve described. Operating systems are master liars.\nHeap-allocated structures like Vec, String, and Box internally call malloc() to allocate memory as necessary. This means that these structures require operating system support to function. In particular, they require the operating system to support memory allocation. We haven\u0026rsquo;t yet started writing our operating system, so clearly there\u0026rsquo;s no memory allocation support for our tiny bare-metal programs to make use of. As such, we can\u0026rsquo;t use heap-allocated structures like Vec until our operating system is further along.\nThis is a real shame because Vec is a nice abstraction! It allows us to think about pushing and poping elements without having to keep track of memory ourselves. How we can get the benefits of the Vec abstraction without supporting memory allocation?\nOne common technique is to pre-allocate memory and then hand that memory to a structure to abstract away. Some ways to pre-allocate memory include using static declarations to set apart memory in the static section of a binary or through stack allocations from local variable declarations. In any case, the allocations is of a fixed, predetermined size.\nIn this subphase, you will implement the StackVec structure, a structure that exposes a Vec-like API when given pre-allocated memory. You will use the StackVec type later in phase 2 when implementing a shell for your Raspberry Pi. You will work in the lib/stack-vec skeleton subdirectory. The subdirectory contains the following files:\nCargo.toml - configuration file for Cargo src/lib.rs - where you will write your code src/tests.rs - tests that will run when cargo test is called The StackVec Interface A StackVec\u0026lt;T\u0026gt; is created by calling StackVec::new(), passing in a mutable slice to values of any type T. The StackVec\u0026lt;T\u0026gt; type implements many of the methods that Vec implements and is used in much the same way. Here\u0026rsquo;s an example of a StackVec\u0026lt;u8\u0026gt; being used:\nlet mut storage = [0u8; 10]; let mut vec = StackVec::new(\u0026amp;mut storage); for i in 0..10 { vec.push(i * i).expect(\u0026#34;can push 10 times\u0026#34;); } for (i, v) in vec.iter().enumerate() { assert_eq!(*v, (i * i) as u8); } let last_element = vec.pop().expect(\u0026#34;has elements\u0026#34;); assert_eq!(last_element, 9 * 9); We\u0026rsquo;ve declared the StackVec structure for you already:\npub struct StackVec\u0026lt;\u0026#39;a, T: \u0026#39;a\u0026gt; { storage: \u0026amp;\u0026#39;a mut [T], len: usize } Understanding StackVec The following questions test your understanding about the StackVec interface:\nWhy does push return a Result? (push-fails)\nThe push method from Vec in the standard library has no return value, but the push method from our StackVec does: it returns a Result indicating that it can fail. Why can StackVec::push() fail where Vec::push() does not?\nWhy is the 'a bound on T required? (lifetime)\nstruct StackVec\u0026lt;\u0026#39;a, T\u0026gt; { buffer: \u0026amp;\u0026#39;a mut [T], len: usize } Rust automatically enforces the bound T: 'a and will complain if type T lives shorter than the lifetime 'a. For instance, if T is \u0026amp;'b str and 'b is strictly shorter than 'a, Rust won\u0026rsquo;t allow you to create the instance of StackVec\u0026lt;'a, \u0026amp;'b str\u0026gt;.\nWhy is the bound required? What could go wrong if the bound wasn\u0026rsquo;t enforced by Rust?\nWhy does StackVec require T: Clone to pop()? (clone-for-pop)\nThe pop method from Vec\u0026lt;T\u0026gt; in the standard library is implemented for all T, but the pop method from our StackVec is only implemented when T implements the Clone trait. Why might that be? What goes wrong when the bound is removed?\nImplementing StackVec Implement all of the unimplemented!() StackVec methods in stack-vec/src/lib.rs. Each method is documented in the source code. We have also provided tests in src/tests.rs that help ensure that your implementations are correct. You can run these tests with cargo test. You\u0026rsquo;ll also need to implement the Deref, DerefMut, and IntoIterator traits for StackVec as well as the IntoIterator trait for \u0026amp;StackVec for all of the cargo test tests to pass. Once you feel confident that you implementation is correct and have answered this subphase\u0026rsquo;s questions, proceed to the next subphase.\nWhich tests make use of the Deref implementations? (deref-in-tests)\nRead through the tests we have provided in src/tests.rs. Which tests would fail to compile if the Deref implementation did not exist? What about the DerefMut implementation? Why?\nOur unit tests are incomplete!\nOur unit tests provide a baseline truth, but they are not complete! We will run additional tests when we grade your assignment. You may wish to find the gaps in our tests and add additional tests of your own to fill them.\nSubphase B: volatile In this subphase, you will learn about volatile memory accesses, read the source code in the volatile skeleton subdirectory, and answer questions related to the source code. You won\u0026rsquo;t be writing any code in this subphase.\nLike operating systems, compilers are masters at making things appear as if they\u0026rsquo;re doing what you think they\u0026rsquo;re doing when in reality, they\u0026rsquo;re really doing something entirely different for the sake of optimization. One such optimization is dead-access elimination: compilers remove memory accesses (reads and writes) when they can prove doing so has no observable effect on the program\u0026rsquo;s execution. For instance, consider the following program:\nfn f() { let mut x = 0; let y = \u0026amp;mut x; *y = 10; } The compiler can completely eliminate the write to *y by reasoning that *y is never read after it\u0026rsquo;s written. The compiler concludes that as a result, the write cannot possibly effect the program, and eliminates it in the compiled binary. For the same reason, it can then proceed to eliminate the declaration for y, the declaration for x, and calls to f() entirely.\nThese kinds of optimizations are almost exclusively beneficial: they speed up our programs without affecting their outcome. But sometimes these optimizations can have unintended consequences. Say, for example, that y was pointing to a write-only memory-mapped register. Then, writes to *y will have observable effects without having to read *y thereafter. If the compiler is not aware of this, it will optimize away these writes, and our program will not function correctly.\nHow can we force the compiler to keep around reads and writes that appear to have no effects at the source code level? This is where volatile memory accesses come in: the compiler promises not to optimize away volatile memory accesses. So if we want to ensure a read or write occurs at runtime, we must perform a volatile memory access.\nRusty volatile In Rust, we use the read_volatile and write_volatile methods to perform volatile reads and writes to a raw pointer.\nWhat\u0026rsquo;s a raw pointer?\nBy now you\u0026rsquo;re familiar with references (\u0026amp;T and \u0026amp;mut T). A raw pointer in Rust (*const T and *mut T) is a \u0026ldquo;reference\u0026rdquo; that isn\u0026rsquo;t tracked with lifetimes by Rust\u0026rsquo;s borrow checker. Because of this, read or writes to these pointers may be invalid, just as in C. Rust considers them unsafe, and code that reads or writes them must be annotated with unsafe to indicate this. You can read more about raw pointers in the rustdocs .\nCalling read_volatile and write_volatile every time we want to perform a volatile read or write is error prone and frustrating. Thankfully Rust provides us the tools to make this easier and safer. Ideally we can simply declare a pointer as volatile (as in C) and ensure that every read or write thereafter is volatile. Even better, we should be able declare a pointer as read-only, write-only (unlike in C), or read/write and ensure only the appropriate memory accesses can be made.\nIntroducing Volatile, ReadVolatile, WriteVolatile, and UniqueVolatile The volatile crate in the volatile/ skeleton subdirectory implements these four types that allow us to do just this. Read the documentation for these types now by running cargo doc --open inside of the volatile/ directory.\nWhy does Unique\u0026lt;Volatile\u0026gt; exist? (unique-volatile)\nBoth Volatile and Unique\u0026lt;Volatile\u0026gt; allow read/write volatile accesses to an underlying pointer. According to the documentation, what is the difference between these two types?\nNow open the source code in src/lib.rs, src/traits.rs, and src/macros.rs. Read through the source code to the best of your abilities. When you\u0026rsquo;re ready, answer the following questions. Once you have answered these questions, you\u0026rsquo;re ready to move on to the next subphase.\nWhat\u0026rsquo;s with #[repr(C)]?\nThe #[repr(C)] annotation forces Rust to lay out the structure\u0026rsquo;s fields in the same way that C would. In general, Rust optimizes the order and padding between fields of structures in an unspecified way. When we cast a raw address to a pointer to a structure, we typically have a very specific memory layout in mind. The #[repr(C)] annotation lets us confide that Rust will arrange the structure as we intend it to, not as it wishes.\nHow are read-only and write-only accesses enforced? (enforcing)\nThe ReadVolatile and WriteVolatile types make it impossible to write and read, respectively, the underlying pointer. How do they accomplish this?\nWhat do the macros do? (macros)\nWhat do the readable!, writeable!, and readable_writeable! macros do?\nSubphase C: xmodem In this subphase, you will implement the XMODEM file transfer protocol in the xmodem library in the xmodem/ skeleton subdirectory. You will primarily be working in xmodem/src/lib.rs.\nXMODEM is a simple file transfer protocol originally developed in 1977. It features packet checksums, cancellation, and automatic retries. It is widely implemented and used for transfers through serial interfaces. Its best feature, however, is its simplicity. For more about its history, see the XMODEM Wikipedia article .\nWe will use the XMODEM protocol to transfer files to the Raspberry Pi. While we could use existing implementations of the XMODEM protocol to send data to the Pi, we will still need to write our own receiver. So, while we\u0026rsquo;re at it, we\u0026rsquo;ll be implementing XMODEM transmission as well.\nThe Protocol The XMODEM protocol is described in detail in the Understanding The X-Modem File Transfer Protocol txt file. We describe it again here, for posterity.\nDo not base your implementation off of Wikipedia\u0026rsquo;s explanation!\nWhile Wikipedia\u0026rsquo;s explanation is helpful at a high level, many of the details presented there are different from the protocol we\u0026rsquo;ll be implementing here. As such, do not use the article as a reference for this subphase.\nXMODEM is a binary protocol: bytes are sent and received in the raw. It is also \u0026ldquo;half duplex\u0026rdquo;: at any point in time, either the sender or receiver is sending data, but never both. Finally it is packet-based: data is separated into 128 byte chunks known as packets. The protocol dictates which bytes are sent when, what they mean, and how they\u0026rsquo;re interpreted.\nFirst, we define a few constants:\nconst SOH: u8 = 0x01; const EOT: u8 = 0x04; const ACK: u8 = 0x06; const NAK: u8 = 0x15; const CAN: u8 = 0x18; To start the file transfer, the receiver sends a NAK byte while the sender waits for a NAK byte. Once the sender has received the NAK byte, packet transmission begins. The receiver only sends a NAK byte to begin the file transfer, not once for every packet.\nOnce file transfer has begun, each packet\u0026rsquo;s transmission and reception is identical. Packets are numbered in sequential order starting at 1 and wrap around to 0 after 255.\nXMODEM protocol diagram To send a packet, the sender:\nSends an SOH byte.\nSends the packet number.\nSends the 1s complement of the packet number (255 - $packet_number).\nSends the packet itself.\nSends the packet checksum.\nThe checksum is the sum of all of the bytes in the packet mod 256. Reads a byte from the receiver.\nIf the byte is NAK, transmission for the same packet is retried up to 10 times. If the byte is ACK, the next packet is sent. The receive a packet, the receiver performs the inverse:\nWaits for an SOH or EOT byte from the sender.\nIf a different byte is received, the receiver cancels the transfer. If an EOT byte is received, the receiver performs end of transmission. Reads the next byte and compares it to the current packet number.\nIf the wrong packet number is received, the receiver cancels the transfer. Reads the next byte and compares it to the 1s complement of the packet number.\nIf the wrong number is received, the receiver cancels the transfer. Reads a packet (128 bytes) from the sender.\nComputes the checksum for the packet.\nThe checksum is the sum of all of the bytes in the packet mod 256. Reads the next byte and compares it to the computed checksum.\nIf the checksum differs, sends a NAK byte and retries reception for the same packet. If the checksum is the same, sends an ACK byte and receives the next packet. To cancel a transfer, a CAN byte is sent by either the receiver or sender. When either side receives a CAN byte, it errors out, aborting the connection.\nTo end the transmission, the sender:\nSends an EOT byte. Waits for a NAK byte. If a different byte is received, the sender errors out. Sends a second EOT byte. Waits for an ACK byte. If a different byte is received, the sender errors out. To end the transmission, the receiver performs the following after receiving the first EOT:\nSends a NAK byte. Waits for a second EOT byte. If a different byte is received, the receiver cancels the transfer. Sends an ACK byte. Implementing XMODEM We have provided an unfinished implementation of the XMODEM protocol in the xmodem skeleton subdirectory. Your task is to complete the implementation by writing the expect_byte, expect_byte_or_cancel, read_packet, and write_packet methods in src/lib.rs. Your implementations should make use of the internal state of the Xmodem type: packet and started. We recommend reading over the existing code before starting.\nYou should begin by implementing the expect_byte and expect_byte_or_cancel methods. You should then make use of all four of the helper methods (including read_byte and write_byte) to implement read_packet and write_packet. To see how these methods are used, read the transmit and receive implementations which transmit or receive a complete data stream using XMODEM via these methods. Be mindful of the specifications in the doc-comments. You can test your implementation using cargo test. Once you are confident that your implementation is correct, proceed to the next subphase.\nDo not use any additional items from std.\nYour implementation should only use items from shim::io. It should not use other items from std or any other libraries.\nOur reference implementations for {read,write}_packet are roughly 43 lines of code each.\nThe io::Read and io::Write rustdocs will be useful.\nUse the ? operator generously.\nThe test source code can be a helpful guide.\nYou can use ioerr! macro to make and return a new io::Error easily. Please refer shim/src/macros.rs to find more macros which can be useful.\nSubphase D: ttywrite In this subphase, you will write a command line utility, ttywrite, that will allow you to send data to your Raspberry Pi in the raw or via the XMODEM protocol. You will use your xmodem library from the previous subphase in your implementation. You will write your code in ttywrite/src/main.rs. To test your ttywrite implementation, use the provided test.sh script.\nWhat is a serial device?\nA serial device is any device that accepts communication one bit at a time. This is known as serial communication. In contrast, in parallel communication multiple bits are being transferred at any point in time in parallel. We will be communicating with our Raspberry Pi via its UART device, a serial communication device.\nWhat is a TTY?\nA TTY is a \u0026ldquo;teletypewriter\u0026rdquo;. It is a vestigial term that was adopted in computing to describe computer terminals. The term later become more general, coming to describe any device intended to be communicated with over serial. For this reason, your computer calls the device mapping to your Raspberry Pi a TTY.\nCommand-Line Interface The skeleton code we have provided for ttywrite already parses and validates command-line arguments. To do so, it uses the structopt crate from crates.io which itself uses clap . You\u0026rsquo;ll notice that we list it as a dependency in the Cargo.toml file. structopt works through code generation. We simply annotate a structure and its fields with a declaration of our command-line arguments and structopt generates the code to actually parse the command-line flags.\nTo see the interface that structopt generates, call the application with --help. Remember that you can pass arbitrary flags when using cargo run: cargo run -- --help. Take a look at the interface now. Then, take a look at the Opt structure in main.rs and compare the interface with its definition.\nWhat happens when a flag\u0026rsquo;s input is invalid? (invalid)\nTry passing in some invalid values for flags. For instance, it should not be possible to set -f to idk. How does structopt know to reject invalid values?\nYou\u0026rsquo;ll notice that there are plenty of options. All of these correspond to settings available on a serial device. For now it\u0026rsquo;s not important to know exactly what these settings do.\nTalking to a Serial Device In main, you\u0026rsquo;ll see a call to serial::open . This is calling the open function from the serial crate, also on crates.io . This open function returns a TTYPort which allows you to read and write to the serial device (via its io::Read and io::Write trait implementations) as well as read and set settings on a serial device (via its SerialDevice trait implementation).\nWriting the Code Implement the ttywrite utility. Your implementation should set all of the appropriate settings passed in via the command-line stored in the opt variable in main. It should read from stdin if no input file is passed in or from the input file if one is passed in. It should write the input data to the passed in serial device. If the -r flag is set, it should send the data as it is. Otherwise, you should use your xmodem implementation from the previous subphase to send the data using the XMODEM protocol. You should print the number of bytes sent on a successful transmission.\nTo transmit using the XMODEM protocol, your code should use either the Xmodem::transmit or Xmodem::transmit_with_progress methods from the xmodem library. We recommend using transmit_with_progress so that your utility indicates progress throughput the transmission. In its simplest form, this might look as follows:\nfn progress_fn(progress: Progress) { println!(\u0026#34;Progress: {:?}\u0026#34;, progress); } Xmodem::transmit_with_progress(data, to, progress_fn) You can test the baseline correctness of your implementation using the test.sh script in the ttywrite directory. When your implementation is at least somewhat correct, you will see the following when the script is run:\nOpening PTYs... Running test 1/10. wrote 333 bytes to input ... Running test 10/10. wrote 232 bytes to input SUCCESS You can retrieve a handle to stdin with io::stdin() .\nYou may find the io::copy() function useful.\nThe main() function in our reference implementation is roughly 35 lines of code.\nKeep the TTYPort documentation open while writing your code.\nWhy does the test.sh script always set -r? (bad-tests)\nThe test.sh script that we have provided always uses the -r flag; it doesn\u0026rsquo;t test that your utility uses the XMODEM protocol when it is asked to. Why might that be? What does the XMODEM protocol expect that sending data in the raw doesn\u0026rsquo;t that makes testing its functionality difficult?\nInstalling ttywrite utility After finish writing the ttywrite utility, install the tool with cargo install --path . command. This command will be used later to communicate with the bootloader.\nPhase 2: Not a Seashell In this phase, you will be implementing drivers for the built-in timer, GPIO, and UART devices. You\u0026rsquo;ll use then these drivers to implement a simple shell. In the next phase, you\u0026rsquo;ll use the same drivers to implement a bootloader.\nWhat\u0026rsquo;s a driver?\nThe term driver, or device driver, describes software that directly interacts with and controls a hardware device. Drivers expose a higher-level interface to the hardware they control. Operating systems may interact with device drivers to expose an even higher-level interface. For instance, the Linux kernel exposes ALSA (Advanced Linux Sound Architecture), an audio API, which interacts with device drivers that in-turn interact directly with sound cards.\nSubphase A: Getting Started Project Structure Let\u0026rsquo;s recall the repository structure we saw at the beginning of this lab.\n. ├── ... ├── boot : bootloader * ├── kern : the main os kernel * └── lib : required libraries ├── pi * ├── shim ├── stack-vec * ├── ttywrite * ├── volatile * └── xmodem * All the libraries used by boot and kernel are located under the lib directory.\nshim library selectively depends on either std or no_std library. With #[cfg(feature = \u0026quot;no_std\u0026quot;)] specified, shim makes use of core_io and the custom no_std module which has minimum library we need such as ffi, path and sync. Otherwise, mostly in the test code, shim just uses std library.\npi subdirectory contains all of your driver code. The pi library makes use of the volatile library. It also depends on the shim library.\nboot and kernel make use of the pi library to communicate with hardware. They also depend on shim. In addition to that, boot also depends on the xmodem library, and kernel depends on the stack-vec library. The volatile library has no dependencies. The diagram below illustrates these relationships:\nKernel The kern directory contains the code for the operating system kernel: the core of your operating system. Calling make inside this directory builds the kernel. The build output is stored in the build/ directory. To run the kernel, copy the build/kernel.bin file to the root of the MicroSD card as kernel8.img. You may wish to use a script to copy the kernel image to the sdcard with make install command. Please refer the Tools page to find details about our Makefile.\nAt present, the kernel does absolutely nothing. By the end of this phase, the kernel will start up a shell which you can communicate with.\nAs we saw above, the kernel crate depends on the pi library. As a result, you can use all of the types and items from the pi library in the kernel.\nDocumentation While writing your device drivers, you\u0026rsquo;ll want to keep the BCM2837 ARM Peripherals Manual open.\nSubphase B: System Timer In this subphase, you will write a device driver for the ARM system timer. You will primarily be working in lib/pi/src/timer.rs and kern/src/main.rs. The ARM system timer is documented on page 172 (section 12) of the BCM2837 ARM Peripherals Manual .\nStart by looking at the existing code in lib/pi/src/timer.rs. In particular, note the relationship between the following sections:\nconst TIMER_REG_BASE: usize = IO_BASE + 0x3000; #[repr(C)] struct Registers { CS: Volatile\u0026lt;u32\u0026gt;, CLO: ReadVolatile\u0026lt;u32\u0026gt;, CHI: ReadVolatile\u0026lt;u32\u0026gt;, COMPARE: [Volatile\u0026lt;u32\u0026gt;; 4] } pub struct Timer { registers: \u0026amp;\u0026#39;static mut Registers } impl Timer { pub fn new() -\u0026gt; Timer { Timer { registers: unsafe { \u0026amp;mut *(TIMER_REG_BASE as *mut Registers) }, } } } The one line of unsafe in this program is very important: it casts the TIMER_REG_BASE address to a *mut Registers and then casts that to an \u0026amp;'static mut Registers. We are telling Rust that we have a static reference to a Registers structure at address TIMER_REG_BASE.\nWhat is at the TIMER_REG_BASE address? On page 172 of the BCM2837 ARM Peripherals Manual , you\u0026rsquo;ll find that 0x3000 is the peripheral offset for the ARM system timer. Thus, TIMER_REG_BASE is the address at which the ARM system timer registers start! After this one line of unsafe, we can use the registers field to access the timer\u0026rsquo;s registers safely. We can read the CLO register with self.registers.CLO.read() and write the CS register with self.registers.CS.write(), then combine them together to represent the number of elapsed microseconds.\nWhy can\u0026rsquo;t you write to CLO or CHI? (restricted-reads)\nThe BCM2837 documentation states that the CLO and CHI registers are read-only. Our code enforces this property. How? What prevents us from writing to CLO or CHI?\nWhat exactly is unsafe?\nIn short, unsafe is a marker for the Rust compiler that you\u0026rsquo;re taking control of memory safety: the compiler won\u0026rsquo;t protect you from memory issues. As a result, in unsafe sections, Rust lets you do anything you can do in C. In particular, you can cast between types with more freedom, dereference raw pointers, and fabricate lifetimes.\nBut note that unsafe is very unsafe. You must ensure that everything you do in an unsafe section is, in fact safe. This is more difficult than it sounds, especially when Rust\u0026rsquo;s idea of safe is much stricter than in other languages. As such, you should try not to use unsafe at all. For operating systems, unfortunately, we must use unsafe so that we can directly speak to hardware, but we\u0026rsquo;ll typically limit our use to one line per driver.\nIf you want to learn more about unsafe, read Chapter 1 of the Nomicon .\nImplement the Driver Implement the Timer::read(), current_time(), and spin_sleep() in lib/pi/src/timer.rs. The signatures on these items indicate their expected functionality. You\u0026rsquo;ll need to read the timer\u0026rsquo;s documentation in the BCM manual to implement Timer::read(). In particular, you should understand which registers to read to obtain the timer\u0026rsquo;s current u64 value. You can build the pi library with cargo build. You can also use cargo check to type-check the library without actually compiling it.\nYou\u0026rsquo;ll find the core::time::Duration page useful.\nTesting Your Driver Let\u0026rsquo;s test your driver by ensuring that spin_sleep() is accurate. We\u0026rsquo;ll write the code to do this in kern/src/main.rs.\nCopy your LED blinky code from phase 4 of lab 1 into main.rs. Instead of the for loop based sleep function, use your newly written spin_sleep() function with Duration to pause between blinks. Compile the kernel, load it onto the MicroSD card as kernel8.img, and then run it on the Raspberry Pi. Ensure that the LED blinks at the frequency that you intended it to. Try other pause times and ensure that they all work as expected. Until you write the bootloader in phase 3, you\u0026rsquo;ll need to keep swapping the MicroSD card between the Pi and your computer to try out different binaries.\nIf your timer driver is working as expected, proceed to the next subphase.\nSubphase C: GPIO In this subphase, you will write a generic, pin-independent device driver for GPIO. You will primarily be working in lib/pi/src/gpio.rs and kern/src/main.rs. The GPIO subsystem is documented on page 89 (section 6) of the BCM2837 ARM Peripherals Manual .\nState Machines All hardware devices are state machines : they begin at a predetermined state and transition to different states based on explicit or implicit inputs. The device exposes different functionality depending on which state it is in. In other words, only some transitions are valid in some states. Importantly, this implies that some transitions are invalid when the device is in a given state.\nMost programming languages make it impossible to faithfully encode the semantics of a state machine in hardware, but not Rust! Rust lets us perfectly encode state machine semantics, and we\u0026rsquo;ll take advantage of this to implement a safer-than-safe device driver for the GPIO subsystem. Our driver will ensure that a GPIO pin is never misused, and it will do so at compile-time.\nBelow is the state diagram for a subset of the GPIO state machine for a single pin:\nGPIO State Diagram Our goal is to encode this state machine in Rust. Let\u0026rsquo;s start by interpreting the diagram:\nThe GPIO starts in the START state.\nFrom the START state it can transition to one of three states:\nALT - no transitions are possible from this state OUTPUT - two \u0026ldquo;self\u0026rdquo; transitions are possible: SET and CLEAR INPUT - one \u0026ldquo;self\u0026rdquo; transition is possible: LEVEL Which transitions did you follow in your lab 1 blinky? (blinky-states)\nWhen you implementing the blinky code in phase 4 of lab 1, you implicitly implemented a subset of this state machine. Which transitions did your code implement?\nWe\u0026rsquo;ll use Rust\u0026rsquo;s type system to ensure that a pin can only be SET and CLEARed if it has been transitioned to the OUTPUT state and the LEVEL read if it is in the INPUT state. Take a look at the declaration for the GPIO structure in lib/pi/src/gpio.rs:\npub struct Gpio\u0026lt;State\u0026gt; { pin: u8, registers: \u0026amp;\u0026#39;static mut Registers, _state: PhantomData\u0026lt;State\u0026gt; } The structure has one generic argument, State. Except for PhantomData, nothing actually uses this argument. This is what PhantomData is there for: to convince Rust that the structure somehow uses the generic even though it otherwise wouldn\u0026rsquo;t. We\u0026rsquo;re going to use the State generic to encode which state the Gpio device is in. Unlike other generics, we must control this parameter and ensure that a client can never fabricate it.\nThe state! macro generates types that represent the states a Gpio can be in:\nstates! { Uninitialized, Input, Output, Alt } // Each parameter expands to an `enum` that looks like: enum Input { } This is also weird; why would we create an enum with no variants? enum\u0026rsquo;s with no variants have a nice property: they can never be instantiated. In this way, these types act purely as markers. No one can ever pass us a value of type Input because such a value can never be constructed. They exist purely at the type-level.\nWe can then implement methods corresponding to valid transitions given that a Gpio is in a certain state:\nimpl Gpio\u0026lt;Output\u0026gt; { /// Sets (turns on) the pin. pub fn set(\u0026amp;mut self) { ... } /// Clears (turns off) the pin. pub fn clear(\u0026amp;mut self) { ... } } impl Gpio\u0026lt;Input\u0026gt; { /// Reads the pin\u0026#39;s value. pub fn level(\u0026amp;mut self) -\u0026gt; bool { ... } } This ensures that a Gpio can only be set and cleared when it is a Gpio\u0026lt;Output\u0026gt; and its level read when it is a Gpio\u0026lt;Input\u0026gt;. Perfect! But how do we actually transition between states? Hello, Gpio::transition()!\nimpl\u0026lt;T\u0026gt; Gpio\u0026lt;T\u0026gt; { fn transition\u0026lt;S\u0026gt;(self) -\u0026gt; Gpio\u0026lt;S\u0026gt; { Gpio { pin: self.pin, registers: self.registers, _state: PhantomData } } } This method lets us transition a Gpio from any state to any other state. Given a Gpio in state T, this method returns a Gpio in state S. Note that it works for all S and T. We must be very careful when calling this method. When called, we are encoding the specification of a transition in the state diagram. If we get the specification or encoding wrong, our driver is wrong.\nTo use the transition() method, we need to tell Rust which type we want as an output S in Gpio\u0026lt;S\u0026gt;. We do this by giving Rust enough information so that it can infer the S type. For instance, consider the implementation of the into_output method:\npub fn into_output(self) -\u0026gt; Gpio\u0026lt;Output\u0026gt; { self.into_alt(Function::Output).transition() } This method requires its return type to be Gpio\u0026lt;Output\u0026gt;. When the Rust type system inspects the call to transition(), it will search for a Gpio::transition() method that returns a Gpio\u0026lt;Output\u0026gt; to satisfy the requirement. Since our transition method returns Gpio\u0026lt;S\u0026gt; for any S, Rust will replace S with Output and use that method. The result is that we\u0026rsquo;ve transformed our Gpio\u0026lt;Alt\u0026gt; (from the into_alt() call) into a Gpio\u0026lt;Output\u0026gt;.\nWhat would go wrong if a client fabricates states? (fake-states)\nConsider what would happen if we let the user choose the initial state for a Gpio structure. What could go wrong?\nWhy is this only possible with Rust?\nNotice that the into_ transition methods take a Gpio by move. This means that once a Gpio is transitioned into a another state, it can never be accessed in the previous state. Rust\u0026rsquo;s move semantics make this possible. As long as a type doesn\u0026rsquo;t implement Clone, Copy, or some other means of duplication, there is no coming back from a transition. No other language, not even C++, affords us this guarantee at compile-time.\nImplement the Driver Implement the unimplemented!() methods in lib/pi/src/gpio.rs. The signatures on these items indicate their expected functionality. You\u0026rsquo;ll need to read the GPIO documentation (page 89, section 6 of the BCM2837 ARM Peripherals Manual ) to implement your driver. Remember that you can use cargo check to type-check the library without actually compiling it.\nTesting Your Driver We\u0026rsquo;ll again write code in kern/src/main.rs to ensure that our driver works as expected.\nInstead of reading/writing to raw memory addresses, use your new GPIO driver to set and clear GPIO pin 16. Your code should get a lot cleaner. Compile the kernel, load it onto the MicroSD card as kernel8.img, run it on the Raspberry Pi, and ensure your LED blinks as before.\nNow, connect more LEDs to your Raspberry Pi. Use GPIO pins 5, 6, 13, 19, and 26. Refer to the pin numbering diagram from assignment 0 to determine their physical location. Have your kernel blink all of the LEDs in a pattern of your choice.\nWhich pattern did you choose? (led-pattern)\nWhat pattern did you have your LEDs blink in? If you haven\u0026rsquo;t yet decided, one fun idea is to have them imitate a \u0026ldquo;loading spinner\u0026rdquo; by arranging the LEDs in a circle and turning them on/off in a sequential, circular pattern. Once your GPIO driver is working as expected, proceed to the next subphase.\nSubphase D: UART In this subphase, you will write a device driver for the mini UART device on the Raspberry Pi. You will primarily be working in lib/pi/src/uart.rs and kern/src/main.rs. The mini UART is documented on page 8 and page 10 (sections 2.1 and 2.2) of the BCM2837 ARM Peripherals Manual .\nUART: Universal Asynchronous RX/TX A UART , or universal asynchronous receiver-transmitter, is a device and serial protocol for communicating over two wires. These are the two wires (rx/tx) that you used in phase 1 of lab 0 to connect the UART device on the CP2102 USB module to the UART device on the Pi. You can send any kind of data over UART: text, binaries, images, anything! As an example, in the next subphase, you\u0026rsquo;ll implement a shell by reading from the UART device on the Pi and writing to the UART device on the CP2102 USB module. In phase 3, you\u0026rsquo;ll read from the UART on the Pi to download a binary being sent via the UART on the CP2102 USB module.\nThe UART protocol has several configuration parameters, and both the receiver and transmitter need to be configured identically to communicate. These parameters are:\nData Size: length of a single data frame (8 or 9 bits) Parity Bit: whether to send a parity (checksum) bit after the data Stop Bits: how many bits to use to signal the end of the data (1 or 2) Baud Rate: transmission rate in bits/second The mini UART on the Pi does not support parity bits and only supports 1 stop bit. As such, only the baud rate and data frame length need to be configured. To learn more about UART, see the Basics of UART Communication article.\nImplement the Driver At this point, you have all of the tools to write a device driver without additional background information (congratulations! ðŸŽ‰).\nImplement the mini UART device driver in lib/pi/src/uart.rs. You\u0026rsquo;ll need to complete the definition of the Registers structure. Ensure that you use the Volatile type with the minimal set of capabilities for each register: read-only registers should use ReadVolatile, write-only registers should use WriteVolatile, and reserved space should use Reserved. Then, initialize the device in new() by setting the baud rate to 115200 (a divider of 270) and data length to 8 bits. Finally, implement the remaining unimplemented!() methods and the fmt::Write, io::Read and io::Write traits for MiniUart.\nYou\u0026rsquo;ll need to write to the LCR, BAUD, and CNTL registers in new.\nUse your GPIO driver from the previous subphase.\nTesting Your Driver Test your driver by writing a simple \u0026ldquo;echo\u0026rdquo; program in kern/src/main.rs: sit in a hot loop writing out every byte you read in. In pseudocode, this looks like:\nloop { write_byte(read_byte()) } Use screen /dev/\u0026lt;your-path\u0026gt; 115200 to communicate over UART. screen sends every keypress over the TTY, so if your echo program works correctly, you\u0026rsquo;ll see every character you type. It might help to send an extra character or two each time you receive a byte to convince yourself things are working as you expect:\nloop { write_byte(read_byte()) write_str(\u0026#34;\u0026lt;-\u0026#34;) } Once your driver works as expected, proceed to the next subphase.\nSubphase E: The Shell In this subphase, you\u0026rsquo;ll use your new UART driver to implement a simple shell that will be the interface to your operating system. You will be working in kern/src/console.rs, kern/src/shell.rs, and kernel/src/main.rs.\nThe Console To write our shell, we\u0026rsquo;ll need some notion of a global default input and output. Unix and friends typically refer to this is as stdin and stdout; we\u0026rsquo;ll be calling it Console. Console will allow us to implement the kprint! and kprintln! macros, our kernel-space versions of the familiar print! and println!, and give us a default source for reading user input. We\u0026rsquo;ll use Console and these macros to implement our shell.\nTake a peek at kernel/src/console.rs. The file contains an unfinished implementation of the Console struct. Console is a singleton wrapper around a MiniUart: only one instance of Console will ever exist in our kernel. That instance will be globally available, for use anywhere and by anything. This will allow us to read and write to the mini UART without explicitly passing around an instance of MiniUart or Console.\nGlobal Mutability The notion of a globally mutable structure is a scary thought, especially in the face of Rust. After all, Rust doesn\u0026rsquo;t allow more than one mutable reference to a value, so how can we possibly convince it to allow as many as we want? The trick, of course, relies on unsafe. The idea is as follows: we\u0026rsquo;ll tell Rust that we\u0026rsquo;re only going to read a value by using an immutable reference, but what we actually do is use unsafe to \u0026ldquo;cast\u0026rdquo; that immutable reference to a mutable reference. Because we can create as many immutable references as we want, Rust will be none the wiser, and we\u0026rsquo;ll have all of the mutable references we desire!\nSuch a function might look like this:\n// This function must never exist. fn make_mut\u0026lt;T\u0026gt;(value: \u0026amp;T) -\u0026gt; \u0026amp;mut T { unsafe { /* magic */ } } Your alarm bells should be ringing: what we\u0026rsquo;ve proposed so far is wildly unsafe. Recall that we still need to ensure that everything we do in unsafe upholds Rust\u0026rsquo;s rules. What we\u0026rsquo;ve proposed thus far clearly does not. As it stands, we\u0026rsquo;re violating the \u0026ldquo;at most one mutable reference at a time\u0026rdquo; rule. The rule states that at any point in the program, a value should have at most one mutable reference to it.\nThe key insight to maintaining this rule while meeting our requirements is as follows: instead of the compiler checking the rule for us with its borrow and ownership checker, we will ensure that the rule is upheld dynamically, at run-time. As a result, we\u0026rsquo;ll be able to share references to a structure as many times as we want (via an \u0026amp; reference) while also being able to safely retrieve a mutable reference when we need it (via our \u0026amp;T -\u0026gt; \u0026amp;mut T dynamic borrow checking function).\nThere are many concrete implementations of this idea. One such implementation ensures that only one mutable reference is returned at a time using a lock:\nfn lock\u0026lt;T\u0026gt;(value: \u0026amp;T) -\u0026gt; Locked\u0026lt;\u0026amp;mut T\u0026gt; { unsafe { lock(value); cast value to Locked\u0026lt;\u0026amp;mut T\u0026gt; } } impl Drop for Locked\u0026lt;\u0026amp;mut T\u0026gt; { fn drop(\u0026amp;mut self) { unlock(self.value) } } This is known as Mutex in the standard library. Another way is to abort the program if more than one mutable reference is about to be created:\nfn get_mut\u0026lt;T\u0026gt;(value: \u0026amp;T) -\u0026gt; Mut\u0026lt;\u0026amp;mut T\u0026gt; { unsafe { if ref_count(value) != 0 { panic!() } ref_count(value) += 1; cast value to Mut\u0026lt;\u0026amp;mut T\u0026gt; } } impl Drop for Mut\u0026lt;\u0026amp;mut T\u0026gt; { fn drop(\u0026amp;mut self) { ref_count(value) -= 1; } } This is RefCell::borrow_mut() . And yet another is to only return a mutable reference if it is known to be exclusive:\nfn get_mut\u0026lt;T\u0026gt;(value: \u0026amp;T) -\u0026gt; Option\u0026lt;Mut\u0026lt;\u0026amp;mut T\u0026gt;\u0026gt; { unsafe { if ref_count(value) != 0 { None } else { ref_count(value) += 1; Some(cast value to Mut\u0026lt;\u0026amp;mut T\u0026gt;) } } } impl Drop for Mut\u0026lt;\u0026amp;mut T\u0026gt; { fn drop(\u0026amp;mut self) { ref_count(value) -= 1; } } This is RefCell::try_borrow_mut() . All of these examples implement some form of \u0026ldquo;interior mutability\u0026rdquo;: they allow a value to be mutated through an immutable reference. For our Console, we\u0026rsquo;ll be using Mutex to accomplish the same goal. Since the std::Mutex implementation requires operating system support, we\u0026rsquo;ve implemented our own Mutex in kern/src/mutex.rs. Our implementation is correct for now, but we\u0026rsquo;ll need to fix it when we introduce caching or concurrency to continue to uphold Rust\u0026rsquo;s rules. You don\u0026rsquo;t need to understand the Mutex implementation for now, but you should understand how to use one.\nThe global singleton is declared as CONSOLE in kern/src/console.rs. The global variable is used by the kprint! and kprintln! macros defined below below. Once you\u0026rsquo;ve implemented Console, you\u0026rsquo;ll be able to use kprint! and kprintln! to print to the console. You\u0026rsquo;ll also be able to use CONSOLE to globally access the console.\nRust also requires static globals to be Sync.\nIn order to store a value of type T in a static global, T must implement Sync. This is because Rust also guarantees data race safety at compile-time. Because global values can be accessed from any thread, Rust must ensure that those accesses are thread-safe. The Send and Sync traits, along with Rust\u0026rsquo;s ownership system, ensure data race freedom.\nWhy should we never return an \u0026amp;mut T directly? (drop-container)\nYou\u0026rsquo;ll notice that every example we\u0026rsquo;ve provided wraps the mutable reference in a container and then implements Drop for that container. What would go wrong if we returned an \u0026amp;mut T directly instead?\nWhere does the write_fmt call go? (write-fmt)\nThe _print helper function calls write_fmt on an instance of MutexGuard\u0026lt;Console\u0026gt;, the return value from Mutex\u0026lt;Console\u0026gt;::lock(). Which type will have its write_fmt method called, and where does the method implementation come from?\nImplement and Test Console implement all of the unimplemented!() methods in kern/src/console.rs. once you\u0026rsquo;ve implemented everything, use the kprint! and kprintln! macros in kern/src/main.rs to write to the console when you receive a character. you can use these macros exactly like print! and println!. use screen /dev/\u0026lt;your-path\u0026gt; 115200 to communicate with your pi and ensure that your kernel works as expected.\nIf this were C\u0026hellip;\nThe fact that we get a println! implementation for free with zero effort is just another advantage to using Rust. If this were C, we\u0026rsquo;d need to implement printf ourselves. In Rust, the compiler provides a generic, abstracted, and safe OS-independent implementation. Whew!\nYour Console implementations should be very short: about a line each.\nImplement the Shell Finished Product You\u0026rsquo;re now ready to implement the shell in kern/src/shell.rs. We\u0026rsquo;ve provided a Command structure for your use. The Command::parse() method provides a simple command-line argument parser, returning a Command struct. The parse method splits the passed in string on spaces and stores all of the non-empty arguments in the args field as a StackVec using the passed in buf as storage. You must implement Command::path() yourself.\nUse all of your available libraries (Command, StackVec, Console via CONSOLE, kprint!, kprintln!, and anything else!) to implement a shell in the shell function. Your shell should print the prefix string on each line it waits for input. In the GIF above, for instance, \u0026quot;\u0026gt; \u0026quot; is being used as the prefix. Your shell should then read a line of input from the user, parse the line into a command, and attempt to execute it. It should do this ad-infinitum. Since our operating system is only just beginning, we can\u0026rsquo;t run any interesting commands just yet. We can, however, build known commands like echo into the shell.\nTo complete your implementation, your shell should\u0026hellip;\nimplement the echo built-in: echo $a $b $c should print $a $b $c accept both \\r and \\n as \u0026ldquo;enter\u0026rdquo;, marking the end of a line accept both backspace and delete (ASCII 8 and 127) to erase a single character ring the bell (ASCII 7) if an unrecognized non-visible character is sent to it print unknown command: $command for an unknown command $command disallow backspacing through the prefix disallow typing more characters than allowed accept commands at most 512 bytes in length accept at most 64 arguments per command start a new line, without error, with the prefix if the user enters an empty command print error: too many arguments if the user passes in too many arguments Test your implementation by calling your new shell() function in kern/src/main.rs. Minus the \u0026ldquo;SOS\u0026rdquo; banner, you should be able to replicate the GIF above. You should also be able to test all of the requirements we\u0026rsquo;ve set. Once your shell works as expected, revel in your accomplishments. Then, proceed to the next phase.\nA byte literal, b'a' is the u8 ASCII value for a character 'a'.\nUse \\u{b} in a string literal to print any character with ASCII byte value b.\nYou must print both \\r and \\n to begin a new line at the line start.\nTo erase a character, backspace, print a space, then backspace again.\nUse StackVec to buffer the user\u0026rsquo;s input.\nYou\u0026rsquo;ll find the core::str::from_utf8() function useful.\nHow does your shell tie the many pieces together? (shell-lookback)\nYour shell makes use of much of the code you\u0026rsquo;ve written. Briefly explain: which pieces does it makes use of and in what way?\nPhase 3: Boot \u0026rsquo;em Up In this phase, you\u0026rsquo;ll use everything you\u0026rsquo;ve written thus far to implement a bootloader for your Raspberry Pi. You\u0026rsquo;ll be working primarily in boot/src/main.rs.\nYou\u0026rsquo;ve likely become frustrated with the monotonous motions of swapping MicroSD cards to load a new binary onto your Pi. The bootloader you will write in this phase eliminates that process entirely. You\u0026rsquo;ll replace the binary on the MicroSD one more time, this time with the bootloader. From then on, you can load new binaries remotely from your computer without ever touching the MicroSD card again.\nThe bootloader itself is a \u0026ldquo;kernel\u0026rdquo; of sorts that accepts XMODEM file transfers over UART. It writes the data received into memory at a known address and then executes it. We\u0026rsquo;ll use our ttywrite utility to send it binaries. As a result, the process to load a new binary onto the Pi will be as simple as:\n#. Resetting the Pi to start the bootloader. #. Run make transmit command, which will build your kernel and transmit it with ttywrite -i build/kern.bin /dev/ttyUSB0 command.\nLoading Binaries By default, the Raspberry Pi 3 loads files named kernel8.img at address 0x80000. Said another way, the Pi will sequentially copy the contents of kernel8.img to 0x80000 and, after some initialization, set the ARM\u0026rsquo;s program counter to 0x80000. As a result, we must ensure that our binary expects to be loaded at this address. This means that all of the addresses in the binary should begin at 0x80000.\nBecause the linker is what decides the addresses for all symbols in our binary, we must somehow inform the linker of this desire. To do this, we use a linker script: a file read by the linker that describes how we want it to assign addresses to symbols in our binary. Our kernel\u0026rsquo;s linker script can be found in kern/.cargo/layout.ld. You\u0026rsquo;ll notice the address 0x80000 on the second line. Indeed, this line instructs the linker to begin allocating addresses at 0x80000.\nTo maintain compatibility with these defaults, our bootloader will also load binaries at address 0x80000. But this raises an issue: if our bootloader\u0026rsquo;s binary is at address 0x80000, loading a different binary at the same address will result in overwriting our bootloader as we\u0026rsquo;re executing it! To avoid this conflict, we must use different start addresses for the bootloader and the binaries it loads. We\u0026rsquo;d like to maintain compatibility with the Pi\u0026rsquo;s defaults, so we\u0026rsquo;ll need to change the start address of the bootloader. How?\nMaking Space The first step is to choose a new address. As you can see in boot/.cargo/layout.ld, we\u0026rsquo;ve chosen 0x4000000 as the start address for our bootloader. While this fixes the addresses in the binary, the Pi will continue to load it at 0x80000. Thankfully, we can ask the Pi to load our binary at a different address via a kernel_address parameter in the firmware\u0026rsquo;s config.txt. Ensure you modify your config.txt in microSD to have kernel_address=0x4000000 line.\nAs a result of this change, the memory between 0x80000 and 0x4000000 will be entirely unused by the bootloader, and we can load binaries up to 0x4000000 - 0x80000 bytes in size without conflict.\nIs 63.5MiB really enough? (small-kernels)\nYou might be thinking that the free space we\u0026rsquo;ve set apart isn\u0026rsquo;t enough. This is a fair concern. One way to answer the question is to look at the file size of kernels from successful operating systems. Would they fit?\nDetermine how large the kernel binary is for the operating system you\u0026rsquo;re running now. On newer versions of macOS, the binary is /System/Library/Kernels/kernel. On older versions of macOS, the binary is /mach_kernel. On Linux, the binary is usually located in /boot/ and is named either vmlinuz, vmlinux, or bzImage. How big is your kernel\u0026rsquo;s binary? Would it fit in the 63.5MiB free space we\u0026rsquo;ve created?\nImplement the Bootloader Implement the bootloader in boot/src/main.rs. We\u0026rsquo;ve declared the bootloader\u0026rsquo;s start address, the loaded binary\u0026rsquo;s start address, and the maximum binary size in const declarations at the top of the file. We\u0026rsquo;ve also provided a jump_to function that unconditionally branches to the address addr. This has the effect of setting the program counter to that address. Your bootloader should use these declarations along with your existing code from the pi and xmodem libraries to receive a transmission over UART and write it to the memory address the binary expects to be loaded at. When the transmission is complete, your bootloader should execute the new binary.\nBe aware that your bootloader should continuously attempt to initiate an XMODEM reception by setting a low timeout value (say, 750ms) and attempting a new reception if a timeout occurs. If a reception fails for any other reason, print an error message and try again. Once you\u0026rsquo;ve implemented the bootloader, test it by sending your kernel binary from kern/build/kernel.bin to your Pi using your ttywrite utility. If all is well, you should see your shell when you screen into your Pi.\nWhy is the timeout necessary? (bootloader-timeout)\nWithout the bootloader timing out and retrying a reception, it is possible for the transmitter to stall indefinitely under some conditions. What are those conditions, and why would the transmitter stall indefinitely?\nconfig.txt\nRemember to use the version of config.txt compatible with bootloader binaries!\nOur reference main() function is 15 lines of code.\nYou\u0026rsquo;ll find the core::slice::from_raw_parts_mut() function useful.\nThe \u0026amp;mut [u8] type implements io::Write.\nSubmission Once you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Congratulations!\nEnsure you\u0026rsquo;ve committed your changes. Any uncommitted changes will not be visible to us, thus unconsidered for grading.\nBefore submitting, check if you\u0026rsquo;ve answered every question and passed every unit tests for the libraries. Note that there are no unit tests for pi and kernel. You\u0026rsquo;re responsible for ensuring that they work as expected.\nWhen you\u0026rsquo;re ready, push a commit to your GitHub repository with a tag named lab2-done.\n# submit lab1 $ git tag lab2-done $ git push --tags ", 
        "url": "\/\/localhost:1313\/post\/lab2\/"
    },
    
    "\/\/localhost:1313\/post\/lab1\/": {
        "title": "Lab 1: Bootstrapping Raspberry Pi",
        "tags": ["Lab",],
        "content": " Handed out: Tuesday, January 21, 2020 Due: Monday, January 27, 2020 Introduction In this assignment, you will set up and test a Raspberry Pi 3, ARM64 development environment for your use throughout the rest of the course. You\u0026rsquo;ll install the necessary tools and write your first bare-metal application, an LED blinky program, in two languages: C and Rust.\nThis assignment is divided into 4 phases. In the first phase, you\u0026rsquo;ll install the necessary software to communicate with your Pi from your machine. You\u0026rsquo;ll also ensure that your Pi works as expected by running a pre-compiled program. In the second phase, you\u0026rsquo;ll connect GPIO pin 16 on your Raspberry Pi to an LED and run a second pre-compiled program to ensure your connections are sound. In the third phase, you\u0026rsquo;ll write, cross-compile, and link a C program that toggles GPIO pin 16 on and off, blinking the LED connected to your Pi. Finally, in phase 4, you\u0026rsquo;ll write, cross-compile, and link the same program in Rust .\nPhase 0: Preflight Check First and foremost, check that your Raspberry Pi kit includes all of the following materials:\n1 x Raspberry Pi 3/B/B+ 1 x microSD card (4-32GB) 1 x microSD reader We will provide you these items in class:\n1 x CP2102 USB 4 x female-female jumper cables 5 x multicolored LEDs 5 x 100 ohm resistors 10 x female-male DuPont jumper cables Electronics are sensitive to electrostatic discharge, so ensure that you ground yourself by touching something conductive before touching any electronics. Prepare two male to female jumper cables for use as well as one resistor (any resistor!) and one LED. Finally, ready the USB microSD card adapter.\nGetting the Skeleton Code To get the skeleton code for lab1, you should fetch the updates from our git repository to your development machine.\n$ git fetch skeleton $ git merge skeleton/lab1 This is the directory structure of our repository.\n. ├── bin : common binaries/utilities ├── doc : reference documents ├── ext : external files (e.g., resources for testing) └── tut : tutorial/practices ├── 0-rustlings └── 1-blinky : this contains files for lab1 As your changes in the previous lab are in tut/0-rustlings, it\u0026rsquo;s unlikely that the merging process should be seamless. If not, please resolve the conflict and proceed to the next phase.\nFeel free to explore the contents of the repository.\nPhase 1: Baking Pi The primary means through which you\u0026rsquo;ll be powering and communicating with your Pi is via a CP2102 USB module. That\u0026rsquo;s the red USB dongle with the five pins.\nIf you are working on virtual machine, you need to configure some setting to enable USB port for microSD card adapter and CP2102. Please refer the Tools page if you haven\u0026rsquo;t set up yet.\nTo test if your CP2102 is working properly, plug it in to a USB port of the development machine with the yellow cap attached. The cap should connext TXD and RXD pins of the serial device. It simply wires both input and output pins of the serial, so you will receive what you send.\nTo see the data you receive from the module, you\u0026rsquo;ll need to run a serial console emulator that connects to the CP2102 module and reads the incoming data. We\u0026rsquo;ll use screen since it comes preinstalled on Linux. Recalling the /dev path for your CP2102 module (this will likely be /dev/ttyUSB0), connect to the CP2102 USB module with screen by running:\n$ screen /dev/\u0026lt;your-path\u0026gt; 115200 You may have to use sudo to run the command. Alternatively, you can add yourself to the dialout user group to avoid using sudo:\n$ sudo gpasswd --add \u0026lt;your-username\u0026gt; dialout If everything goes well, you will see what you type in the screen console as well as the small led lights on the USB module blinking whenever you type. If your CP2102 works as expected, it\u0026rsquo;s time to wire CP2102 to Raspberry Pi. To exit screen, use \u0026lt;ctrl-a\u0026gt; k then answer y at the prompt.\nPowering the Pi First, disconnect the CP2102 module from your computer. Now connect the Raspberry Pi to the CP2102 module using the four female-female jumper cables. The table below shows which pins on the CP2102 module you should connect to which pins on the Raspberry Pi. You can see the CP2102 module pins\u0026rsquo; names if you look at the back of the module\u0026rsquo;s board.\nCP2102 Pin Pi Physical Pin +5v 4 GND 6 RXD 8 TXD 10 Here\u0026rsquo;s how the pins are numbered on the Raspberry Pi 3:\nHere\u0026rsquo;s what the connections should look like (connections not ordered in this pic, just look at the name of the pins):\nDouble-check, triple-check, and quadruple-check your connections! Then, get a friend to double-check your connections! Incorrect wiring can result in a fried Raspberry Pi, which will result in an automatic F in this course. Okay, that last bit isn\u0026rsquo;t true, but Raspberry Pi 3\u0026rsquo;s are expensive, and we don\u0026rsquo;t want to pass out additional ones if we can prevent it. So, quintuple-check your connections before continuing!\nIf you\u0026rsquo;re confident in your connections, it\u0026rsquo;s time to plug in your CP2102 module into your computer. If all went well, you should see a red LED light on your Raspberry Pi shining bright. Congratulations! You\u0026rsquo;re now able to power and communicate with your Pi.\nRunning Programs As discussed in lecture, the Raspberry Pi loads programs on boot from the on-board microSD card. We\u0026rsquo;ll now set up our microSD card with a custom program for the Raspberry Pi to load.\nIn the repo, you can find a testing programs: ext/rpi3-gpio16. To load this program on the Pi, you can simply copy files in the directory to the microSD card - take the microSD card and insert it into the USB microSD card reader. Then, plug the adapter into your machine. You should see a new volume mounted on your machine. (If not, see tools page) Format the microSD card with FAT32 filesystem. Then copy the all files inside the directory (i.e., bootcode.bin, config.txt, fixup.dat, start.elf and kernel8.img), to the root of the microSD card. As discussed in lecture, these are four of the five files read by the GPU on boot. The fifth, kernel8.img, is the boot program, which we are about to install.\nWhat are bootcode.bin, config.txt, and start.elf?\nThese specially-named files are recognized by the Raspberry Pi\u0026rsquo;s GPU on boot-up and used to configure and boostrap the system. bootcode.bin is the GPU\u0026rsquo;s first-stage bootloader. Its primary job is to load start.elf, the GPU\u0026rsquo;s second-stage bootloader. start.elf initializes the ARM CPU, configuring it as indicated in config.txt, loads kernel8.img into memory, and instructs the CPU to start executing the newly loaded code from kernel8.img.\nNow, let\u0026rsquo;s ensure your pi and serial works as expected. Having copied all the five files inside ext/rpi3-gpio16 from the repo to the root of the microSD card,unmount the microSD card, disconnect the USB adapter from your machine, and remove the microSD card from the USB adapter. Ensure your Raspberry Pi 3 isn\u0026rsquo;t currently powered. Then, insert the microSD card into the Rasperry Pi 3. Plug in the Pi. In just a short moment, you should see a green LED on the Raspberry Pi board blinking very fast. You should also see a red LED on the USB CP2102 adapter blinking at the same frequency. This indicates that data is being sent to/from the Raspberry Pi.\nThis process is very tedious and error-prone. Feel free to use a script prepared (bin/install-kernel.py). You can simply specify a kernel or directory for installation, like bin/install-kernel.py ext/rpi3-gpio16. The script will prompt you for the directory where the microSD card was mounted, after you provide that, the script will copy all the files inside the directory you gave in the arguments to the microSD and unmount the microSD for you; so you can directly disconnect it from the computer.\nTo see the data the Raspberry Pi is sending, you\u0026rsquo;ll need to run a serial console emulator that connects to the CP2102 module and reads the incoming data, like described above.\nMake sure your Pi has enough power\nYour Pi will get its power from your computer, through the CP2102 module. Your computer may not supply enough power, which will cause your Pi to be under-powered. An under-powered Pi have indeterministic behavior, it may even corrupt your microSD card. Make sure that your Pi has enough power by checking the red led light on the corner of your Pi: it should be a steady ON when your Pi is connected.\nPhase 2: LED There Be Light In this phase, you\u0026rsquo;ll connect GPIO pin 16 (physical pin 36) on the Raspberry Pi to an LED light. You\u0026rsquo;ll test the LED using a pre-compiled binary. Before starting, ensure your Raspberry Pi is unplugged.\nIs it safe to simply unplug the USB TTL adapter?\nYes! On a traditional computer, and later on in the course, removing the power source without consideration is a bad idea because operations, typically on the disk, may be in-flight. Stopping these operations midway can result in damaged hardware or inconsistent state on the next boot-up. For now, our Pis have no state or moving hardware, so we don\u0026rsquo;t need to worry about this. Feel free to simply unplug its power source!\nGPIO: General Purpose I/O GPIO stands for General Purpose Input/Output. As the name implies, GPIO is a general mechanism for transmitting data/signals into and out of some device through electrical pins, known as GPIO pins.\nA GPIO pin can act as either an output or input. When a GPIO pin is acting as an output, it can either be set on or off. When on, the Raspberry Pi drives the pin at 3.3v. When the GPIO pin is off, no current flows through the pin. When a GPIO pin is acting as an input, the Raspberry Pi reports whether the pin is being driven at 3.3v or not.\nGPIO pins are incredibly versatile and can be used to implement a wide array of functionality. You can read more about GPIO on the Raspberry Pi Foundation\u0026rsquo;s GPIO Usage Documentation .\nTesting the LED We\u0026rsquo;ll start by constructing the circuit below:\nThis circuit connects an LED to always-on 3.3v power on the Raspberry Pi. Note that pin 1 (+3.3v) on the Raspberry Pi should go to the longer leg of your LED. The shorter leg is connected to the resistor which in-turn is connected to pin 14 (ground) on the Pi.\nAfter you\u0026rsquo;ve confirmed your connections, plug the Raspberry Pi in. Your LED should turn on. After you\u0026rsquo;ve confirmed that the LED works as expected, unplug your Rapsberry Pi. Then, move the jumper cable from pin 1 on the Pi to pin 36 (GPIO Pin 16) as illustrated below:\nHaving installed our ext/rpi3-gpio16 kernel in the microSD card, which drives GPIO pin 16 on and off repeatedly; once you plug your Pi in with the microSD inserted, you should see your LED start blinking.\nPhase 3: Shining C In this phase, you\u0026rsquo;ll write the program that produced ext/rpi3-gpio16/kernel8.img in C. You\u0026rsquo;ll write your code in tut/1-blinky/phase3/blinky.c. To be able to compile C and assembly programs for the Raspberry Pi, we\u0026rsquo;ll use a cross compiler for the aarch64-none-elf target that is in the bin directory on the repository.\nTalking to Hardware The vast majority of modern hardware devices communicate with software through memory-mapped I/O. The concept is simple: devices expose their functionality through the machine\u0026rsquo;s memory and provide a specification about what will happen if certain addresses are read or written to. Addresses are usually separated into 32 or 64-bit sized regions known as registers. Registers are usually named to indicate their functionality. Registers can be read-only, write-only, or read/write.\nHow do we know which registers a device exposes, where in memory they\u0026rsquo;re mapped, and what they do? Device manufacturers document all of this information in what is typically referred to as a \u0026ldquo;data sheet\u0026rdquo;, \u0026ldquo;device manual\u0026rdquo;, or simply \u0026ldquo;documentation\u0026rdquo;. There is no widespread format for how devices are documented, and documentation quality is hit or miss. Reading and understanding hardware documentation is a skill and art.\nGPIO Memory-Mapped Interface The documentation for many of the peripherals on-board the Rasperry Pi can be found in Broadcom\u0026rsquo;s BCM2837 ARM Peripherals Manual (or in doc/BCM2837-ARM-Peripherals.pdf). The documentation for GPIO, for example, is on page 89.\nWait, aren\u0026rsquo;t we using a BCM2837 chip?\nIf you open the manual we\u0026rsquo;ve linked you to, you\u0026rsquo;ll see references to the BCM2835 chip everywhere. This is because we\u0026rsquo;ve simply taken the documentation for the BCM2835 chip, fixed relevant errata, and fixed the title to say BCM2837.\nThe BCM2837 and BCM2835 share the same peripherals with the same relative memory-mapped interfaces. The main difference is that the chips differ in their physical memory configuration. This results in the BCM2837 having a peripheral physical base address of 0x3F000000 as opposed to the BCM2835\u0026rsquo;s 0x20000000. But both chips map this range to the peripheral base address of 0x7E000000. In short, a peripheral address 0x7EXXXXXX is at physical address 0x3FXXXXXX on the BCM2837. The \u0026ldquo;BCM2837\u0026rdquo; documentation we\u0026rsquo;ve linked to contains this change.\nFor this assignment, we\u0026rsquo;ll only need to use the following three registers:\nname peripheral address description size read/write GPFSEL1 0x7E200004 GPIO Function Select 1 32 bits R/W GPSET0 0x7E20001C GPIO Pin Output Set 0 32 bits W GPCLR0 0x7E200028 GPIO Pin Output Clear 0 32 bits W We\u0026rsquo;ve copied this information directly from page 90 of the documentation.\nNow, read the documentation for GPFSELn register on pages 91 and 92. We write to this register to set up a pin as an output or input. Which value to which field in register GPFSEL1 must be written so that GPIO pin 16 is set as an output?\nNow, read the documentation for the GPSET0 and GPCLR0 registers on page 95. We write to GPSET0 to set a pin (turn it on) and write to GPCLR0 to clear a pin (turn it off). Which value do we write to which field in these registers to set/clear pin 16?\nWriting the Code The tut/1-blinky/phase3/ directory in the assignment skeleton contains a scaffold for building a binary suitable for running on the Raspberry Pi 3. We won\u0026rsquo;t explain crt0.S, layout.ld, or Makefile for now. Instead, you\u0026rsquo;ll work on blinky.c.\nIn blinky.c, you\u0026rsquo;ll find that we\u0026rsquo;ve declared the physical addresses of the three relevant registers at the top of the file. Your task is to complete the main() function so that GPIO pin 16 is set-up as an output and then continuously set and cleared to blink the LED. We\u0026rsquo;ve also provided rudimentary \u0026ldquo;sleep\u0026rdquo; functions that stall the CPU for roughly the amount of time the function name indicates. You can use these to pause between sets and clears.\nWhen you are ready to test your program, compile it by running make in your shell. If all goes well, this will create the file blinky.bin which is our first bare metal program running on the Pi. To run the program on the Raspberry Pi, copy it as kernel8.img to the microSD, unmount the microSD, and insert the microSD back to your Pi. We provide a convenient script to copy the kernel image to the sdcard - try make install.\nIf your program blinks the led as expected, proceed to phase 4.\nPin function selection, setting, and clearing can each be implemented in one line of code.\nRecall the \u0026lt;\u0026lt;, |, \u0026amp;, and ~ bitwise operators in C.\nRecall that binary/hex numbers in C are written as 0b011 and 0x03, respectively.\nPhase 4: Rusting Away In this phase, you\u0026rsquo;ll write the same program in phase 3, but with Rust! You\u0026rsquo;ll write your code in tut/1-blinky/phase4/src/main.rs.\nAll required programs have already been installed to your system via bin/setup.sh before. Let\u0026rsquo;s double check.\n$ make --version GNU Make 4.1 ... $ rustc --version rustc 1.37.0-nightly (0af8e872e 2019-06-30) $ cargo xbuild --version cargo-xbuild 0.5.20 $ cargo objcopy --version cargo-objcopy 0.1.7 That\u0026rsquo;s it! You have everything ready for this assignment.\nCheck your Rust compiler version\nMake sure your Rust version matches rustc 1.37.0-nightly (0af8e872e 2019-06-30). The toolchain name for this version is nightly-2019-07-01. We will use a few unstable features of Rust from this assignment (e.g., inline assembly), and not matching the compiler version may result in spurious compile error due to the incompatibility.\nWriting the Code To write the required code in tut/1-blinky/phase4/src/main.rs, you\u0026rsquo;ll only need to know the following Rust:\nYou can read and write from a raw pointer (*mut T) using the read_volatile() and write_volatile() methods.\nFor example, if we have the following declarations:\nconst A: *mut u32 = 0x12 as *mut u32; const B: *mut u32 = 0x34 as *mut u32; We can write the 32-bit unsigned integer at address 0x12 to 0x34 with the following:\nB.write_volatile(A.read_volatile()); Local variables can be declared with let variable_name = expression;.\nUsing the A declaration from the previous example, we can read the value at address 0x12 to a local variable value as follows:\nlet value = A.read_volatile() You can call a function fn f(param: usize); with f(123);.\nA loop block can be used to repeat a block infinitely:\nloop { do_this_again_and_again(); } Rust defines the following bitwise operators:\n!: unary bitwise inversion \u0026lt;\u0026lt;: left shift binary operator \u0026gt;\u0026gt;: right shift binary operator |: bitwise OR binary operator \u0026amp;: bitwise AND binary operator You are now ready to implement the blink program in tut/1-blinky/phase4/src/main.rs. Translate your C implementation into Rust in the kmain function. You\u0026rsquo;ll find that we\u0026rsquo;ve declared the physical addresses of the three relevant registers at the top of the file. We\u0026rsquo;ve also provided a rudimentary \u0026ldquo;sleep\u0026rdquo; function that stalls the CPU for roughly the amount of time the function name indicates. You can use the function to pause between sets and clears.\nWhen you are ready to test your program, compile it by running make from the phase4 directory in your shell. If all goes well, this will create the file build/blinky.bin. You can install blinky.bin to the microSD via make install. If your Rust program starts blinking the led, you have successfully completed assignment 0! Proceed to the next phase for submission instructions.\nYour Rust and C code should look very similar.\nSubmission Once you\u0026rsquo;ve completed the tasks above, you\u0026rsquo;re done and ready to submit! Ensure you\u0026rsquo;ve committed your changes. Any uncommitted changes will not be visible to us, thus unconsidered for grading.\nWhen you\u0026rsquo;re ready, push a commit to your GitHub repository with a tag named lab1-done.\n$ git status $ git add -A $ git commit -a -m \u0026#34;yay, lab1 done!\u0026#34; $ git tag lab1-done $ git push --tags ", 
        "url": "\/\/localhost:1313\/post\/lab1\/"
    },
    
    "\/\/localhost:1313\/post\/lab0\/": {
        "title": "Lab 0: Rustlings",
        "tags": ["Lab",],
        "content": " Handed out: Tuesday, January 7, 2020 Due: Monday, January 20, 2020 Introduction Welcome to CS3210! We\u0026rsquo;re really excited that you\u0026rsquo;ve decided to join us for this inaugural incarnation. We hope you have as much fun taking this class as we\u0026rsquo;ve had developing it! And, of course, we hope you learn a ton.\nWe\u0026rsquo;re hoping to do things a little differently in CS3210 than the norm. In particular, we want you to write the majority of the code, and we want your code to execute on real hardware, doing real things. There\u0026rsquo;s going to be very little scaffolding provided for you, and all of your code will target and run on the Raspberry Pi 3/B/B+, a quad-core ARMv8 Cortex-A53 based embedded platform.\nUnlike last serveral years, we\u0026rsquo;ll be programming a toy OS in Rust , \u0026ldquo;a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety.\u0026rdquo; You can think of Rust as a successor to C and C++, but it\u0026rsquo;s more useful to think of it as a brand new language with new concepts. Don\u0026rsquo;t try to program in Rust as if it\u0026rsquo;s C, C++, Java, or any other language you\u0026rsquo;re used to. Instead, see it as an opportunity to learn a new way to write software - a new rusty way. By the end of the term, we think you\u0026rsquo;ll agree that Rust helps you write more correct, more reliable, and importantly more secure software, which are the key properties for operating systems!\nYou\u0026rsquo;ll be writing a lot of code in CS3210, though we think that the majority of your time will be spent thinking about what needs to happen. We expect that most assignments will take about 15-20 hours a week to complete, which means you should, under no circumstance, procrastinate in CS3210. Start early, ask questions, visit us at the office hours, and importantly have fun - you\u0026rsquo;ll learn much more if you give yourself a chance to think through hard problems.\nWe\u0026rsquo;re here to answer any and all questions. Please don\u0026rsquo;t hesitate to reach out to us. Best of luck!\nOverview Mastering a programming language takes time. In fact, truly mastering a single programming language takes long time, perhaps our lifetime. In other words, it\u0026rsquo;s unrealistic to assume that we become a true master of Rust in a week\nbut we all hope we can start tinkering anything with Rust at the end of this semester. As a warm up, we will spend sometime to understand syntactic as well as semantic foundation of the Rust programming language by answering a series of questions we prepared.\nTo get set up your environment, let\u0026rsquo;s first install required dependencies and the Rust ecosystems, namely rustc and cargo. We assume that you are doing the assignments on a latest LTS Ubuntu distribution (i.e., Ubuntu 18.04 LTS). If you are not running Ubuntu natively, please check the Tool page to prepare a Ubuntu virtual machine. Also if you are not familiar with git, please first check Pro Git Book .\n$ git clone https://github.com/sslab-gatech/cs3210-rustos-public.git --origin skeleton rustos $ cd rustos # This script automatically setup the environment $ bin/setup.sh ... $ export PATH=$HOME/.cargo/bin:$PATH $ rustc --version rustc 1.37.0-nightly (0af8e872e 2019-06-30) In the repository, we try hard to keep everything in place\nno additional download or dependencies. It looks like: . ├── bin : common binaries/utilities (e.g., objdump for aarch64) ├── doc : reference documents └── tut : tutorial/practices (i.e., no further reuse for later labs) └── 0-rustlings : this contains files for lab0 Rustlings Our goal of lab0 is to guide you to read from Chapter 1 to Chapter 18 of Rust By Example .\nTo check your understanding, we will use Rustlings , which is a neat way to get familiarized with Rust by solving a series of simple exercises. It provides an interactive session of training by comprehending the compiler error messages and resolving it. While doing exercises, you would like to read various chapters of The Rust Book . You can also find more pointers on Rust in the course\u0026rsquo;s reference page.\n$ cd tut/0-rustlings $ ./rustlings help ... SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) hint Returns a hint for the current exercise run Runs/Tests a single exercise verify Verifies all exercises according to the recommended order watch Reruns `verify` when files were edited To start your exercise, you can invoke the watch command that guides you which exercise to work on in a proper order:\n$ ./rustlings watch If you\u0026rsquo;d like, you can also do each exercise (files in exercises/) by invoking ./rustling run [name].\n$ ./rustlings run variables1 ! Compilation of exercises/variables/variables1.rs failed! Compiler error message: error[E0425]: cannot find value `x` in this scope --\u0026gt; exercises/variables/variables1.rs:12:5 | 12 | x = 5; | ^ not found in this scope error[E0425]: cannot find value `x` in this scope --\u0026gt; exercises/variables/variables1.rs:13:36 | 13 | println!(\u0026#34;x has the value {}\u0026#34;, x); | ^ not found in this scope error: aborting due to 2 previous errors For more information about this error, try `rustc --explain E0425`. If you find the exercise too difficult, ask rustlings for hints of the exercise, like below:\n$ rustlings hint variables1 Hint: The declaration on line 12 is missing a keyword that is needed in Rust to create a new variable binding To complete this assignment, ./rusting verify should say all the prepared exercises are successfully completed!\nAssignment Submission We are going to use Georgia Tech\u0026rsquo;s GitHub for the assignment submission in CS3210. After logging into the system, create a repository named rustos. When creating the repository, make it private and do not initialize the repository with a README. Next, give TAs\u0026rsquo; accounts access to your repository for the grading purpose. You should add us in Settings \u0026gt; Collaborators in your repository menu.\nYechan Bae (ybae38) Sujin Park (spark800) Mansour Alharthi (malharthi9) Taesoo Kim (tkim92) Once you have created the repository, inform us via Google Form .\nTo submit your code for lab0, push a commit to the GitHub repository with a tag named lab0-done. We will fetch the assignment code from your repository on each deadline. The following git command samples might be helpful.\n# connect your local repository to GitHub $ git remote add origin https://github.gatech.edu/[id]/rustos $ git push -u origin master # make a commit and publish the update $ git status $ git add -A $ git commit -a -m \u0026#34;yay, lab0 done!\u0026#34; $ git push # submit lab0 $ git tag lab0-done $ git push --tags Grading Policy Your score will be simply calculated based on the submission date of the \u0026ldquo;completed\u0026rdquo; assignment. The meaning of \u0026ldquo;completion\u0026rdquo; is for your repository to be tagged as \u0026ldquo;labN-done\u0026rdquo; (e.g., lab0-done). If your code in the repo passes all test cases provided as part of the assignment on time, you will get the maximum score (i.e., 100) for the corresponding lab. If you are late in submitting the completed assignment, your score of the lab will be max(0, 100*[#passes/#testcases] - [#late-day]*10), in theory.\nHowever, we give a week (7 days in total for all labs) of a grace period, which you can use without even letting us know! For example, if you are one-day late in submitting the first lab (i.e., the completed lab0), you still get 100 points for the lab but your grace period is first reduced to 6 days. Once you don\u0026rsquo;t have any grace period left, the above rule for the lab effectively takes place. For example, if you don\u0026rsquo;t have any grace period but again late in submitting the lab one day, your score would be max(0, 100 - 1*10), so 90 for the completed lab.\nWe will regularly fetch your repository after the initial deadline, and your score and the grace period will be automatically updated on canvas. Please send us an email if you made a late change to the assignment and your score had not updated for a week.\nWe really hope all the students in this class get A!\n", 
        "url": "\/\/localhost:1313\/post\/lab0\/"
    },
    
    "\/\/localhost:1313\/": {
        "title": "",
        "tags": [],
        "content": " ", 
        "url": "\/\/localhost:1313\/"
    },
    
    "\/\/localhost:1313\/categories\/": {
        "title": "Categories",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/"
    },
    
}
</script>
<script defer src="/js/lunr.js"></script>
<script defer src="/js/search.js"></script>

</footer>

</body>
</html>

